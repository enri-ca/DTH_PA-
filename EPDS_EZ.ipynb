{"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#Intro\n\nThis Jupyter Notebook has been created for the <a href=\"https://www.unibo.it/it/didattica/insegnamenti/insegnamento/2021/443749\" target=\"_blank\">90154 - Electronic Publishing and Digital Storytelling</a> course, taught by **Prof. Marilena Daquino**, in the framework of the 2nd year of the <a href=\"https://corsi.unibo.it/2cycle/DigitalHumanitiesKnowledge\" target=\"_blank\">DHDK Master Degree</a>, a.a. 2021-22.<br>\nHere listed the main steps for the realization of the project **Partizione Antica**:     \n       \n    1. Data Preparation:\n          - creation of two complexive xml files for F and OA records coming from the Federico Zeri Foundation catalogues\n          - extraction of relevant information for the project from nested xml stucture and structuring them in plain tabular format\n          - extraction from previpus tabular data of unstructured annotations\n    2. Data Elaboration: seeking for furter analysis elements via:\n          - deeper work on photographer for enhancing their information (workplace, timespan of activity, etc.)\n          - deeper work on places for enhancing their gelocation\n          - work on unstructured annotations trough NLP and NER\n    3. Data Visualization\n    - \n    -\n    -","metadata":{"id":"D76tTDYyH-z9"}},{"cell_type":"markdown","source":"# 1. Data preparation\n\nThis research started from a **record data extraction** of the Supino Partizione Antica fund provided from the Federico Zeri Foundation: the original data counted 3.260 records for photographs and 2.634 records for depicted works of art. <br>\nThe original data have been used for **illustrative and didactical purposes only**: all the credits and reuse authorizations must be asked to <a href=\"mailto:fondazionezeri.fototeca@unibo.it\">Federico Zeri Foundation</a>.\n\n**1.1 Creation of the F and OA complexive xml files**\n\nTo allow a better management and manipulation, as well as to anonymize personal data, complexive files (via <a href=\"/content/sample_data/0_Creation_UniqeXML.xquery\" target=\"_blank\">0_Creation_UniqeXML.xquery</a> collection command) have been created and published. \nThey collect:\n\n*   all the single photograph xml files' records in the F_entries.xml file (data/0_source_data)\n*   all the single works of art xml files' records in the OA_entries.xml file (data/0_source_data)\n\n**1.2 Creation of the flat tabular dataset extracting relevant information for the project from the nested xml elements and attributes**\n\nDue to the hypernested and not consistently presence of elements at different levels, <pandas.read_xml> method was not effectively parsing what was needed.\nThe <xml.etree.ElementTree> library has then been preferred because it allows to call for single elements at different nesting levels. Nevertheless, this approach presents some drawbacks as the need of a previous and deep knowledge of the database structure that does not allow to uncover unexpected correlations possible through the exploration of a comprehensive dataset.\n\n**1.3 Preliminary installation**(Uncomment the first line to install the library)\n- libraries\n- imports:\n  - xml.etree.ElementTree, pandas, csv for managing the dataset\n  - ...\n","metadata":{"id":"TYn9-Vu-cI03"}},{"cell_type":"code","source":"#preliminary imports\n#!pip install python-csv\n#!pip install elementpath\n#!pip install pandas\n\nimport xml.etree.ElementTree as ET\nimport csv\nimport pandas as pd","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"4ZW8b0GdYl6l","outputId":"ffd5c978-2c92-4c67-8174-b98bfd519316"},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting python-csv\n  Using cached python-csv-0.0.13.tar.gz (26 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting argparse\n  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\nRequirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-csv) (1.21.6)\nRequirement already satisfied: pandas in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-csv) (1.3.5)\nCollecting matplotlib\n  Using cached matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\nCollecting xlrd\n  Using cached xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\nCollecting xmltodict\n  Using cached xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\nCollecting demjson\n  Using cached demjson-2.2.4.tar.gz (131 kB)\n  Preparing metadata (setup.py) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[1 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m error in demjson setup command: use_2to3 is invalid.\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\nRequirement already satisfied: elementpath in /srv/conda/envs/notebook/lib/python3.7/site-packages (3.0.2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"1.1 Prepare structured data from metadata","metadata":{}},{"cell_type":"code","source":"#preliminary imports\n#!pip install python-csv\n#!pip install elementpath\n#!pip install pandas\n\nimport xml.etree.ElementTree as ET\nimport csv\nimport pandas as pd","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"4ZW8b0GdYl6l","outputId":"ffd5c978-2c92-4c67-8174-b98bfd519316","trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pandas\n  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (2.8.2)\nCollecting numpy>=1.17.3\n  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (2022.7)\nRequirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\nInstalling collected packages: numpy, pandas\nSuccessfully installed numpy-1.21.6 pandas-1.3.5\n","output_type":"stream"}]},{"cell_type":"code","source":"#function to have back the element texts\ndef extract_data(path):\n    if SCHEDA.find(path) != None:\n        name = SCHEDA.find(path).text\n    else:\n        name = None\n    return name\n\n#parse the complexive Fxml and OAxml files\nF_tree = ET.parse(\"data/0_source_data/F_entries.xml\")\nF_root = F_tree.getroot()\nF_root.attrib[\"test\"]\n\nOA_tree = ET.parse(\"data/0_source_data/OA_entries.xml\")\nOA_root = OA_tree.getroot()\nOA_root.attrib[\"test2\"]\n\n#set the colums' headers for the choosen elements\nheader = [\"sercdf_F_ser\", \"sercdoa_OA_ser\", \"INVN_F\", \"UBFC_Fshelfmark\", #ids\n          \"PVCS_OAcountry\", \"PVCC_OAtown\", \"LDCN_OArep\", \"PRVC_OAprev_town\", \"AUFI_Fatelier_address\", #places\n          \"AUFN_Faut\", \"SGLT_Ftitle\", \"SGTT_OAtitle\", \"AUTN_OAaut\", #authors/titles\n          \"OGTT_OAtype\", \"AUTB_Fsubj_main\", \"OGTDOA_OAsubj_sub\", #subjects\n          \"ROFI_Fneg\", \"BIBA_OAbib\",#external relations\n          \"OSS_Fnotes\", \"OSS_OAnotes\", #unstructured infos\n         \"FTAN_filename\", \"NCTN_F_entry\", \"NRSCHEDA_OA_entry\", #2ary ids\n        \"DTZG_OAcentury\", \"LRD_Fshotdates\", \"DTSI_Fprintdates\", \"DTSF_Fprintdates\", \"AUFA_Faut_dates\"] #time\n\n#setting an empty list\ndata = []\n\n#iterate on F_entries - and on correspondig OA_entries -\n#for extracting elements texts, store them in a list and add it to the data\n#two fields from original data are futherly modify for our purposes\n\nfor SCHEDA in F_root.findall(\"SCHEDA\"):\n    oa_ser = SCHEDA.get(\"sercdoa\")\n    f_ser = SCHEDA.get(\"sercdf\")\n    inv = extract_data(\"./PARAGRAFO/INVN\")\n    container = extract_data(\"./PARAGRAFO/UBFT\")\n    shelf = extract_data(\"./PARAGRAFO/UBFC\")\n    title_f = extract_data(\"./PARAGRAFO/RIPETIZIONE/SGLT\")\n    aut_f = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUFN\") #the original data do not distinguish AUFN and AUFB for collective agents\n    aut_f_dates = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUFA\")#timespan of photographer's actvity\n    aut_f_addr = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUFI\")#place of photographer's actvity as reported in the photograph >AF of variants\n    aut_oa = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUTN\")\n    subj_main = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUTB\")\n    subj_sub = extract_data(\"./PARAGRAFO/RIPETIZIONE/OGTDOA\")\n    notes_f = extract_data(\"./PARAGRAFO/OSS\")\n    neg_num = extract_data(\"./PARAGRAFO/ROFI\")\n    f_entry = extract_data(\"./PARAGRAFO/NCTN\")\n    filename = extract_data(\"./PARAGRAFO/FTAN\")\n    shotdates = extract_data(\"./PARAGRAFO/LRD\")\n    if shotdates != None:\n        #reduce uncertainty: if /ante in field, put 1855 as conventional beginning date\n        #for collodium negatives (accordign to other Zeri cataloguing)\n        if \"/ante\" in shotdates:\n            shotdates.replace(\"/ante\", \"/ ante\")\n        if \"/ ante\" in shotdates:\n        #if re.match(\"/ante|\\/ ante\", shotdates):\n            shotdates = \"1855-\"+shotdates[:-6]\n    printdates_start = extract_data(\"./PARAGRAFO/DTSI\")\n    printdates_end = extract_data(\"./PARAGRAFO/DTSF\")\n\n    for SCHEDA in OA_root.findall(\"SCHEDA\"):\n        if SCHEDA.get(\"sercdoa\") == oa_ser:\n            title_oa = extract_data(\"./PARAGRAFO/SGTT\")\n            century_oa = extract_data(\"./PARAGRAFO/DTZG\")\n            country_oa = extract_data(\"./PARAGRAFO/PVCS\") #Original data report just 2 LRCS: name of the country where the shot was taken.\n            town_oa = extract_data(\"./PARAGRAFO/PVCC\") #Original data report just 2 LRCC: name of the country where the shot was taken.\n            rep_oa = extract_data(\"./PARAGRAFO/LDCN\")\n            prev_town_oa = extract_data(\"./PARAGRAFO/RIPETIZIONE/PRVC\")\n            if prev_town_oa != None:\n                #save the previous locations only if in 1800-1899 timespan (PRDU) otherwise put \"NR\" (not relevant)\n                if extract_data(\"./PARAGRAFO/RIPETIZIONE/PRDU\") != None:\n                    if \"1799\" < extract_data(\"./PARAGRAFO/RIPETIZIONE/PRDU\") < \"1900\": #PRDU last date the OA was in that location\n                        prev_town_oa = prev_town_oa + \" | \" + str(extract_data(\"./PARAGRAFO/RIPETIZIONE/PRDU\"))\n                    else:\n                        prev_town_oa = \"NR\"\n            type_oa = extract_data(\"./PARAGRAFO/OGTT\")\n            notes_oa = extract_data(\"./PARAGRAFO/OSS\")\n            oa_entry = extract_data(\"./PARAGRAFO/NRSCHEDA\")\n            beg_date_oa = extract_data(\"./PARAGRAFO/DTSI\")\n            if extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBA\") != None:\n                #save the bib ref only if in 1800-1899 timespan (BIBD) otherwise put \"NR\" (not relevant)\n                if extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBD\") != None:\n                    if \"1799\" < extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBD\") < \"1900\": #PRDU last date the OA was in that location\n                        bib_oa = extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBA\")\n                        bib_oa = bib_oa + \" | \" + str(extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBD\"))\n                    else:\n                        bib_oa = \"NR\"\n            else:\n                bib_oa = None\n\n    row = [oa_ser, f_ser, inv, shelf,\n           country_oa, town_oa, rep_oa, prev_town_oa, aut_f_addr,\n           aut_f, title_f, title_oa, aut_oa,\n           type_oa, subj_main, subj_sub,\n           neg_num, bib_oa,\n           notes_f, notes_oa,\n           filename, f_entry, oa_entry,\n           century_oa, shotdates, printdates_start, printdates_end, aut_f_dates]\n    data.append(row)\n\n#Write the data and their header in a new csv dataset\nwith open(\"data/F_OA_selected_data.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as tabular_data:\n    # create the csv writer\n    writer = csv.writer(tabular_data)\n    writer.writerow(header)\n    writer.writerows(data)\n\n#have a look at the data\ndata_df = pd.read_csv('data/F_OA_selected_data.csv')\n#data_df.describe()\n#print(data_df.head(10))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"id":"d_Z8m2qdjB8K","outputId":"fd84ccf1-9ca7-484c-e2db-bba17285e26a","trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"        sercdf_F_ser  sercdoa_OA_ser  NCTN_F_entry  NRSCHEDA_OA_entry\ncount    3260.000000     3260.000000  3.260000e+03        3260.000000\nmean   100774.133436   180861.499693  8.038541e+05       97363.334049\nstd     17153.184334     1803.448015  1.261344e+05       17909.959133\nmin       630.000000   177166.000000  8.000010e+05           6.000000\n25%    106678.750000   179395.750000  8.008268e+05      102329.750000\n50%    107546.500000   180637.500000  8.016455e+05      105018.500000\n75%    108512.000000   182324.250000  8.024692e+05      105878.250000\nmax    109559.000000   184628.000000  8.003261e+06      106877.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sercdf_F_ser</th>\n      <th>sercdoa_OA_ser</th>\n      <th>NCTN_F_entry</th>\n      <th>NRSCHEDA_OA_entry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3260.000000</td>\n      <td>3260.000000</td>\n      <td>3.260000e+03</td>\n      <td>3260.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>100774.133436</td>\n      <td>180861.499693</td>\n      <td>8.038541e+05</td>\n      <td>97363.334049</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>17153.184334</td>\n      <td>1803.448015</td>\n      <td>1.261344e+05</td>\n      <td>17909.959133</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>630.000000</td>\n      <td>177166.000000</td>\n      <td>8.000010e+05</td>\n      <td>6.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>106678.750000</td>\n      <td>179395.750000</td>\n      <td>8.008268e+05</td>\n      <td>102329.750000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>107546.500000</td>\n      <td>180637.500000</td>\n      <td>8.016455e+05</td>\n      <td>105018.500000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>108512.000000</td>\n      <td>182324.250000</td>\n      <td>8.024692e+05</td>\n      <td>105878.250000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>109559.000000</td>\n      <td>184628.000000</td>\n      <td>8.003261e+06</td>\n      <td>106877.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"1.2 Prepare unstructured data from transcribed annotations","metadata":{}},{"cell_type":"code","source":"#!pip install pandas\nimport pandas as pd","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"f_L0Nt8bOhrU","outputId":"40c00cfb-a824-4acc-d8b4-291b0ea5c407"},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /srv/conda/envs/notebook/lib/python3.7/site-packages (1.3.5)\nRequirement already satisfied: python-dateutil>=2.7.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: numpy>=1.17.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (1.21.6)\nRequirement already satisfied: pytz>=2017.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (2022.7)\nRequirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"data_df = pd.read_csv('data/F_OA_selected_data.csv')\n\n#reduce the dataset to just the columns needed, the not-empty and not-duplicates rows\nOAnotes_df = data_df[[\"OSS_OAnotes\"]].dropna()\nOAnotes_df = OAnotes_df.drop_duplicates()\n\n#split multilines rows and once again remove duplicates rows\nOAnotes_df[\"OSS_OAnotes\"] = OAnotes_df[\"OSS_OAnotes\"].str.split(\"&#10;|\"\". Foto \", expand = False)\nOAnotes_df = OAnotes_df.explode(\"OSS_OAnotes\")\nOAnotes_df = OAnotes_df.drop_duplicates()\n\n#save just rows with transcriptions notes (including \"Foto sup \\d{1,4}\" string)\nOAnotes_df = OAnotes_df[OAnotes_df[\"OSS_OAnotes\"].str.contains(\"sup \\d{1,4}\")== True].reset_index(drop=True)\nOAnotes_df = OAnotes_df[OAnotes_df[\"OSS_OAnotes\"].str.startswith(\"La foto \")== False].reset_index(drop=True)\n\n#separe note texts from other infos and remove the column containing the whole infos, save and check the result\nOAnotes_df[[\"Inv\", \"Note\"]] = OAnotes_df[\"OSS_OAnotes\"].str.split(': \"', n=1, expand=True)\nOAnotes_df= OAnotes_df.drop(columns=[\"OSS_OAnotes\"]).reset_index(drop=True)\nprint(\"Photographs which annotations have been transcribed in OA entries: \", OAnotes_df.shape[0], \"(/over 3.222 photographs\") #1839\nprint(OAnotes_df.head(10))\nOAnotes_df.to_csv(\"data/1_working_data/1_OAnotes01.csv\", encoding=\"utf-8\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"f_L0Nt8bOhrU","outputId":"40c00cfb-a824-4acc-d8b4-291b0ea5c407","trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Photographs which annotations have been transcribed in OA entries:  1839 (/over 3.222 photographs\n                                                 Inv  \\\n0                  Foto sup 748, verso: nota anonima   \n1      Foto sup 763, verso: nota anonima manoscritta   \n2      Foto sup 982, verso: nota anonima manoscritta   \n3  Foto sup 893, verso: nota anonima manoscritta:...   \n4      Foto sup 988, verso: nota anonima manoscritta   \n5      Foto sup 991, verso: nota anonima manoscritta   \n6      Foto sup 998, verso: nota manoscritta anonima   \n7     Foto sup 1000, verso: nota anonima manoscritta   \n8     Foto sup 1002, verso: nota anonima manoscritta   \n9     Foto sup 1003, verso: nota anonima manoscritta   \n\n                                                Note  \n0  Near Avezzano and not far from Tagliacozzo. He...  \n1  Aquila. S. Maria di Collemaggio. Founded by Pi...  \n2  The pulpit of San Giovanni del Toro, of the mi...  \n3                                               None  \n4  Here also his buried Sibylla of Burgundy. \"Rex...  \n5  Queen Margherita widow of Carlo III, who died ...  \n6  Piissimi Patris Nicolai Piscicelli optimi pres...  \n7  Rude sarcophagus in the porch of the church. T...  \n8  Amalfi. This campanile is said to date from 11...  \n9  Cloister of the Canonica founded in 1213 by Ca...  \n","output_type":"stream"}]},{"cell_type":"code","source":"#manual checking and adjusting for 1)\"manoscritta:\">\"manoscritta\",\" 2)\"\".\",>\"\".\" 3)\\n\",>\"\n#inv: 943, 75, 559, 1635, 1648, 1702, (1768 non riporta), 1787, 2397, 2789, 2869, 2849,\n# 2984, 2222,2270,2880, saved in data\\OAnotes02.csv\n\n#open the manually modified dataframe, search for unsuseful informations in 'Inv' and eliminate them\nOAnotes2_df = pd.read_csv('data/1_working_data/1_OAnotes02.csv', encoding=\"utf-8\").dropna(subset=['Inv']).reset_index(drop=True)\npattern = 'Foto |, (.+)'\nOAnotes2_df[\"Inv\"] = OAnotes2_df[\"Inv\"].replace(to_replace=pattern, value='', regex=True).reset_index(drop=True)\n\n#check and save the third version of OAnotes_df\nprint(OAnotes2_df.head(15))\nOAnotes2_df.to_csv(\"data/1_working_data/1_OAnotes03.csv\", encoding=\"utf-8\")\n\n#check how many of them are incomplete\nannotations_incompleted = OAnotes2_df[OAnotes2_df['Note'].str.contains(\"[...]\")== True].reset_index(drop=True)\nprint(\"Photographs which transcribed annotations are likely to be incomplete: \", annotations_incompleted.shape[0], \"(/over \",OAnotes_df.shape[0],\" transcribed)\")\n\n#create the corpus to be passed with spacy\ncorpus = \"\"\nfor OAnote in OAnotes2_df[\"Note\"]:\n    corpus = corpus+\"---\"+str(OAnote)+\"---\\n\"\nwith open(\"data/OAnotes_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n        f.write(corpus)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":276},"id":"tNX-1to7PfDw","outputId":"fb472629-61eb-4438-828b-8cd0b3848817","trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"    Unnamed: 0       Inv                                               Note\n0            0   sup 748  Near Avezzano and not far from Tagliacozzo. He...\n1            1   sup 763  Aquila. S. Maria di Collemaggio. Founded by Pi...\n2            2   sup 982  The pulpit of San Giovanni del Toro, of the mi...\n3            3   sup 893  Clara pudicicie dux Paulabianca potentis / A g...\n4            4   sup 988  Here also his buried Sibylla of Burgundy. \"Rex...\n5            5   sup 991  Queen Margherita widow of Carlo III, who died ...\n6            6   sup 998  Piissimi Patris Nicolai Piscicelli optimi pres...\n7            7  sup 1000  Rude sarcophagus in the porch of the church. T...\n8            8  sup 1002  Amalfi. This campanile is said to date from 11...\n9            9  sup 1003  Cloister of the Canonica founded in 1213 by Ca...\n10          10  sup 1012  In cloister of Amalfi Duomo. Sarc. of an archb...\n11          11  sup 1005  Amalfi. Cloister of San Francesco founded by t...\n12          12   sup 874  Urbino. / Palazzo Ducale, the old Montefeltro ...\n13          13   sup 882  Santa Casa. Loreto. / The Annunciation by the ...\n14          14  sup 1007  The great Oscan godders of maternity. This of ...\nPhotographs which transcribed annotations are likely to be incomplete:  1830 (/over  1839  transcribed)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2. Data elaboration\n\n\n","metadata":{"id":"WvgsBKu0cKpF"}},{"cell_type":"markdown","source":"1. Work on photographers","metadata":{"id":"66Ejy3lGkspx"}},{"cell_type":"code","source":"!pip install SPARQLWrapper\n!pip install geopy\nfrom csv import DictReader\nfrom SPARQLWrapper import SPARQLWrapper, JSON\nimport pandas as pd\nimport ssl\nfrom geopy.geocoders import Nominatim","metadata":{"id":"DRaKAR62kXIV","trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: SPARQLWrapper in /srv/conda/envs/notebook/lib/python3.7/site-packages (2.0.0)\nRequirement already satisfied: rdflib>=6.1.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from SPARQLWrapper) (6.2.0)\nRequirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.7/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (65.6.3)\nRequirement already satisfied: importlib-metadata in /srv/conda/envs/notebook/lib/python3.7/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (4.11.4)\nRequirement already satisfied: isodate in /srv/conda/envs/notebook/lib/python3.7/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (0.6.1)\nRequirement already satisfied: pyparsing in /srv/conda/envs/notebook/lib/python3.7/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata->rdflib>=6.1.1->SPARQLWrapper) (3.11.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata->rdflib>=6.1.1->SPARQLWrapper) (4.4.0)\nRequirement already satisfied: six in /srv/conda/envs/notebook/lib/python3.7/site-packages (from isodate->rdflib>=6.1.1->SPARQLWrapper) (1.16.0)\nCollecting geopy\n  Downloading geopy-2.3.0-py3-none-any.whl (119 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting geographiclib<3,>=1.52\n  Downloading geographiclib-2.0-py3-none-any.whl (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: geographiclib, geopy\nSuccessfully installed geographiclib-2.0 geopy-2.3.0\n","output_type":"stream"}]},{"cell_type":"code","source":"ssl._create_default_https_context = ssl._create_unverified_context\ngeolocator = Nominatim(timeout=10, user_agent=\"myGeolocator\")","metadata":{"id":"XuizLixzktEN","trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# functions\n# define a function to open file in reading mode\ndef process_csv(data_file_path):\n    import csv\n    source = open(data_file_path, mode=\"r\", encoding=\"UTF8\")\n    source_reader = csv.DictReader(source)\n    source_data = list(source_reader)\n    return source_data\n\n#define a function for transforming lists of elements in strings\ndef write_string(source, output_txt_name):\n    string = \"\"\n    for source_data in source:\n        string = string+source_data+\"|\"\n    string = string[:-1]\n    with open(output_txt_name, \"w\", encoding=\"utf-8\") as f:\n        f.write(string)\n    return string\n\n#define a function to query endpoints\ndef query_endpoint(endpoint_url, SPRQL_query):\n    get_endpoint = endpoint_url\n    sparql_w = SPARQLWrapper(get_endpoint)\n    sparql_w.setQuery(SPRQL_query)\n    sparql_w.setReturnFormat(JSON)\n    spqrl_w_res = sparql_w.query().convert()\n    return spqrl_w_res\n\n#define a function to manipulate results and have back 1. a set of wd_URI corresponding to our wd_names,\n# 2. update of ph_matrix, 3. not matched wd_names\n\ndef manipulate(spqrl_w_res, dataset_to_enhance):\n    res_dic = {}\n    res_NF_tem = set()\n    res_F = set()\n    for res in spqrl_w_res[\"results\"][\"bindings\"]:\n        for datum in dataset_to_enhance:\n            if datum[\"ph_wd_URI\"]:\n                continue\n            else:\n                if datum[\"ph_wd_name\"] not in res_dic:\n                    if res[\"fLabel\"][\"value\"] == datum[\"ph_wd_name\"]:\n                        res_F.add(res[\"f\"][\"value\"])\n                        new_pairs = {\"ph_wd_URI\": res[\"f\"][\"value\"]}\n                        res_dic.update({datum[\"ph_wd_name\"]: new_pairs})\n                        datum.update([(\"ph_wd_URI\", res[\"f\"][\"value\"])])\n                    else:\n                        res_NF_tem.add(datum[\"ph_wd_name\"])\n    res_NF_def = res_NF_tem - set(list(res_dic.keys()))\n    print(\"labels matched: \", len(res_F))\n    print(\"labels not found: \", len(res_NF_def))\n    return res_F, res_NF_def","metadata":{"id":"PDzPD39uj7pn","trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#import pandas as pd\n# open source data with pandas\ndata_df = pd.read_csv(\"data/F_OA_selected_data.csv\")\n\n#initialize a photograph's frequency dataframe\nph_freq = pd.DataFrame(data_df[\"AUFN_Faut\"].value_counts().reset_index().values, columns=[\"AUFN_Faut\", \"count\"])\n\n#extend dataframe colums to host next datas\nph_freq[\"ph_wd_name\"], ph_freq[\"ph_wd_URI\"], ph_freq[\"gender\"], ph_freq[\"workplace\"], ph_freq[\"lat\"], ph_freq[\"lon\"],\\\nph_freq[\"born\"], ph_freq[\"died\"], ph_freq[\"lat\"] = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]","metadata":{"id":"sz5v6r-4k7UO","trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#create the firts string for the SPARQL query by \n#normalizing (personal) names in form \"surname, name\" to \"name surname\" as in wikidata\n#and create a list of the modified names tobe added to the dataframe\n\nfirst_ph_names_string =\"\"\nph_wd_name_list = []\nfor ph in ph_freq.index:\n    ph_name = str(ph_freq[\"AUFN_Faut\"][ph])\n    # reverse only (personal) names in form \"surname, name\" > \"name surname\"\n    if \", \" in ph_name:\n        ph_split = ph_name.split(\", \")\n        ph_wd_name = ph_split[1] + \" \" + ph_split[0]\n    else:\n        ph_wd_name = ph_name\n    ph_wd_name_list.append(ph_wd_name)\n    first_ph_names_string = first_ph_names_string + ph_wd_name + \"|\"\nfirst_ph_names_string = first_ph_names_string[:-1]\n\n#show a sample of the string and save it\nprint(first_ph_names_string[0:200])\nwith open(\"data/1_working_data/2_PHstring01.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(first_ph_names_string)","metadata":{"id":"1XL0DDREpSb5","trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Anonimo|Fratelli Alinari|Romualdo Moscioni|Brogi|Giorgio Sommer|Jean Laurent|Incorpora|Giraudon|Paolo Lombardi|Naya|Carlo Baldassarre Simelli|Pietro Poppi|Séraphin-Médéric  Mieusement|Robert Rive|John\n","output_type":"stream"}]},{"cell_type":"code","source":"#add ph_wd_name_list to the dataframe and show a sample of the current dataframe\nph_freq[\"ph_wd_name\"] = ph_wd_name_list\nprint(ph_freq.head(10))\n#save the dataframe in a csv file and open it as a dictionary to iterate\nph_freq.to_csv(\"data/1_working_data/2_PH_freq_01.csv\", encoding=\"utf-8\")","metadata":{"id":"1XL0DDREpSb5","trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"            AUFN_Faut count         ph_wd_name ph_wd_URI gender workplace lat  \\\n0             Anonimo  1336            Anonimo                                  \n1   Alinari, Fratelli   556   Fratelli Alinari                                  \n2  Moscioni, Romualdo   159  Romualdo Moscioni                                  \n3               Brogi   158              Brogi                                  \n4     Sommer, Giorgio   147     Giorgio Sommer                                  \n5       Laurent, Jean    73       Jean Laurent                                  \n6           Incorpora    57          Incorpora                                  \n7            Giraudon    55           Giraudon                                  \n8     Lombardi, Paolo    53     Paolo Lombardi                                  \n9                Naya    48               Naya                                  \n\n  lon born died  \n0                \n1                \n2                \n3                \n4                \n5                \n6                \n7                \n8                \n9                \n","output_type":"stream"}]},{"cell_type":"code","source":"ph_matrix = process_csv(\"data/1_working_data/2_PH_freq_01.csv\")\nfirst_ph_names_string = open('data/1_working_data/2_PHstring01.txt', 'r', encoding=\"utf-8\").read()\n\n#prepare the first query string to collect wikidata URI\nfirst_ph_SPARQL_query = \"\"\"\nSELECT DISTINCT ?f ?fLabel\nWHERE\n{    { ?f wdt:P106 wd:Q33231 } UNION { ?f wdt:P31 wd:Q672070}. #P106_has_for_occupation wd:Q33231_photographer \n                                                                #P31_is instance wd:Q672070_studios\n    ?f rdfs:label ?fLabel.\n     FILTER regex(?fLabel, \\\" \"\"\"+first_ph_names_string+\"\"\" \\\")\n     FILTER(LANG(?fLabel) = \"en\").\n}\"\"\"\n\n#perform the first SPARQL query and result manipulation\nfirst_ph_wd_res = query_endpoint(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\", first_ph_SPARQL_query)\nfirst_ph_manipulate = manipulate(first_ph_wd_res, ph_matrix)\nfirst_F_set = first_ph_manipulate[0]\nfirst_NF = first_ph_manipulate[1]\n#check not found\nprint(first_NF)","metadata":{"id":"3z0IeTINsOwE","trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"labels matched:  45\nlabels not found:  67\n{'Robert MacPherson', 'Emilio Anriot', 'Neue Photographische Gesellschaft', 'Pascal Sébah', 'A. Dumaine', 'Stereoscopic Co.', 'Albert', 'Istituto Fotografico Antonio Fortunato Perini', 'Brogi', 'Abdullah Frères', 'Guillaume Gustave Berggren', 'Jean Giletta', 'J. Garrigues', 'Istituto Centrale per il Catalogo e la Documentazione: Fototeca Nazionale', 'Neurdein', 'Edith Emily Coulson James', 'Budtz Muller & Co.', 'Anonimo', 'Carl Prior Merlin', 'Peter Paul Mackey', 'Artistico ed Etnoantropologico e per il Polo Museale della città di Firenze Gabinetto Fotografico della Soprintendenza Speciale per il Patrimonio Storico', 'Clarke & Davies', 'Francesco Fiorani', 'Rozier', 'Vincenzo Paganori', 'Lombardi', 'Giovanni Battista Unterverger', 'Tagliarini', 'G. Stuffler', 'Zedler & Vogel', 'George Wilson Washington', 'Enrico Pezzani', 'Poulton Series', 'Incorpora', 'Dimitris Konstantinou', 'Pierre Henry  Voland', 'Fotografia A. Premi', 'Béchard Henri', 'Vasari', 'Séraphin-Médéric  Mieusement', 'Michele Petagna', 'Filippo Lais', 'Giuseppe Polozzi', 'Nicola De Mattia', 'Luigi Brillet-Buyet', 'Sommer & Behles', 'W. F. Mansell', 'Goupil & C.ie Editeurs', 'Fratelli Esposito', 'J. Kuhn', 'Johannes  Jaeger', 'Pere Pallejá Domenech', 'Fratelli Amodio', 'Lyon E.D.', 'William Lawrence', 'Studio Fotografico Ciappei', 'Giraudon', 'Tuminello Lodovico', 'Cesare Benvenuti', 'Bruckmann Verlag', 'Naya', 'Francisco Soler', 'Francesco De Rubeis', 'P. Famin & Cie.', 'Francesco Venturi', 'Paul des Granges', 'Alary & Geiser'}\n","output_type":"stream"}]},{"cell_type":"code","source":"#after revising first results, refine the unmatched labels\nnew_list = []\nfor ph_wd_NF in first_NF:\n    if \"  \" in ph_wd_NF:\n        ph_wd_new = ph_wd_NF.replace(\"  \", \" \") #cancel double spaces\n    elif \"Fratelli\" in ph_wd_NF:\n        ph_wd_new = ph_wd_NF.replace(\"Fratelli\", \"\") #cancel \"Fratelli\"\n    elif \"&\" in ph_wd_NF:\n        ph_wd_new = ph_wd_NF.replace(\"&\", \"and\") #change \"&\" in \"and\"\n    #check for corresponding form\n    elif \"Brogi\" == ph_wd_NF:\n        ph_wd_new = \"Giacomo Brogi\" \n    elif \"Incorpora\" == ph_wd_NF:\n        ph_wd_new = \"Giuseppe Incorpora\"\n    elif \"Giraudon\" == ph_wd_NF:\n        ph_wd_new = \"Adolphe Giraudon\"\n    else:\n        continue\n    new_list.append(ph_wd_new)\n    for ph_data in ph_matrix:\n        if ph_data[\"ph_wd_name\"] == ph_wd_NF:\n            ph_data.update([(\"ph_wd_name\", ph_wd_new)])\n\n#from the new modified names, by using the function, obtain a second string to query \nsecond_ph_string = write_string(new_list, \"data/1_working_data/2_PHstring02.txt\")\nprint(second_ph_string)","metadata":{"id":"PjIOKX7Yts7A","trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Giacomo Brogi|Budtz Muller and Co.|Clarke and Davies|Zedler and Vogel|Giuseppe Incorpora|Pierre Henry Voland|Séraphin-Médéric Mieusement|Sommer and Behles|Goupil and C.ie Editeurs| Esposito|Johannes Jaeger| Amodio|Adolphe Giraudon|P. Famin and Cie.|Alary and Geiser\n","output_type":"stream"}]},{"cell_type":"code","source":"#prepare the second query string to collect wikidata URI\nsecond_ph_SPARQL_query = \"\"\"\nSELECT DISTINCT ?f ?fLabel\nWHERE\n{    { ?f wdt:P106 wd:Q33231 } UNION { ?f wdt:P31 wd:Q672070}. #P106_has_for_occupation wd:Q33231_photographer \n                                                                #P31_is instance wd:Q672070_studios\n    ?f rdfs:label ?fLabel.\n     FILTER regex(?fLabel, \\\" \"\"\"+second_ph_string+\"\"\" \\\")\n     FILTER(LANG(?fLabel) = \"en\").\n}\n\"\"\"\n#perform the second SPARQL query and result manipulation\nsecond_ph_wd_res = query_endpoint(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\", second_ph_SPARQL_query)\nsecond_manipulate = manipulate(second_ph_wd_res, ph_matrix)\nsecond_F_set = second_manipulate[0]","metadata":{"id":"eZjQSnZsuRZX","trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"labels matched:  5\nlabels not found:  62\n","output_type":"stream"}]},{"cell_type":"code","source":"#obtain the list of found wikidata URI\ncomplex_F_set = second_F_set.union(first_F_set)\n\n#prepare the thirtd string to be passed in SPARQL query and save it\nthird_ph_string_URI =\"\"\nfor F_URI in complex_F_set:\n    third_ph_string_URI = third_ph_string_URI+\"<\"+F_URI+\">\"\n    \nwith open(\"data/1_working_data/2_PHstring03.txt\", \"w\", encoding=\"utf8\") as f:\n    f.write(third_ph_string_URI)\n\n#third query\nthird_ph_SPARQL_query = \"\"\"\nSELECT DISTINCT ?ph ?genderLabel ?countryLabel ?birthyear ?deathyear\n    WHERE\n    { VALUES ?ph {\"\"\"+third_ph_string_URI+\"\"\"} \n        ?ph rdfs:label ?phLabel;\n        wdt:P937 ?country; #P937_worklocation\n        #wdt:P27 ?citiz;        \n        wdt:P569 ?birth;\n        wdt:P570 ?death.\n        OPTIONAL {FILTER(LANG(?fLabel) = \"en\").\n                    ?ph wdt:P21 ?gender;\n                    #wdt:P937 ?worklocation; #P937_worklocation\n        }\n        BIND(year(?birth) AS ?birthyear)\n        BIND(year(?death) AS ?deathyear)\n\n        #BIND(COALESCE(?worklocation, ?citiz, \"NaN\") AS ?country).\n        #BIND(IF(BOUND(?worklocation),?worklocation,?citiz) AS ?country).\n    SERVICE wikibase:label {bd:serviceParam wikibase:language \"en\".}     \n    }\"\"\"\n#OPTIONAL { ?ph wdt:P569 ?birthdate;        wdt:P570 ?deathdate.} ci servono...\n\n#perform the third query\nthird_ph_wd_res = query_endpoint(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\", third_ph_SPARQL_query)\n\n#manipulate results\nwd_total_dic = {}\nfor result in third_ph_wd_res[\"results\"][\"bindings\"]:\n    item_key = result[\"ph\"][\"value\"]\n    item_value = {\"workplace\": result[\"countryLabel\"][\"value\"],\n                  \"lat\": geolocator.geocode(result[\"countryLabel\"][\"value\"]).latitude,\n                  \"lon\": geolocator.geocode(result[\"countryLabel\"][\"value\"]).longitude,\n                  \"born\": result[\"birthyear\"][\"value\"],\n                  \"died\": result[\"deathyear\"][\"value\"]}\n    if item_key not in wd_total_dic:\n        wd_total_dic.update({item_key: item_value})\n        for ph_data in ph_matrix:\n            if ph_data[\"ph_wd_URI\"] == item_key:\n                item2=item_value.items()\n                ph_data.update(item2)\n\n#wd_total_list = list(wd_total_dic.values())\n#print(wd_total_list)\n\n#save the enhanced matrix\nkeys = ph_matrix[0].keys()\nwith open(\"data/2_PHfreq.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as output_file:\n    dict_writer = csv.DictWriter(output_file, keys)\n    dict_writer.writeheader()\n    dict_writer.writerows(ph_matrix)\n\n#transform the enhanced matrix in a df and have a look at it\nph_freq_df = pd.DataFrame.from_dict(ph_matrix, orient='columns', dtype=None, columns=None)\nprint(ph_freq_df.head())","metadata":{"id":"RsSNGDE8v1py","trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"               AUFN_Faut count         ph_wd_name  \\\n0  0             Anonimo  1336            Anonimo   \n1  1   Alinari, Fratelli   556   Fratelli Alinari   \n2  2  Moscioni, Romualdo   159  Romualdo Moscioni   \n3  3               Brogi   158      Giacomo Brogi   \n4  4     Sommer, Giorgio   147     Giorgio Sommer   \n\n                                 ph_wd_URI gender workplace        lat  \\\n0                                                                        \n1   http://www.wikidata.org/entity/Q644689                               \n2  http://www.wikidata.org/entity/Q3441292             Rome   41.89332   \n3                                                                        \n4    http://www.wikidata.org/entity/Q64212           Naples  40.835885   \n\n         lon  born  died  \n0                         \n1                         \n2  12.482932  1849  1925  \n3                         \n4  14.248768  1834  1914  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"2. Work on places","metadata":{"id":"0xSgal3tQruP"}},{"cell_type":"code","source":"#define function to store lat-lon from a list of places\ndef get_coordinates(list, df):\n    check = df[\"place\"].unique().tolist()\n    for place in list:\n        if place not in check:\n            if geolocator.geocode(place) != None:\n                lon = geolocator.geocode(place).longitude\n                lat = geolocator.geocode(place).latitude\n            else:\n                lon = \"NaN\"\n                lat = \"NaN\"\n            new_place = [str(place), lat, lon]\n            df.loc[len(df)] = new_place\n            check = df[\"place\"].unique().tolist()\n        else:\n            continue\n    df.to_csv(\"data/3_PLcoordinates.csv\", encoding=\"UTF-8\")","metadata":{"id":"Z_E9BD3XQrSn","trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#open the saved file, reduce columns and change column name, check first rows\nph_freq_df = pd.read_csv(\"data/2_PHfreq.csv\", encoding=\"UTF-8\")\nplaces_F = ph_freq_df[['workplace', \"lat\", \"lon\"]].dropna()\nplaces_F = places_F.rename(columns={\"workplace\": \"place\"}).sort_values(by=['place']).reset_index(drop=True)\nplaces_F.head()\n\n#extract towns and country unique names from original dataframe\ndata_df = pd.read_csv(\"data/F_OA_selected_data.csv\", encoding=\"UTF-8\")\ntowns_OA = data_df['PVCC_OAtown'].unique().tolist()\ncountries_OA = data_df['PVCS_OAcountry'].unique().tolist()\n\n#obtain coordinates from the town list and store them in a df\nget_coordinates(towns_OA, places_F)","metadata":{"id":"Lnd5nBkERNH5","trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"3. Work on Annotations","metadata":{"id":"8ZRD3mhvZ1z3"}},{"cell_type":"code","source":"!pip install spacy\n!python -m spacy download en_core_web_sm\n!python -m spacy download xx_ent_wiki_sm\nimport spacy\nfrom spacy.matcher import Matcher\nfrom spacy.attrs import POS\n#import pandas as pd","metadata":{"id":"3qzXjNVGbnY6","trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Collecting spacy\n  Downloading spacy-3.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\nRequirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (65.6.3)\nCollecting smart-open<7.0.0,>=5.2.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\nCollecting typer<0.8.0,>=0.3.0\n  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\nCollecting tqdm<5.0.0,>=4.38.0\n  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n  Downloading srsly-2.4.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.0/490.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n  Downloading murmurhash-1.0.9-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\nCollecting cymem<2.1.0,>=2.0.2\n  Downloading cymem-2.0.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\nCollecting pathy>=0.10.0\n  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n  Downloading preshed-3.0.8-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (126 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.6/126.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (4.4.0)\nCollecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n  Downloading pydantic-1.10.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (22.0)\nCollecting spacy-legacy<3.1.0,>=3.0.11\n  Downloading spacy_legacy-3.0.11-py2.py3-none-any.whl (24 kB)\nRequirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (3.1.2)\nCollecting catalogue<2.1.0,>=2.0.6\n  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (2.28.1)\nCollecting langcodes<4.0.0,>=3.2.0\n  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting thinc<8.2.0,>=8.1.0\n  Downloading thinc-8.1.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (815 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.9/815.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (1.21.6)\nRequirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.11.0)\nRequirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.13)\nRequirement already satisfied: charset-normalizer<3,>=2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\nCollecting blis<0.8.0,>=0.7.8\n  Downloading blis-0.7.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\nCollecting click<9.0.0,>=7.1.1\n  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from jinja2->spacy) (2.1.1)\nRequirement already satisfied: importlib-metadata in /srv/conda/envs/notebook/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy) (4.11.4)\nInstalling collected packages: cymem, wasabi, tqdm, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, catalogue, blis, srsly, preshed, click, typer, confection, thinc, pathy, spacy\nSuccessfully installed blis-0.7.9 catalogue-2.0.8 click-8.1.3 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.4 smart-open-6.3.0 spacy-3.5.0 spacy-legacy-3.0.11 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.7 tqdm-4.64.1 typer-0.7.0 wasabi-1.1.1\nCollecting en-core-web-sm==3.5.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from en-core-web-sm==3.5.0) (3.5.0)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.5)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\nRequirement already satisfied: pathy>=0.10.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\nRequirement already satisfied: numpy>=1.15.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.21.6)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.4)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\nRequirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\nRequirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.11)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\nRequirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (22.0)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\nRequirement already satisfied: typer<0.8.0,>=0.3.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\nRequirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\nRequirement already satisfied: thinc<8.2.0,>=8.1.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.7)\nRequirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.11.0)\nRequirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.13)\nRequirement already satisfied: charset-normalizer<3,>=2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\nRequirement already satisfied: importlib-metadata in /srv/conda/envs/notebook/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.11.4)\nInstalling collected packages: en-core-web-sm\nSuccessfully installed en-core-web-sm-3.5.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\nCollecting xx-ent-wiki-sm==3.5.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-3.5.0/xx_ent_wiki_sm-3.5.0-py3-none-any.whl (11.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from xx-ent-wiki-sm==3.5.0) (3.5.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (3.0.11)\nRequirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (3.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.4.5)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (3.3.0)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.0.9)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.1.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.0.8)\nRequirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (4.4.0)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.28.1)\nRequirement already satisfied: numpy>=1.15.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.21.6)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.0.4)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.10.4)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (3.0.8)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (4.64.1)\nRequirement already satisfied: pathy>=0.10.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (0.10.1)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (6.3.0)\nRequirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (65.6.3)\nRequirement already satisfied: typer<0.8.0,>=0.3.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (0.7.0)\nRequirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (22.0)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.0.7)\nRequirement already satisfied: thinc<8.2.0,>=8.1.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (8.1.7)\nRequirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (3.11.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (1.26.13)\nRequirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2022.12.7)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (0.7.9)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (0.0.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (2.1.1)\nRequirement already satisfied: importlib-metadata in /srv/conda/envs/notebook/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->xx-ent-wiki-sm==3.5.0) (4.11.4)\nInstalling collected packages: xx-ent-wiki-sm\nSuccessfully installed xx-ent-wiki-sm-3.5.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('xx_ent_wiki_sm')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2.3.1 Work on annotations - Find for personal experiences","metadata":{"id":"sxPYRxMacLDt"}},{"cell_type":"code","source":"#open the file with annotations texts\nwith open(\"data/OAnotes_corpus.txt\", mode=\"r\") as f:\n    contents = f.read()\n\n#load the model\nnlp = spacy.load(\"en_core_web_sm\")\n\n#look for POS combination I(lemma)+verb and have a look to the list\nmatcher = Matcher(nlp.vocab)\nmatcher.add(\"PA_creator\", [[{\"LEMMA\": \"I\"}, {POS: 'VERB'}]])\ndoc = nlp(contents)\nmatches = matcher(doc)\n\nmatched = []\nfor match_id,start,end in matches:\n    I_verb = str(doc[start:end])\n    matched.append(I_verb)\n#print(matched)\n\n#have a look to the occurrencies\nmatched_df = pd.DataFrame()\nmatched_df[\"Match\"] = matched\nmatched_freq = pd.DataFrame(matched_df[\"Match\"].value_counts().reset_index().values, columns=[\"Match\", \"count\"])\nprint(matched_freq.head(25))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"cOWvR2Z3Z62_","outputId":"c55a54a4-08ee-4072-8021-4bad942ea9ce","trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"           Match count\n0         I have     7\n1        I think     4\n2          I saw     4\n3        I doubt     2\n4         I page     2\n5    I respected     1\n6          I put     1\n7      I believe     1\n8        i corpi     1\n9        me look     1\n10        i farn     1\n11            Im     1\n12        I told     1\n13  I discovered     1\n14    i ritratti     1\n15          I et     1\n16     me posuit     1\n17        i suoi     1\n18     i diritti     1\n","output_type":"stream"}]},{"cell_type":"code","source":"#select the not pertinent items to be removed\nitems_to_remove = {'I page', 'I et', 'i ritratti', 'Im', 'i suoi', 'i cittadini', 'i migliori', 'i farn', 'i diritti'}\n\n#obtain a pertinent set of occurrencies and a string to search for it in the Notes df\npertinent_matched_set = set(matched_freq[\"Match\"])-items_to_remove\n#print(pertinent_matched_set)\n\nmatched_list = \"\"\nfor item in pertinent_matched_set:\n    matched_list = matched_list+item + \"|\"\nmatched_list = matched_list[:-1]\n\nprint(\"Personal experiences (I+verbs) combinations in Notes: \", matched_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#filter the annotations dataframe if item in matched list is present and save them\nOA_data = pd.read_csv(\"data/1_working_data/1_OAnotes03.csv\")\npersonal_experiences_df = OA_data[OA_data[\"Note\"].str.contains(matched_list)==True].reset_index(drop=True)\npersonal_experiences_df.to_csv(\"data/1_working_data/1_OAnotes04_Iverb.csv\", encoding=\"UTF-8\")\n        \nprint(\"Number of personal experiences (I+verbs) combinations in Notes: \", personal_experiences_df.shape[0])\nprint(personal_experiences_df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save the restricted corpus\npersonal_experiences_df = pd.read_csv(\"data/1_working_data/1_OAnotes04_Iverb.csv\", encoding=\"UTF-8\")\nrestricted_corpus = \"\"\nfor OAnote in personal_experiences_df[\"Note\"]:\n    restricted_corpus = restricted_corpus+\"---\"+str(OAnote)+\"---\\n\"\nwith open(\"data\\OAnotes_corpus2.txt\", \"w\", encoding=\"utf8\") as file:\n        file.write(restricted_corpus)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.3.2 Work on annotations - Find for dated and located personal experiences","metadata":{"id":"sxPYRxMacLDt"}},{"cell_type":"code","source":"#filter the _personal_experiences_df if a contemporary date (\"18[4-9]\\d{1}\") is present\ndated_personal_experiences_df = pd.read_csv(\"data/1_working_data/1_OAnotes04_Iverb.csv\", encoding=\"utf8\")\ndated_personal_experiences_df = personal_experiences_df[personal_experiences_df[\"Note\"].str.contains(\"18[4-9]\\d{1}\") == True].reset_index(drop=True)\ndated_personal_experiences_df.to_csv(\"data/1_working_data/1_OAnotes05_Iverb_data.csv\", encoding=\"UTF-8\")\nprint(\"Number of personal experiences (I+verbs) combinations in Notes: \", dated_personal_experiences_df.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#manual extraction of time-place pairs saved in \"data/1_working_data/6_Time-place_visited.csv\"\nvisited_places_df = pd.read_csv(\"data/1_working_data/6_Time-place_visited.csv\", encoding=\"utf8\")\nprint(visited_places_df.head())\n\n#launch get_coordinates function for new named places","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.3. Work on annotations - NER exploration","metadata":{"id":"sxPYRxMacLDt"}},{"cell_type":"code","source":"import pandas as pd\n#define a function to perform NER on strings and obtain, visualize and save a df with count for entity\ndef get_entities(source_file_path, model, LABEL, path):\n    with open(source_file_path, mode=\"r\") as file:\n        contents = file.read()\n    NER = spacy.load(model)  # possible models for our Notes: it_core_news_md -- | xx_ent_wiki_sm | -- en_core_web_sm\n    parsed = NER(contents)\n    ent_count = dict()\n    ent_list = []\n    for ent in parsed.ents:\n        if ent.label_ == LABEL:\n            ent_str = str(ent)\n            ent_list.append(ent_str)\n    for ent_str in ent_list:\n        if ent_str not in ent_count:\n            ent_count.update({ent_str: ent_list.count(ent_str)})\n    #entities_df = pd.DataFrame.from_dict(ent_count, orient=\"index\", columns=[\"count\"])\n    ent_df = pd.DataFrame(ent_list, columns=[\"ent\"])\n    ent_freq = pd.DataFrame(ent_df[\"ent\"].value_counts().reset_index().values, columns=[\"ent\", \"count\"])\n    #print(ent_freq.sort_values(by=\"count\", ascending=False).head(35))\n    ent_freq.to_csv(path, encoding=\"UTF-8\")\n    return ent_freq\n\n#explore some NER extractions\nPERSON = get_entities(\"data/OAnotes_corpus.txt\", \"en_core_web_sm\", \"PERSON\", \"data/1_working_data/4_OAentities0_PERSON.csv\")\nPER = get_entities(\"data/OAnotes_corpus.txt\", \"xx_ent_wiki_sm\", \"PER\", \"data/1_working_data/4_OAentities01_PER.csv\")\nprint(\"Results for en_core_web_sm model: \")\nprint(PERSON.head(10))\nprint(\"Results for xx_ent_wiki_sm model: \")\nprint(PER.head(10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The \"xx_ent_wiki_sm\" model seems to give back more pertinent results for our corpus compared to\n# the \"en_core_web_sm\" model so, the rest of extractions when possible ('PER' 'ORG' 'misc' 'LOC')\n# will be done on that one\nget_entities(\"data/OAnotes_corpus2.txt\", \"xx_ent_wiki_sm\", \"PER\", \"data/1_working_data/4_OAentities01_PER2.csv\")\nget_entities(\"data/OAnotes_corpus.txt\", \"xx_ent_wiki_sm\", \"LOC\", \"data/1_working_data/4_OAentities02_LOC.csv\")\nget_entities(\"data/OAnotes_corpus2.txt\", \"xx_ent_wiki_sm\", \"LOC\", \"data/1_working_data/4_OAentities02_LOC2.csv\")\nget_entities(\"data/OAnotes_corpus.txt\", \"xx_ent_wiki_sm\", \"ORG\", \"data/1_working_data/4_OAentities03_ORG.csv\")\nget_entities(\"data/OAnotes_corpus2.txt\", \"xx_ent_wiki_sm\", \"ORG\", \"data/1_working_data/4_OAentities03_ORG2.csv\")\n\n#have a look to other NER extraction  #CARDINAL, DATE, EVENT, FAC, GPE, LANGUAGE, LAW, LOC, MONEY, NORP, ORDINAL,\n# ORG, PERCENT, PERSON, PRODUCT, QUANTITY, TIME, WORK_OF_ART\nget_entities(\"data/OAnotes_corpus.txt\", \"en_core_web_sm\", \"DATE\", \"data/1_working_data/4_OAentities04_DATE.csv\")\nget_entities(\"data/OAnotes_corpus2.txt\", \"en_core_web_sm\", \"DATE\", \"data/1_working_data/4_OAentities04_DATE2.csv\")\nget_entities(\"data/OAnotes_corpus.txt\", \"en_core_web_sm\", \"WORK_OF_ART\", \"data/1_working_data/4_OAentities05_OA.csv\")\n\n#get_entities(\"data/OAnotes_corpus.txt\", \"en_core_web_sm\", \"LANGUAGE\", \"data/1_working_data/4_0Aentities06_LANGUAGE.csv\")\n#get_entities(\"data/OAnotes_corpus.txt\", \"en_core_web_sm\", \"MONEY\", \"data/1_working_data/4_OAentities07_MONEY.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data visualization","metadata":{"id":"sxPYRxMacLDt"}},{"cell_type":"markdown","source":"\n---\n**Analyse**\npandas library in order to examine our data.\n     \n       \n    1. Data Preparation:\n          - creation of two complexive xml files for F and OA records coming from the Federico Zeri Foundation catalogues\n          - extraction from nested xml stucture of relevant information for the project and structuring them in plain tabular format\n    2. Data Elaboration: seeking for furter analysis elements via:\n          - deeper work on photographer for enhance their information\n          - deeper work on places\n          - work on unstructured annotations: NER\n     2. Data Visualization\n","metadata":{"id":"rSDRDeMOjB8S"}},{"cell_type":"markdown","source":"0. Data overview","metadata":{"id":"7U57aNxDdeOH"}},{"cell_type":"code","source":"import pandas as pd\nimport pandas_profiling as pp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pprint\npp = pprint.PrettyPrinter(indent=1)","metadata":{"id":"pgU6VKzNdtTp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# parse the csv into a dataframe\ndata_df = pd.read_csv('data\\F_OA_selected_data.csv')\n\n# reduce the dataset to just the columns needed\ndata_df = data_df[['INVN_F', 'PVCS_OAcountry', 'PVCC_OAtown', 'LDCN_OArep', 'PRVC_OAprev_town',\n          'AUFN_Faut', 'LRD_Fdates', 'OGTT_OAtype', 'AUTB_Fsubj_main', 'OGTDOA_OAsubj_sub', 'AUTN_OAaut']]\nprint(data_df.head(15))","metadata":{"id":"Kk-RBvonXIBI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import pandas_profiling as pp\nreport = pp.ProfileReport(data_df, title=\"Partizione Antica Fund - overview\")\nreport.to_file(\"data/2_data_viz/0.1.html\")","metadata":{"id":"CDgOvQUheO2B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169},"id":"7Hb6VMBkf7LJ","outputId":"8a7d3687-1578-40fe-99da-12a242f965f6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1.1 Works of art - typology proportions pie","metadata":{"id":"JtgJeWIngJcr"}},{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px\n\n# filter cities and number of photos from data\ndata = pd.read_csv(\"data/F_OA_selected_data.csv\", encoding=\"UTF-8\")\nOAt_df = pd.DataFrame(data[\"OGTT_OAtype\"].value_counts().reset_index().values, columns=[\"OGTT_OAtype\", \"count\"])\n\nfig = px.pie(OAt_df, values='count', names=\"OGTT_OAtype\",\n            title='OA typologies',\n            color_discrete_sequence=px.colors.sequential.RdBu,\n            labels = OAt_df['OGTT_OAtype'], hover_name = 'OGTT_OAtype',\n            hover_data = {'OGTT_OAtype':False}\n            )\nfig.show()\nfig.write_html(\"data/2_data_viz/1.1.html\")","metadata":{"id":"saYsZpZ2gI6x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1.2 Works of art - countries proportions pie\n\n","metadata":{"id":"7rauFSANgaYi"}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nimport plotly\nimport plotly.graph_objects as go\nimport csv\n\n# filter cities and number of photos from data\ndata = pd.read_csv(\"UNIQUE_new2.csv\", encoding=\"utf8\")\nsup_df = data[['PVCS_OAcountry', 'PVCC_OAtown', 'LDCN_OArep']]\n\n#create a pie chart of supposed OA country\ndf_data = pd.read_csv(\"OAcountry_freq.csv\", encoding=\"utf8\")\ndf_data.loc[df_data['count'] < 10, 'PVCS_OAcountry'] = 'Other countries' # Represent only large countries\n#df = px.df_data()\nfig = px.pie(df_data, values='count', names=\"PVCS_OAcountry\",\n            title='Depicted OA for country',\n            color_discrete_sequence=px.colors.sequential.RdBu,\n            labels = df_data['PVCS_OAcountry'], hover_name = 'PVCS_OAcountry',\n            hover_data = {'PVCS_OAcountry':False, 'lat':False, 'lon': False}\n            )\nfig.show()\nfig.write_html(\"data/2_data_viz/1.2.html\")","metadata":{"id":"g_EONtm7gaIE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1.3 Works of art - countries proportions map","metadata":{"id":"nYz-E7P8g7fc"}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom geopy.geocoders import Nominatim\ngeolocator = Nominatim(timeout=10, user_agent = \"myGeolocator\")\n\nimport plotly\nimport plotly.graph_objects as go\nimport csv\n\n# filter cities and number of photos from data\ndf_data = pd.read_csv(\"OAcountry_freq.csv\", encoding=\"utf8\")\nph_geol = px.scatter_mapbox(df_data, lon=df_data['lon'],\n                            lat=df_data['lat'], size=df_data[\"count\"], zoom=2, color=df_data['PVCS_OAcountry'],\n                            color_continuous_scale=px.colors.cyclical.Twilight,\n                            #color_discrete_sequence=px.colors.sequential.RdBu,\n                            title=\"Depicted OA\",\n                            size_max=80,\n                            labels=df_data['PVCS_OAcountry'], hover_name='PVCS_OAcountry',\n                            hover_data={'PVCS_OAcountry':False, 'lat':False, 'lon':False})\n\n# mapbox style\nph_geol.update_layout(mapbox_style='carto-positron')\nph_geol.show()\nph_geol.write_html(\"data/2_data_viz/1.3.html\")","metadata":{"id":"Rea-Mj6qg3II"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.1 Photograps - photographers proportions pie /distribution barchart","metadata":{"id":"iifJu-PthWVN"}},{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px\n\ndf_data2 = pd.read_csv(\"ph_newfreq.csv\", encoding=\"utf8\")\ndf_data2 = df_data2[df_data2[\"count\"]>=20]\n#df_data2.loc[df_data2['count'] < 10, 'AUFN_Faut'] = 'Other photographs' # Represent only large countries\n#df = px.df_data()\nfig2 = px.pie(df_data2, values='count', names=\"AUFN_Faut\",\n            title='Photographs (>=20) for photographer',\n            color_discrete_sequence=px.colors.qualitative.Dark24, #color_discrete_sequence/color_continuous_scale =px.colors.sequential.RdBu,\n            labels = df_data2['AUFN_Faut'], hover_name='AUFN_Faut',\n            hover_data = {'AUFN_Faut':False, 'workplace':True}\n            #sistema le caselle non piene di ph_freq etc penso fillna()\n            )\nfig2.show()\nfig2.write_html(\"data/2_data_viz/2.1.html\")","metadata":{"id":"RrH1jF4rhWHG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.2 Photographs - map distribution based on ateliers locations","metadata":{"id":"pHJdaH1PiEYU"}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n#to show up everything directly in jupyter notebook\n\nimport plotly\nimport plotly.graph_objects as go\nimport csv\n\n#open the whole dataset and select just the AUFN_FAUT column\ndata = pd.read_csv(\"data\\F_OA_selected_data.csv\", encoding=\"UTF-8\")\ndata_d = data[[\"AUFN_Faut\"]]\nprint(len(data_d))\n\n#open the ph_freq dataset and select just the AUFN_FAUT and the workplace column; create a second df on workplace lat and lon\ndata_ph = pd.read_csv(\"data\\ph_freq.csv\", encoding=\"UTF-8\")\ndata_ph_df = data_ph[[\"AUFN_Faut\", \"workplace\"]]\ndata_lat_lon = data_ph[[\"workplace\", \"lat\", \"lon\"]].drop_duplicates(subset=['workplace'])\n\n#merge the two dataset on the AUFN_Faut column\ndata_comp = data_d.merge(data_ph_df, how='left', on=\"AUFN_Faut\").reset_index(drop=True)\nprint(len(data_comp))\nprint(data_comp.head(25))\n\n#count the values on workplace column\ndata_count = pd.DataFrame(data_comp['workplace'].value_counts().reset_index().values, columns=['workplace', 'count'])\nprint(len(data_count))\nprint(data_count.head(25))\n\n#add lat and lon data to the previous df\ndata_df = data_count.merge(data_lat_lon, how='left', on=\"workplace\").reset_index(drop=True)\ndata_df['count'] = pd.to_numeric(data_df['count'])\nprint(len(data_df))\nprint(data_df.head(25))\n\nph_geol = px.scatter_mapbox(data_df, lon=data_df['lon'], lat=data_df['lat'],\n                            size=data_df['count'], zoom=3, color=data_df['workplace'],\n                            size_max=80,\n                            color_discrete_sequence=px.colors.sequential.RdBu,\n                            title=\"Photohgrapher ateliers\",\n                            labels=data_df['workplace'], hover_name=\"workplace\")\n\n# mapbox style\nph_geol.update_layout(mapbox_style='carto-positron')\nph_geol.show()\nph_geol.write_html(\"data/2_data_viz/2.2.html\")","metadata":{"id":"t3ndEp0MiEiX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.3 Photographs - map distribution of anonimous photographs based on place of shooting (limited to immobles)","metadata":{"id":"nfzogXtzioNa"}},{"cell_type":"code","source":"#!pip install regex\nimport pandas as pd\nimport plotly.express as px\nimport regex as re\n\ndata = pd.read_csv(\"UNIQUE_new2.csv\", encoding=\"UTF-8\")\n\n#reduce to only columns rows needed and check\nsup_df = data[['OGTT_OAtype', 'PVCS_OAcountry', 'PVCC_OAtown', 'AUFN_Faut']]\nsup_df = sup_df[sup_df.AUFN_Faut == \"Anonimo\"]\n\ndf_data3 = pd.DataFrame(sup_df[\"PVCS_OAcountry\"].value_counts().reset_index().values, columns=[\"PVCS_OAcountry\", \"count\"])\n\nfig3 = px.pie(df_data3, values='count', names=\"PVCS_OAcountry\",\n              title='Anonimous Photographs (1331/3111) for OAcountry',\n              color_discrete_sequence=px.colors.sequential.Brwnyl,\n              labels = df_data3['PVCS_OAcountry'], hover_name='PVCS_OAcountry',\n              hover_data = {'PVCS_OAcountry':False}\n              )\n\nfig3.update_layout(\n    font = dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        )\n    )\nfig3.show()\n\nsup_df= sup_df[sup_df['OGTT_OAtype'].str.contains(\"architettura|architettura\\ scultura|complesso archeologico|sito archeologico\")== True].reset_index(drop=True)\nsup_df.to_csv(\"daje.csv\")\nprint(sup_df.tail(200))\ndf_data4 = pd.DataFrame(sup_df[\"PVCS_OAcountry\"].value_counts().reset_index().values, columns=[\"PVCS_OAcountry\", \"count\"])\nfig4 = px.pie(df_data4, values='count', names=\"PVCS_OAcountry\",\n              title='Anonimous Photographs (1331/3111) for OAcountry of immobles',\n              color_discrete_sequence=px.colors.sequential.Brwnyl, #px.colors.sequential.RdBu https://plotly.com/python/discrete-color/\n              labels = df_data3['PVCS_OAcountry'], hover_name='PVCS_OAcountry',\n              hover_data = {'PVCS_OAcountry':False}  #sistema le caselle non piene di ph_freq etc penso fillna()\n              )\n\nfig4.update_layout(\n    font = dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        )\n    )\nfig4.show()\nfig4.write_html(\"data/2_data_viz/2.3.html\")","metadata":{"id":"VxUk_S0BioDO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3.1 Annotations - complete/incomplete/missing transcriptions proportions pie","metadata":{"id":"RVfqj_usjf1s"}},{"cell_type":"code","source":"import plotly.express as px\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n#open the needed dataframes\ndata_df = pd.read_csv('data\\F_OA_selected_data.csv')\nall_inv = data_df[[\"INVN_F\"]].rename(columns={\"INVN_F\": \"Inv\"}).reset_index(drop=True)\nOAnotes_df = pd.read_csv('data/1_working_data/OAnotes03.csv')\n\n#define a function to check status of transcriptions\ndef check(row):\n    if \"...\" in str(row[\"Note\"]):\n        status = \"incomplete\"\n    else:\n        status = \"complete\"\n    return status\n\n#apply the function to df and add a status column, check it \nOAnotes_df[\"status\"] = OAnotes_df.apply(check, axis=1)\nprint(OAnotes_df.head(25))\n\n#merge the df with all the inventories to check for missing transcriptions and count according to the status \nmerged = all_inv.merge(OAnotes_df, how='left', on=\"Inv\").reset_index(drop=True)\nnew=pd.DataFrame(merged[\"status\"].value_counts(dropna=False).reset_index().values, columns=[\"status\", \"count\"])\n#change the empty rows with \"missing\" and check\nnew['status'] = new['status'].fillna('missing')\nprint(new.tail(50))\n\nfig = px.pie(new, values='count', names=\"status\",\n              title='Annotations on photographs',\n              color_discrete_sequence=px.colors.sequential.Brwnyl, #px.colors.sequential.RdBu https://plotly.com/python/discrete-color/\n              labels = new['status'], hover_name='status',\n              hover_data = {'status':True}  #sistema le caselle non piene di ph_freq etc penso fillna())\n             )\n\nfig.update_layout(\n    font = dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        )\n    )\nfig.show()\nfig.write_html(\"data/2_data_viz/3.1.html\")","metadata":{"id":"g634EN9LjfuH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3.2 Annotations","metadata":{"id":"-X5aqyMQlSii"}},{"cell_type":"code","source":"","metadata":{"id":"1ldFH4F4liEi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3.3 Annotations and metadata: compared dates distribution","metadata":{"id":"gEWFNsy5liLo"}},{"cell_type":"code","source":"","metadata":{"id":"BD4odwiSluHA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4.1 Annotations - time-place pairs related to ...","metadata":{"id":"qI4y50zSluSH"}},{"cell_type":"code","source":"import plotly.express as px\nimport pandas as pd\n\n#open and merge data from annotations reporting time-place pairs and place coordinates, check for it\nmovement_df = pd.read_csv(\"data/1_working_data/OAnotes05bis.csv\", encoding=\"utf8\")\nplaces_df = pd.read_csv(\"data/1_working_data/places_coordinates2.csv\", encoding=\"utf8\")\nmovement_df_coor = movement_df.merge(places_df, how='left', on=\"place\")\nmovement_df_coor.drop('Unnamed: 0', axis=1, inplace=True)\nprint(movement_df_coor.head(10))\n\n#break the texts at about 30 characters to let them better visualized\nmovement_df_coor['Note_br'] = movement_df_coor.apply(lambda row: ('<br>'.join(str(row.Note)[i:i+30] for i in range(0, len(str(row.Note)), 30))), axis = 1)\n\n#set scatter with d\nfig = px.scatter_geo(movement_df_coor, color=\"date\",\n                  lat=movement_df_coor[\"lat\"].values.tolist(),\n                  lon=movement_df_coor[\"lon\"].values.tolist(),\n                  title=\"Movements\", size=\"date\",\n                  projection=\"natural earth\", scope=\"europe\",\n            labels = movement_df_coor['place'], hover_name='place',\n            hover_data = {'place':False, 'Inv':True, 'Note_br':True}\n            )\nfig.show()\nfig.write_html(\"data/2_data_viz/4.1.html\")","metadata":{"id":"2x3WJ6drlSYT"},"execution_count":null,"outputs":[]}]}