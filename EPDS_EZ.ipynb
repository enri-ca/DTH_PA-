{"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#Intro\n\nThis Jupyter Notebook has been created for the <a href=\"https://www.unibo.it/it/didattica/insegnamenti/insegnamento/2021/443749\" target=\"_blank\">90154 - Electronic Publishing and Digital Storytelling</a> course, taught by **Prof. Marilena Daquino**, in the framework of the 2nd year of the <a href=\"https://corsi.unibo.it/2cycle/DigitalHumanitiesKnowledge\" target=\"_blank\">DHDK Master Degree</a>, a.a. 2021-22.<br>\nHere listed the main steps for the realization of the project **Partizione Antica**:     \n       \n    1. Data Preparation:\n          - creation of two complexive xml files for F and OA records coming from the Federico Zeri Foundation catalogues\n          - extraction of relevant information for the project from nested xml stucture and structuring them in plain tabular format\n          - extraction from previpus tabular data of unstructured annotations\n    2. Data Elaboration: seeking for furter analysis elements via:\n          - deeper work on photographer for enhancing their information (workplace, timespan of activity, etc.)\n          - deeper work on places for enhancing their gelocation\n          - work on unstructured annotations trough NLP and NER\n    3. Data Visualization\n    - \n    -\n    -","metadata":{"id":"D76tTDYyH-z9"}},{"cell_type":"markdown","source":"# 1. Data preparation\n\nThis research started from a **record data extraction** of the Supino Partizione Antica fund provided from the Federico Zeri Foundation: the original data counted 3.260 records for photographs and 2.634 records for depicted works of art. <br>\nThe original data have been used for **illustrative and didactical purposes only**: all the credits and reuse authorizations must be asked to <a href=\"mailto:fondazionezeri.fototeca@unibo.it\">Federico Zeri Foundation</a>.\n\n**1.1 Creation of the F and OA complexive xml files**\n\nTo allow a better management and manipulation, as well as to anonymize personal data, complexive files (via <a href=\"/content/sample_data/0_Creation_UniqeXML.xquery\" target=\"_blank\">0_Creation_UniqeXML.xquery</a> collection command) have been created and published. \nThey collect:\n\n*   all the single photograph xml files' records in the F_entries.xml file (data/0_source_data)\n*   all the single works of art xml files' records in the OA_entries.xml file (data/0_source_data)\n\n**1.2 Creation of the flat tabular dataset extracting relevant information for the project from the nested xml elements and attributes**\n\nDue to the hypernested and not consistently presence of elements at different levels, <pandas.read_xml> method was not effectively parsing what was needed.\nThe <xml.etree.ElementTree> library has then been preferred because it allows to call for single elements at different nesting levels. Nevertheless, this approach presents some drawbacks as the need of a previous and deep knowledge of the database structure that does not allow to uncover unexpected correlations possible through the exploration of a comprehensive dataset.\n\n**1.3 Preliminary installation**(Uncomment the first line to install the library)\n- libraries\n- imports:\n  - xml.etree.ElementTree, pandas, csv for managing the dataset\n  - ...\n","metadata":{"id":"TYn9-Vu-cI03"}},{"cell_type":"code","source":"#preliminary imports\n!pip install python-csv\n!pip install elementpath\nimport csv\nimport xml.etree.ElementTree as ET","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"4ZW8b0GdYl6l","outputId":"ffd5c978-2c92-4c67-8174-b98bfd519316","trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Collecting python-csv\n  Using cached python-csv-0.0.13.tar.gz (26 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting argparse\n  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\nRequirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-csv) (1.21.6)\nRequirement already satisfied: pandas in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-csv) (1.3.5)\nCollecting matplotlib\n  Using cached matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\nCollecting xlrd\n  Using cached xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\nCollecting xmltodict\n  Using cached xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\nCollecting demjson\n  Using cached demjson-2.2.4.tar.gz (131 kB)\n  Preparing metadata (setup.py) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[1 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m error in demjson setup command: use_2to3 is invalid.\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\nRequirement already satisfied: elementpath in /srv/conda/envs/notebook/lib/python3.7/site-packages (3.0.2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"1.1 Prepare structured data from metadata","metadata":{}},{"cell_type":"code","source":"#preliminary imports\n!pip install python-csv\n!pip install elementpath\nimport csv\nimport xml.etree.ElementTree as ET","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"4ZW8b0GdYl6l","outputId":"ffd5c978-2c92-4c67-8174-b98bfd519316","trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Collecting python-csv\n  Using cached python-csv-0.0.13.tar.gz (26 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting argparse\n  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\nRequirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-csv) (1.21.6)\nRequirement already satisfied: pandas in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-csv) (1.3.5)\nCollecting matplotlib\n  Using cached matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\nCollecting xlrd\n  Using cached xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\nCollecting xmltodict\n  Using cached xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\nCollecting demjson\n  Using cached demjson-2.2.4.tar.gz (131 kB)\n  Preparing metadata (setup.py) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[1 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m error in demjson setup command: use_2to3 is invalid.\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\nRequirement already satisfied: elementpath in /srv/conda/envs/notebook/lib/python3.7/site-packages (3.0.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"#function to have back the element texts\ndef extract_data(path):\n    if SCHEDA.find(path) != None:\n        name = SCHEDA.find(path).text\n    else:\n        name = None\n    return name\n\n#parse the complexive Fxml and OAxml files\nF_tree = ET.parse(\"data/0_source_data/F_entries.xml\")\nF_root = F_tree.getroot()\nF_root.attrib[\"test\"]\n\nOA_tree = ET.parse(\"data/0_source_data/OA_entries.xml\")\nOA_root = OA_tree.getroot()\nOA_root.attrib[\"test2\"]\n\n#set the colums' headers for the choosen elements\nheader = [\"sercdf_F_ser\", \"sercdoa_OA_ser\", \"INVN_F\", \"UBFC_Fshelfmark\", #ids\n          \"PVCS_OAcountry\", \"PVCC_OAtown\", \"LDCN_OArep\", \"PRVC_OAprev_town\", \"AUFI_Fatelier_address\", #places\n          \"AUFN_Faut\", \"SGLT_Ftitle\", \"SGTT_OAtitle\", \"AUTN_OAaut\", #authors/titles\n          \"OGTT_OAtype\", \"AUTB_Fsubj_main\", \"OGTDOA_OAsubj_sub\", #subjects\n          \"ROFI_Fneg\", \"BIBA_OAbib\",#external relations\n          \"OSS_Fnotes\", \"OSS_OAnotes\", #unstructured infos\n         \"FTAN_filename\", \"NCTN_F_entry\", \"NRSCHEDA_OA_entry\", #2ary ids\n        \"DTSI_OAdate\", \"LRD_Fshotdates\", \"DTSI_Fprintdates\", \"DTSF_Fprintdates\", \"AUFA_Faut_dates\"] #time\n\n#setting an empty list\ndata = []\n\n#iterate on F_entries - and on correspondig OA_entries -\n#for extracting elements texts, store them in a list and add it to the data\n#two fields from original data are futherly modify for our purposes\n\nfor SCHEDA in F_root.findall(\"SCHEDA\"):\n    oa_ser = SCHEDA.get(\"sercdoa\")\n    f_ser = SCHEDA.get(\"sercdf\")\n    inv = extract_data(\"./PARAGRAFO/INVN\")\n    container = extract_data(\"./PARAGRAFO/UBFT\")\n    shelf = extract_data(\"./PARAGRAFO/UBFC\")\n    title_f = extract_data(\"./PARAGRAFO/RIPETIZIONE/SGLT\")\n    aut_f = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUFN\") #the original data do not distinguish AUFN and AUFB for collective agents\n    aut_f_dates = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUFA\")#timespan of photographer's actvity\n    aut_f_addr = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUFI\")#place of photographer's actvity as reported in the photograph >AF of variants\n    aut_oa = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUTN\")\n    subj_main = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUTB\")\n    subj_sub = extract_data(\"./PARAGRAFO/RIPETIZIONE/OGTDOA\")\n    notes_f = extract_data(\"./PARAGRAFO/OSS\")\n    neg_num = extract_data(\"./PARAGRAFO/ROFI\")\n    f_entry = extract_data(\"./PARAGRAFO/NCTN\")\n    filename = extract_data(\"./PARAGRAFO/FTAN\")\n    shotdates = extract_data(\"./PARAGRAFO/LRD\")\n    if shotdates != None:\n        #reduce uncertainty: if /ante in field, put 1855 as conventional beginning date\n        #for collodium negatives (accordign to other Zeri cataloguing)\n        if \"/ante\" in shotdates:\n            shotdates.replace(\"/ante\", \"/ ante\")\n        if \"/ ante\" in shotdates:\n        #if re.match(\"/ante|\\/ ante\", shotdates):\n            shotdates = \"1855-\"+shotdates[:-6]\n    printdates_start = extract_data(\"./PARAGRAFO/DTSI\")\n    printdates_end = extract_data(\"./PARAGRAFO/DTSF\")\n\n    for SCHEDA in OA_root.findall(\"SCHEDA\"):\n        if SCHEDA.get(\"sercdoa\") == oa_ser:\n            title_oa = extract_data(\"./PARAGRAFO/SGTT\")\n            date_from_oa = extract_data(\"./PARAGRAFO/DTSI\")\n            date_to_oa = extract_data(\"./PARAGRAFO/DTSF\")\n            country_oa = extract_data(\"./PARAGRAFO/PVCS\") #Original data report just 2 LRCS: name of the country where the shot was taken.\n            town_oa = extract_data(\"./PARAGRAFO/PVCC\") #Original data report just 2 LRCC: name of the country where the shot was taken.\n            rep_oa = extract_data(\"./PARAGRAFO/LDCN\")\n            prev_town_oa = extract_data(\"./PARAGRAFO/RIPETIZIONE/PRVC\")\n            if prev_town_oa != None:\n                #save the previous locations only if in 1800-1899 timespan (PRDU) otherwise put \"NR\" (not relevant)\n                if extract_data(\"./PARAGRAFO/RIPETIZIONE/PRDU\") != None:\n                    if \"1799\" < extract_data(\"./PARAGRAFO/RIPETIZIONE/PRDU\") < \"1900\": #PRDU last date the OA was in that location\n                        prev_town_oa = prev_town_oa + \" | \" + str(extract_data(\"./PARAGRAFO/RIPETIZIONE/PRDU\"))\n                    else:\n                        prev_town_oa = \"NR\"\n            type_oa = extract_data(\"./PARAGRAFO/OGTT\")\n            notes_oa = extract_data(\"./PARAGRAFO/OSS\")\n            oa_entry = extract_data(\"./PARAGRAFO/NRSCHEDA\")\n            beg_date_oa = extract_data(\"./PARAGRAFO/DTSI\")\n            if extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBA\") != None:\n                #save the bib ref only if in 1800-1899 timespan (BIBD) otherwise put \"NR\" (not relevant)\n                if extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBD\") != None:\n                    if \"1799\" < extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBD\") < \"1900\": #PRDU last date the OA was in that location\n                        bib_oa = extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBA\")\n                        bib_oa = bib_oa + \" | \" + str(extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBD\"))\n                    else:\n                        bib_oa = \"NR\"\n            else:\n                bib_oa = None\n\n    row = [oa_ser, f_ser, inv, shelf,\n           country_oa, town_oa, rep_oa, prev_town_oa, aut_f_addr,\n           aut_f, title_f, title_oa, aut_oa,\n           type_oa, subj_main, subj_sub,\n           neg_num, bib_oa,\n           notes_f, notes_oa,\n           filename, f_entry, oa_entry,\n           beg_date_oa, shotdates, printdates_start, printdates_end, aut_f_dates]\n    data.append(row)\n\n#Write the data and their header in a new csv dataset\nwith open(\"data/F_OA_selected_data.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as tabular_data:\n    # create the csv writer\n    writer = csv.writer(tabular_data)\n    writer.writerow(header)\n    writer.writerows(data)\n\n#have a look at the data\ndata_df = pd.read_csv('data/F_OA_selected_data.csv')\nprint(data_df.head(10))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"id":"d_Z8m2qdjB8K","outputId":"fd84ccf1-9ca7-484c-e2db-bba17285e26a","trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"   sercdf_F_ser  sercdoa_OA_ser   INVN_F  UBFC_Fshelfmark PVCS_OAcountry  \\\n0        106452          177166  sup 748   PA_Abruzzo 002         Italia   \n1        106452          177715  sup 747   PA_Abruzzo 001         Italia   \n2        106457          177733  sup 751   PA_Abruzzo 003         Italia   \n3        106458          177737  sup 867    PA_Marche 001         Italia   \n4        106460          177745  sup 753   PA_Abruzzo 004         Italia   \n5        106461          177748  sup 755   PA_Abruzzo 005         Italia   \n6        106462          177749  sup 756   PA_Abruzzo 006         Italia   \n7        106464          177750  sup 757   PA_Abruzzo 007         Italia   \n8        106463          177751  sup 758   PA_Abruzzo 008         Italia   \n9        106465          177767  sup 982  PA_Campania 001         Italia   \n\n              PVCC_OAtown                         LDCN_OArep PRVC_OAprev_town  \\\n0            Massa d'Albe                                NaN              NaN   \n1            Massa d'Albe                                NaN              NaN   \n2                L'Aquila                                NaN              NaN   \n3                  Urbino                                NaN              NaN   \n4                L'Aquila                                NaN              NaN   \n5                 Sulmona            Palazzo dell'Annunziata              NaN   \n6                 Sulmona            Palazzo dell'Annunziata              NaN   \n7  Castiglione a Casauria  Abbazia di S. Clemente a Casauria              NaN   \n8                L'Aquila     Chiesa di S. Michele Arcangelo              NaN   \n9                 Ravello     Chiesa di S. Giovanni del Toro              NaN   \n\n                               AUFI_Fatelier_address           AUFN_Faut  ...  \\\n0                                                NaN  Moscioni, Romualdo  ...   \n1                                                NaN  Moscioni, Romualdo  ...   \n2                                                NaN             Anonimo  ...   \n3                                            Alinari   Alinari, Fratelli  ...   \n4                                                NaN             Anonimo  ...   \n5                                                NaN  Moscioni, Romualdo  ...   \n6                                                NaN  Moscioni, Romualdo  ...   \n7  Fotografia Artistica. Eredi di R. Moscioni. Vi...  Moscioni, Romualdo  ...   \n8                            Fot. R. Moscioni - Roma  Moscioni, Romualdo  ...   \n9                                                NaN             Anonimo  ...   \n\n                                          OSS_Fnotes  \\\n0  Fotografia incollata su cartoncino; supporto s...   \n1  Fotografia incollata su cartoncino; supporto s...   \n2  Fotografia incollata su cartoncino; supporto s...   \n3  Fotografia incollata su cartoncino; supporto s...   \n4  Fotografia incollata su cartoncino; supporto s...   \n5  Fotografia incollata su cartoncino; supporto s...   \n6  Fotografia incollata su cartoncino; supporto s...   \n7                                                NaN   \n8                                                NaN   \n9  Fotografia incollata su cartoncino; supporto s...   \n\n                                         OSS_OAnotes  \\\n0  Foto sup 748, verso: nota anonima: \"Near Avezz...   \n1  Foto sup 748, verso: nota anonima: \"Near Avezz...   \n2                                                NaN   \n3                                                NaN   \n4  Foto sup 763, verso: nota anonima manoscritta:...   \n5                                                NaN   \n6                                                NaN   \n7                                                NaN   \n8                                                NaN   \n9  Foto sup 982, verso: nota anonima manoscritta:...   \n\n                               FTAN_filename NCTN_F_entry NRSCHEDA_OA_entry  \\\n0   \\FONDO_SUPINO\\Italia\\Abruzzo\\sup 748.jpg       800748             17783   \n1   \\FONDO_SUPINO\\Italia\\Abruzzo\\sup 747.jpg       800747             17783   \n2   \\FONDO_SUPINO\\Italia\\Abruzzo\\sup 751.jpg       800751             18185   \n3    \\FONDO_SUPINO\\Italia\\Marche\\sup 867.jpg       800867             18541   \n4   \\FONDO_SUPINO\\Italia\\Abruzzo\\sup 753.jpg       800753             18187   \n5   \\FONDO_SUPINO\\Italia\\Abruzzo\\sup 755.jpg       800755             18359   \n6   \\FONDO_SUPINO\\Italia\\Abruzzo\\sup 756.jpg       800756             18361   \n7   \\FONDO_SUPINO\\Italia\\Abruzzo\\sup 757.jpg       800757             18495   \n8   \\FONDO_SUPINO\\Italia\\Abruzzo\\sup 758.jpg       800758             18383   \n9  \\FONDO_SUPINO\\Italia\\Campania\\sup 982.jpg       800982             18540   \n\n  DTSI_OAdate LRD_Fshotdates DTSI_Fprintdates DTSF_Fprintdates AUFA_Faut_dates  \n0    300 a.C.      1885-1899             1885             1899             NaN  \n1    300 a.C.      1885-1899             1885             1899             NaN  \n2        1250      1855-1899             1855             1899             NaN  \n3        1400      1881-1887             1881             1887       1854-2019  \n4        1287      1855-1899             1855             1899             NaN  \n5        1415      1885-1899             1885             1899             NaN  \n6        1483      1885-1899             1885             1899             NaN  \n7        1175      1885-1903             1925             1929             NaN  \n8        1100      1885-1903             1885             1925             NaN  \n9        1200            NaN             1855             1899             NaN  \n\n[10 rows x 28 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"1.2 Prepare unstructured data from transcribed annotations","metadata":{}},{"cell_type":"code","source":"!pip install pandas\nimport pandas as pd","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"f_L0Nt8bOhrU","outputId":"40c00cfb-a824-4acc-d8b4-291b0ea5c407","trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /srv/conda/envs/notebook/lib/python3.7/site-packages (1.3.5)\nRequirement already satisfied: numpy>=1.17.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (1.21.6)\nRequirement already satisfied: python-dateutil>=2.7.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (2022.7)\nRequirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"data_df = pd.read_csv('data/F_OA_selected_data.csv')\n\n#reduce the dataset to just the columns needed, the not-empty and not-duplicates rows\nOAnotes_df = data_df[[\"OSS_OAnotes\"]].dropna()\nOAnotes_df = OAnotes_df.drop_duplicates()\n\n#split multilines rows and once again remove duplicates rows\nOAnotes_df[\"OSS_OAnotes\"] = OAnotes_df[\"OSS_OAnotes\"].str.split(\"&#10;|\"\". Foto \", expand = False)\nOAnotes_df = OAnotes_df.explode(\"OSS_OAnotes\")\nOAnotes_df = OAnotes_df.drop_duplicates()\n\n#save just rows with transcriptions notes (including \"Foto sup \\d{1,4}\" string)\nOAnotes_df = OAnotes_df[OAnotes_df[\"OSS_OAnotes\"].str.contains(\"sup \\d{1,4}\")== True].reset_index(drop=True)\nOAnotes_df = OAnotes_df[OAnotes_df[\"OSS_OAnotes\"].str.startswith(\"La foto \")== False].reset_index(drop=True)\n\n#separe note texts from other infos and remove the column containing the whole infos, save and check the result\nOAnotes_df[[\"Inv\", \"Note\"]] = OAnotes_df[\"OSS_OAnotes\"].str.split(': \"', n=1, expand=True)\nOAnotes_df= OAnotes_df.drop(columns=[\"OSS_OAnotes\"]).reset_index(drop=True)\nprint(\"Photographs which annotations have been transcribed in OA entries: \", OAnotes_df.shape[0], \"(/over 3.222 photographs\") #1839\nprint(OAnotes_df.head(10))\nOAnotes_df.to_csv(\"data/1_working_data/1_OAnotes01.csv\", encoding=\"utf-8\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"f_L0Nt8bOhrU","outputId":"40c00cfb-a824-4acc-d8b4-291b0ea5c407","trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Photographs which annotations have been transcribed in OA entries:  1839 (/over 3.222 photographs\n                                                 Inv  \\\n0                  Foto sup 748, verso: nota anonima   \n1      Foto sup 763, verso: nota anonima manoscritta   \n2      Foto sup 982, verso: nota anonima manoscritta   \n3  Foto sup 893, verso: nota anonima manoscritta:...   \n4      Foto sup 988, verso: nota anonima manoscritta   \n5      Foto sup 991, verso: nota anonima manoscritta   \n6      Foto sup 998, verso: nota manoscritta anonima   \n7     Foto sup 1000, verso: nota anonima manoscritta   \n8     Foto sup 1002, verso: nota anonima manoscritta   \n9     Foto sup 1003, verso: nota anonima manoscritta   \n\n                                                Note  \n0  Near Avezzano and not far from Tagliacozzo. He...  \n1  Aquila. S. Maria di Collemaggio. Founded by Pi...  \n2  The pulpit of San Giovanni del Toro, of the mi...  \n3                                               None  \n4  Here also his buried Sibylla of Burgundy. \"Rex...  \n5  Queen Margherita widow of Carlo III, who died ...  \n6  Piissimi Patris Nicolai Piscicelli optimi pres...  \n7  Rude sarcophagus in the porch of the church. T...  \n8  Amalfi. This campanile is said to date from 11...  \n9  Cloister of the Canonica founded in 1213 by Ca...  \n","output_type":"stream"}]},{"cell_type":"code","source":"#manual checking and adjusting for 1)\"manoscritta:\">\"manoscritta\",\" 2)\"\".\",>\"\".\" 3)\\n\",>\"\n#inv: 943, 75, 559, 1635, 1648, 1702, (1768 non riporta), 1787, 2397, 2789, 2869, 2849,\n# 2984, 2222,2270,2880, saved in data\\OAnotes02.csv\n\n#open the manually modified dataframe, search for unsuseful informations in 'Inv' and eliminate them\nOAnotes2_df = pd.read_csv('data/1_working_data/1_OAnotes02.csv', encoding=\"utf-8\").dropna(subset=['Inv']).reset_index(drop=True)\npattern = 'Foto |, (.+)'\nOAnotes2_df[\"Inv\"] = OAnotes2_df[\"Inv\"].replace(to_replace=pattern, value='', regex=True).reset_index(drop=True)\n\n#check and save the third version of OAnotes_df\nprint(OAnotes2_df.head(15))\nOAnotes2_df.to_csv(\"data/1_working_data/1_OAnotes03.csv\", encoding=\"utf-8\")\n\n#check how many of them are incomplete\nannotations_incompleted = OAnotes2_df[OAnotes2_df['Note'].str.contains(\"[...]\")== True].reset_index(drop=True)\nprint(\"Photographs which transcribed annotations are likely to be incomplete: \", annotations_incompleted.shape[0], \"(/over \",OAnotes_df.shape[0],\" transcribed)\")\n\n#create the corpus to be passed with spacy\ncorpus = \"\"\nfor OAnote in OAnotes2_df[\"Note\"]:\n    corpus = corpus+\"---\"+str(OAnote)+\"---\\n\"\nwith open(\"data/OAnotes_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n        f.write(corpus)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":276},"id":"tNX-1to7PfDw","outputId":"fb472629-61eb-4438-828b-8cd0b3848817","trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"    Unnamed: 0       Inv                                               Note\n0            0   sup 748  Near Avezzano and not far from Tagliacozzo. He...\n1            1   sup 763  Aquila. S. Maria di Collemaggio. Founded by Pi...\n2            2   sup 982  The pulpit of San Giovanni del Toro, of the mi...\n3            3   sup 893  Clara pudicicie dux Paulabianca potentis / A g...\n4            4   sup 988  Here also his buried Sibylla of Burgundy. \"Rex...\n5            5   sup 991  Queen Margherita widow of Carlo III, who died ...\n6            6   sup 998  Piissimi Patris Nicolai Piscicelli optimi pres...\n7            7  sup 1000  Rude sarcophagus in the porch of the church. T...\n8            8  sup 1002  Amalfi. This campanile is said to date from 11...\n9            9  sup 1003  Cloister of the Canonica founded in 1213 by Ca...\n10          10  sup 1012  In cloister of Amalfi Duomo. Sarc. of an archb...\n11          11  sup 1005  Amalfi. Cloister of San Francesco founded by t...\n12          12   sup 874  Urbino. / Palazzo Ducale, the old Montefeltro ...\n13          13   sup 882  Santa Casa. Loreto. / The Annunciation by the ...\n14          14  sup 1007  The great Oscan godders of maternity. This of ...\nPhotographs which transcribed annotations are likely to be incomplete:  1830 (/over  1839  transcribed)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2. Data elaboration\n\n\n","metadata":{"id":"WvgsBKu0cKpF"}},{"cell_type":"markdown","source":"1. Work on photographers","metadata":{"id":"66Ejy3lGkspx"}},{"cell_type":"code","source":"!pip install SPARQLWrapper\n!pip install geopy\nfrom csv import DictReader\nfrom SPARQLWrapper import SPARQLWrapper, JSON\nimport pandas as pd\nimport ssl\nfrom geopy.geocoders import Nominatim","metadata":{"id":"DRaKAR62kXIV","trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Requirement already satisfied: SPARQLWrapper in /srv/conda/envs/notebook/lib/python3.7/site-packages (2.0.0)\nRequirement already satisfied: rdflib>=6.1.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from SPARQLWrapper) (6.2.0)\nRequirement already satisfied: importlib-metadata in /srv/conda/envs/notebook/lib/python3.7/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (4.11.4)\nRequirement already satisfied: pyparsing in /srv/conda/envs/notebook/lib/python3.7/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.0.9)\nRequirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.7/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (65.6.3)\nRequirement already satisfied: isodate in /srv/conda/envs/notebook/lib/python3.7/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (0.6.1)\nRequirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata->rdflib>=6.1.1->SPARQLWrapper) (3.11.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata->rdflib>=6.1.1->SPARQLWrapper) (4.1.1)\nRequirement already satisfied: six in /srv/conda/envs/notebook/lib/python3.7/site-packages (from isodate->rdflib>=6.1.1->SPARQLWrapper) (1.16.0)\nRequirement already satisfied: geopy in /srv/conda/envs/notebook/lib/python3.7/site-packages (2.3.0)\nRequirement already satisfied: geographiclib<3,>=1.52 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from geopy) (2.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"ssl._create_default_https_context = ssl._create_unverified_context\ngeolocator = Nominatim(timeout=10, user_agent=\"myGeolocator\")","metadata":{"id":"XuizLixzktEN","trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# functions\n# define a function to open file in reading mode\ndef process_csv(data_file_path):\n    import csv\n    source = open(data_file_path, mode=\"r\", encoding=\"UTF8\")\n    source_reader = csv.DictReader(source)\n    source_data = list(source_reader)\n    return source_data\n\n#define a function for transforming lists of elements in strings\ndef write_string(source, output_txt_name):\n    string = \"\"\n    for source_data in source:\n        string = string+source_data+\"|\"\n    string = string[:-1]\n    with open(output_txt_name, \"w\", encoding=\"utf-8\") as f:\n        f.write(string)\n    return string\n\n#define a function to query endpoints\ndef query_endpoint(endpoint_url, SPRQL_query):\n    get_endpoint = endpoint_url\n    sparql_w = SPARQLWrapper(get_endpoint)\n    sparql_w.setQuery(SPRQL_query)\n    sparql_w.setReturnFormat(JSON)\n    spqrl_w_res = sparql_w.query().convert()\n    return spqrl_w_res\n\n#define a function to manipulate results and have back 1. a set of wd_URI corresponding to our wd_names,\n# 2. update of ph_matrix, 3. not matched wd_names\n\ndef manipulate(spqrl_w_res, dataset_to_enhance):\n    res_dic = {}\n    res_NF_tem = set()\n    res_F = set()\n    for res in spqrl_w_res[\"results\"][\"bindings\"]:\n        for datum in dataset_to_enhance:\n            if datum[\"ph_wd_URI\"]:\n                continue\n            else:\n                if datum[\"ph_wd_name\"] not in res_dic:\n                    if res[\"fLabel\"][\"value\"] == datum[\"ph_wd_name\"]:\n                        res_F.add(res[\"f\"][\"value\"])\n                        new_pairs = {\"ph_wd_URI\": res[\"f\"][\"value\"]}\n                        res_dic.update({datum[\"ph_wd_name\"]: new_pairs})\n                        datum.update([(\"ph_wd_URI\", res[\"f\"][\"value\"])])\n                    else:\n                        res_NF_tem.add(datum[\"ph_wd_name\"])\n    res_NF_def = res_NF_tem - set(list(res_dic.keys()))\n    print(\"labels matched: \", len(res_F))\n    print(\"labels not found: \", len(res_NF_def))\n    return res_F, res_NF_def","metadata":{"id":"PDzPD39uj7pn","trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n# open source data with pandas\ndata_df = pd.read_csv(\"data/F_OA_selected_data.csv\")\n\n#initialize a photograph's frequency dataframe\nph_freq = pd.DataFrame(data_df[\"AUFN_Faut\"].value_counts().reset_index().values, columns=[\"AUFN_Faut\", \"count\"])\n\n#extend dataframe colums to host next datas\nph_freq[\"ph_wd_name\"], ph_freq[\"ph_wd_URI\"], ph_freq[\"gender\"], ph_freq[\"workplace\"], ph_freq[\"lat\"], ph_freq[\"lon\"],\\\nph_freq[\"born\"], ph_freq[\"died\"], ph_freq[\"lat\"] = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]","metadata":{"id":"sz5v6r-4k7UO","trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"#create the firts string for the SPARQL query by \n#normalizing (personal) names in form \"surname, name\" to \"name surname\" as in wikidata\n#and create a list of the modified names tobe added to the dataframe\n\nfirst_ph_names_string =\"\"\nph_wd_name_list = []\nfor ph in ph_freq.index:\n    ph_name = str(ph_freq[\"AUFN_Faut\"][ph])\n    # reverse only (personal) names in form \"surname, name\" > \"name surname\"\n    if \", \" in ph_name:\n        ph_split = ph_name.split(\", \")\n        ph_wd_name = ph_split[1] + \" \" + ph_split[0]\n    else:\n        ph_wd_name = ph_name\n    ph_wd_name_list.append(ph_wd_name)\n    first_ph_names_string = first_ph_names_string + ph_wd_name + \"|\"\nfirst_ph_names_string = first_ph_names_string[:-1]\n\n#show a sample of the string and save it\nprint(first_ph_names_string[0:200])\nwith open(\"data/1_working_data/2_PHstring01.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(first_ph_names_string)","metadata":{"id":"1XL0DDREpSb5","trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Anonimo|Fratelli Alinari|Romualdo Moscioni|Brogi|Giorgio Sommer|Jean Laurent|Incorpora|Giraudon|Paolo Lombardi|Naya|Carlo Baldassarre Simelli|Pietro Poppi|Séraphin-Médéric  Mieusement|Robert Rive|John\n","output_type":"stream"}]},{"cell_type":"code","source":"#add ph_wd_name_list to the dataframe and show a sample of the current dataframe\nph_freq[\"ph_wd_name\"] = ph_wd_name_list\nprint(ph_freq.head(10))\n#save the dataframe in a csv file and open it as a dictionary to iterate\nph_freq.to_csv(\"data/1_working_data/2_PH_freq_01.csv\", encoding=\"utf-8\")","metadata":{"id":"1XL0DDREpSb5","trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"            AUFN_Faut count         ph_wd_name ph_wd_URI gender workplace lat  \\\n0             Anonimo  1336            Anonimo                                  \n1   Alinari, Fratelli   556   Fratelli Alinari                                  \n2  Moscioni, Romualdo   159  Romualdo Moscioni                                  \n3               Brogi   158              Brogi                                  \n4     Sommer, Giorgio   147     Giorgio Sommer                                  \n5       Laurent, Jean    73       Jean Laurent                                  \n6           Incorpora    57          Incorpora                                  \n7            Giraudon    55           Giraudon                                  \n8     Lombardi, Paolo    53     Paolo Lombardi                                  \n9                Naya    48               Naya                                  \n\n  lon born died  \n0                \n1                \n2                \n3                \n4                \n5                \n6                \n7                \n8                \n9                \n","output_type":"stream"}]},{"cell_type":"code","source":"ph_matrix = process_csv(\"data/1_working_data/2_PH_freq_01.csv\")\nfirst_ph_names_string = open('data/1_working_data/2_PHstring01.txt', 'r', encoding=\"utf-8\").read()\n\n#prepare the first query string to collect wikidata URI\nfirst_ph_SPARQL_query = \"\"\"\nSELECT DISTINCT ?f ?fLabel\nWHERE\n{    { ?f wdt:P106 wd:Q33231 } UNION { ?f wdt:P31 wd:Q672070}. #P106_has_for_occupation wd:Q33231_photographer \n                                                                #P31_is instance wd:Q672070_studios\n    ?f rdfs:label ?fLabel.\n     FILTER regex(?fLabel, \\\" \"\"\"+first_ph_names_string+\"\"\" \\\")\n     FILTER(LANG(?fLabel) = \"en\").\n}\"\"\"\n\n#perform the first SPARQL query and result manipulation\nfirst_ph_wd_res = query_endpoint(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\", first_ph_SPARQL_query)\nfirst_ph_manipulate = manipulate(first_ph_wd_res, ph_matrix)\nfirst_F_set = first_ph_manipulate[0]\nfirst_NF = first_ph_manipulate[1]\n#check not found\nprint(first_NF)","metadata":{"id":"3z0IeTINsOwE","trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"labels matched:  45\nlabels not found:  67\n{'Goupil & C.ie Editeurs', 'P. Famin & Cie.', 'Jean Giletta', 'Poulton Series', 'J. Garrigues', 'J. Kuhn', 'A. Dumaine', 'Fratelli Esposito', 'Tuminello Lodovico', 'Séraphin-Médéric  Mieusement', 'Stereoscopic Co.', 'Naya', 'Fratelli Amodio', 'Carl Prior Merlin', 'Brogi', 'Filippo Lais', 'Lombardi', 'Michele Petagna', 'Vasari', 'Incorpora', 'Fotografia A. Premi', 'Giuseppe Polozzi', 'Artistico ed Etnoantropologico e per il Polo Museale della città di Firenze Gabinetto Fotografico della Soprintendenza Speciale per il Patrimonio Storico', 'Pere Pallejá Domenech', 'Robert MacPherson', 'George Wilson Washington', 'Enrico Pezzani', 'Giovanni Battista Unterverger', 'Rozier', 'Istituto Centrale per il Catalogo e la Documentazione: Fototeca Nazionale', 'William Lawrence', 'Johannes  Jaeger', 'W. F. Mansell', 'Budtz Muller & Co.', 'Neurdein', 'Pierre Henry  Voland', 'Albert', 'Paul des Granges', 'Istituto Fotografico Antonio Fortunato Perini', 'Francesco Fiorani', 'Clarke & Davies', 'Nicola De Mattia', 'Guillaume Gustave Berggren', 'Dimitris Konstantinou', 'Luigi Brillet-Buyet', 'Alary & Geiser', 'Peter Paul Mackey', 'Cesare Benvenuti', 'Béchard Henri', 'Zedler & Vogel', 'Tagliarini', 'Francisco Soler', 'Giraudon', 'Edith Emily Coulson James', 'Neue Photographische Gesellschaft', 'Pascal Sébah', 'G. Stuffler', 'Vincenzo Paganori', 'Francesco De Rubeis', 'Lyon E.D.', 'Emilio Anriot', 'Francesco Venturi', 'Studio Fotografico Ciappei', 'Sommer & Behles', 'Bruckmann Verlag', 'Abdullah Frères', 'Anonimo'}\n","output_type":"stream"}]},{"cell_type":"code","source":"#after revising first results, refine the unmatched labels\nnew_list = []\nfor ph_wd_NF in first_NF:\n    if \"  \" in ph_wd_NF:\n        ph_wd_new = ph_wd_NF.replace(\"  \", \" \") #cancel double spaces\n    elif \"Fratelli\" in ph_wd_NF:\n        ph_wd_new = ph_wd_NF.replace(\"Fratelli\", \"\") #cancel \"Fratelli\"\n    elif \"&\" in ph_wd_NF:\n        ph_wd_new = ph_wd_NF.replace(\"&\", \"and\") #change \"&\" in \"and\"\n    #check for corresponding form\n    elif \"Brogi\" == ph_wd_NF:\n        ph_wd_new = \"Giacomo Brogi\" \n    elif \"Incorpora\" == ph_wd_NF:\n        ph_wd_new = \"Giuseppe Incorpora\"\n    elif \"Giraudon\" == ph_wd_NF:\n        ph_wd_new = \"Adolphe Giraudon\"\n    else:\n        continue\n    new_list.append(ph_wd_new)\n    for ph_data in ph_matrix:\n        if ph_data[\"ph_wd_name\"] == ph_wd_NF:\n            ph_data.update([(\"ph_wd_name\", ph_wd_new)])\n\n#from the new modified names, by using the function, obtain a second string to query \nsecond_ph_string = write_string(new_list, \"data/1_working_data/2_PHstring02.txt\")\nprint(second_ph_string)","metadata":{"id":"PjIOKX7Yts7A","trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Goupil and C.ie Editeurs|P. Famin and Cie.| Esposito|Séraphin-Médéric Mieusement| Amodio|Giacomo Brogi|Giuseppe Incorpora|Johannes Jaeger|Budtz Muller and Co.|Pierre Henry Voland|Clarke and Davies|Alary and Geiser|Zedler and Vogel|Adolphe Giraudon|Sommer and Behles\n","output_type":"stream"}]},{"cell_type":"code","source":"#from the new modified names, by using the function obtain a second string to query\nsecond_ph_names_string = write_string(new_list, \"data/1_working_data/2_PHstring02.txt\")\n\n#prepare the second query string to collect wikidata URI\nsecond_ph_SPARQL_query = \"\"\"\nSELECT DISTINCT ?f ?fLabel\nWHERE\n{    { ?f wdt:P106 wd:Q33231 } UNION { ?f wdt:P31 wd:Q672070}. #P106_has_for_occupation wd:Q33231_photographer \n                                                                #P31_is instance wd:Q672070_studios\n    ?f rdfs:label ?fLabel.\n     FILTER regex(?fLabel, \\\" \"\"\"+second_ph_string+\"\"\" \\\")\n     FILTER(LANG(?fLabel) = \"en\").\n}\n\"\"\"\n#perform the second SPARQL query and result manipulation\nsecond_ph_wd_res = query_endpoint(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\", second_ph_SPARQL_query)\nsecond_manipulate = manipulate(second_ph_wd_res, ph_matrix)\nsecond_F_set = second_manipulate[0]","metadata":{"id":"eZjQSnZsuRZX","trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"labels matched:  5\nlabels not found:  62\n","output_type":"stream"}]},{"cell_type":"code","source":"#obtain the list of found wikidata URI\ncomplex_F_set = second_F_set.union(first_F_set)\n\n#prepare the thirtd string to be passed in SPARQL query and save it\nthird_ph_string_URI =\"\"\nfor F_URI in complex_F_set:\n    third_ph_string_URI = third_ph_string_URI+\"<\"+F_URI+\">\"\n    \nwith open(\"data/1_working_data/2_PHstring03.txt\", \"w\", encoding=\"utf8\") as f:\n    f.write(third_ph_string_URI)\n\n#third query\nthird_ph_SPARQL_query = \"\"\"\nSELECT DISTINCT ?ph ?genderLabel ?countryLabel ?birthyear ?deathyear\n    WHERE\n    { VALUES ?ph {\"\"\"+third_ph_string_URI+\"\"\"} \n        ?ph rdfs:label ?phLabel;\n        wdt:P937 ?country; #P937_worklocation\n        #wdt:P27 ?citiz;        \n        wdt:P569 ?birth;\n        wdt:P570 ?death.\n        OPTIONAL {FILTER(LANG(?fLabel) = \"en\").\n                    ?ph wdt:P21 ?gender;\n                    #wdt:P937 ?worklocation; #P937_worklocation\n        }\n        BIND(year(?birth) AS ?birthyear)\n        BIND(year(?death) AS ?deathyear)\n\n        #BIND(COALESCE(?worklocation, ?citiz, \"NaN\") AS ?country).\n        #BIND(IF(BOUND(?worklocation),?worklocation,?citiz) AS ?country).\n    SERVICE wikibase:label {bd:serviceParam wikibase:language \"en\".}     \n    }\"\"\"\n#OPTIONAL { ?ph wdt:P569 ?birthdate;        wdt:P570 ?deathdate.} ci servono...\n\n#perform the third query\nthird_ph_wd_res = query_endpoint(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\", third_ph_SPARQL_query)\n\n#manipulate results\nwd_total_dic = {}\nfor result in third_ph_wd_res[\"results\"][\"bindings\"]:\n    item_key = result[\"ph\"][\"value\"]\n    item_value = {\"workplace\": result[\"countryLabel\"][\"value\"],\n                  \"lat\": geolocator.geocode(result[\"countryLabel\"][\"value\"]).latitude,\n                  \"lon\": geolocator.geocode(result[\"countryLabel\"][\"value\"]).longitude,\n                  \"born\": result[\"birthyear\"][\"value\"],\n                  \"died\": result[\"deathyear\"][\"value\"]}\n    if item_key not in wd_total_dic:\n        wd_total_dic.update({item_key: item_value})\n        for ph_data in ph_matrix:\n            if ph_data[\"ph_wd_URI\"] == item_key:\n                item2=item_value.items()\n                ph_data.update(item2)\n\n#wd_total_list = list(wd_total_dic.values())\n#print(wd_total_list)\n\n#save the enhanced matrix\nkeys = ph_matrix[0].keys()\nwith open(\"data/2_PHfreq.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as output_file:\n    dict_writer = csv.DictWriter(output_file, keys)\n    dict_writer.writeheader()\n    dict_writer.writerows(ph_matrix)\n\n#transform the enhanced matrix in a df\nph_freq_df = pd.DataFrame.from_dict(ph_matrix, orient='columns', dtype=None, columns=None)","metadata":{"id":"RsSNGDE8v1py","trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"2. Work on places","metadata":{"id":"0xSgal3tQruP"}},{"cell_type":"code","source":"#define function to store lat-lon from a list of places\ndef get_coordinates(list, df):\n    for place in list:\n        if place not in df[\"place\"].unique().tolist():\n            if geolocator.geocode(place) != None:\n                lon = geolocator.geocode(place).longitude\n                lat = geolocator.geocode(place).latitude\n            else:\n                lon = \"NaN\"\n                lat = \"NaN\"\n            new_place = [place, lat, lon]\n            df.loc[len(df)] = new_place\n        df.to_csv(\"data/3_PLcoordinates.csv\", encoding=\"UTF-8\")","metadata":{"id":"Z_E9BD3XQrSn","trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"#open the saved file\ndata_df = pd.read_csv(\"data/F_OA_selected_data.csv\")\nph_freq_df = pd.read_csv(\"data/2_PHfreq.csv\")\n\n#reduce columns and change column name, check first rows\nplaces_F = ph_freq_df[['workplace', \"lat\", \"lon\"]].dropna()\nplaces = places_F.rename(columns={\"workplace\": \"place\"})\nplaces.head()\n\n#extract towns and country unique names from original dataframe\ntowns_OA = data_df['PVCC_OAtown'].unique().tolist()\ncountries_OA = data_df['PVCS_OAcountry'].unique().tolist()\n\n#obtain coordinates from the two list and store them in a df\nget_coordinates(countries_OA, places)\nget_coordinates(towns_OA, places)","metadata":{"id":"Lnd5nBkERNH5","trusted":true},"execution_count":41,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAdapterHTTPError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/geopy/geocoders/base.py\u001b[0m in \u001b[0;36m_call_geocoder\u001b[0;34m(self, url, callback, timeout, is_json, headers)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreq_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/geopy/adapters.py\u001b[0m in \u001b[0;36mget_json\u001b[0;34m(self, url, timeout, headers)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/geopy/adapters.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, url, timeout, headers)\u001b[0m\n\u001b[1;32m    478\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m                     \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m                 )\n","\u001b[0;31mAdapterHTTPError\u001b[0m: Non-successful status code 502","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mGeocoderServiceError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_185/1358224450.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#obtain coordinates from the two list and store them in a df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mget_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountries_OA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mget_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtowns_OA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_185/2647495798.py\u001b[0m in \u001b[0;36mget_coordinates\u001b[0;34m(list, df)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mplace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mplace\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"place\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mgeolocator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeocode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                 \u001b[0mlon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeolocator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeocode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlongitude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mlat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeolocator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeocode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatitude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/geopy/geocoders/nominatim.py\u001b[0m in \u001b[0;36mgeocode\u001b[0;34m(self, query, exactly_one, timeout, limit, addressdetails, language, geometry, extratags, country_codes, viewbox, bounded, featuretype, namedetails)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s.geocode: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexactly_one\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexactly_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_geocoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     def reverse(\n","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/geopy/geocoders/base.py\u001b[0m in \u001b[0;36m_call_geocoder\u001b[0;34m(self, url, callback, timeout, is_json, headers)\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapter_error_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNONE_RESULT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/geopy/geocoders/base.py\u001b[0m in \u001b[0;36m_adapter_error_handler\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m    409\u001b[0m                 ) from error\n\u001b[1;32m    410\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_geocoder_exception_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mGeocoderServiceError\u001b[0m: Non-successful status code 502"],"ename":"GeocoderServiceError","evalue":"Non-successful status code 502","output_type":"error"}]},{"cell_type":"markdown","source":"3. Work on Annotations","metadata":{"id":"8ZRD3mhvZ1z3"}},{"cell_type":"code","source":"!pip install spacy\nimport spacy\nfrom spacy.matcher import Matcher\nfrom spacy.attrs import POS","metadata":{"id":"3qzXjNVGbnY6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#open the file with annotations texts\nwith open(\"data/OAnotes_corpus.txt\", mode=\"r\", encoding=\"UTF-8\") as f:\n    contents = f.read()\n \nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = Matcher(nlp.vocab)\nmatcher.add(\"PA_creator\", [[{\"LEMMA\": \"I\"}, {POS: 'VERB'}]])\ndoc = nlp(contents)\nmatches = matcher(doc)\n\nmatched = []\nfor match_id,start,end in matches:\n    I_verb = str(doc[start:end])\n    matched.append(I_verb)\nprint(matched)\n#I doubt, I was, I think(3), I discovered, I have(4), I saw(4), I AM(2?), I told, I respected, I farn, me look,\n# I believe, I put","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"cOWvR2Z3Z62_","outputId":"c55a54a4-08ee-4072-8021-4bad942ea9ce","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a dataframe from list of matched and check occurrencies\nmatched_df = pd.DataFrame()\nmatched_df[\"Match\"] = matched\nmatched_freq = pd.DataFrame(matched_df[\"Match\"].value_counts().reset_index().values, columns=[\"Match\", \"count\"])\nprint(matched_freq.head(25))","metadata":{"id":"WLMGHtjXcNko","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data visualization","metadata":{"id":"sxPYRxMacLDt"}},{"cell_type":"markdown","source":"\n---\n**Analyse**\npandas library in order to examine our data.\n     \n       \n    1. Data Preparation:\n          - creation of two complexive xml files for F and OA records coming from the Federico Zeri Foundation catalogues\n          - extraction from nested xml stucture of relevant information for the project and structuring them in plain tabular format\n    2. Data Elaboration: seeking for furter analysis elements via:\n          - deeper work on photographer for enhance their information\n          - deeper work on places\n          - work on unstructured annotations: NER\n     2. Data Visualization\n","metadata":{"id":"rSDRDeMOjB8S"}},{"cell_type":"markdown","source":"0. Data overview","metadata":{"id":"7U57aNxDdeOH"}},{"cell_type":"code","source":"import pandas as pd\nimport pandas_profiling as pp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pprint\npp = pprint.PrettyPrinter(indent=1)","metadata":{"id":"pgU6VKzNdtTp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# parse the csv into a dataframe\ndata_df = pd.read_csv('data\\F_OA_selected_data.csv')\n\n# reduce the dataset to just the columns needed\ndata_df = data_df[['INVN_F', 'PVCS_OAcountry', 'PVCC_OAtown', 'LDCN_OArep', 'PRVC_OAprev_town',\n          'AUFN_Faut', 'LRD_Fdates', 'OGTT_OAtype', 'AUTB_Fsubj_main', 'OGTDOA_OAsubj_sub', 'AUTN_OAaut']]\nprint(data_df.head(15))","metadata":{"id":"Kk-RBvonXIBI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import pandas_profiling as pp\nreport = pp.ProfileReport(data_df, title=\"Partizione Antica Fund - overview\")\nreport.to_file(\"ProfileReport_sup.html\")","metadata":{"id":"CDgOvQUheO2B","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169},"id":"7Hb6VMBkf7LJ","outputId":"8a7d3687-1578-40fe-99da-12a242f965f6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1.1 Works of art - typology proportions pie","metadata":{"id":"JtgJeWIngJcr"}},{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px\n\n# filter cities and number of photos from data\ndata = pd.read_csv(\"data/F_OA_selected_data.csv\", encoding=\"UTF-8\")\nOAt_df = pd.DataFrame(data[\"OGTT_OAtype\"].value_counts().reset_index().values, columns=[\"OGTT_OAtype\", \"count\"])\n\nfig = px.pie(OAt_df, values='count', names=\"OGTT_OAtype\",\n            title='OA typologies',\n            color_discrete_sequence=px.colors.sequential.RdBu,\n            labels = OAt_df['OGTT_OAtype'], hover_name = 'OGTT_OAtype',\n            hover_data = {'OGTT_OAtype':False}\n            )\nfig.show()\nfig.write_html(\"data/2_data_viz/1.1.html\")","metadata":{"id":"saYsZpZ2gI6x","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1.2 Works of art - countries proportions pie\n\n","metadata":{"id":"7rauFSANgaYi"}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nimport plotly\nimport plotly.graph_objects as go\nimport csv\n\n# filter cities and number of photos from data\ndata = pd.read_csv(\"UNIQUE_new2.csv\", encoding=\"utf8\")\nsup_df = data[['PVCS_OAcountry', 'PVCC_OAtown', 'LDCN_OArep']]\n\n#create a pie chart of supposed OA country\ndf_data = pd.read_csv(\"OAcountry_freq.csv\", encoding=\"utf8\")\ndf_data.loc[df_data['count'] < 10, 'PVCS_OAcountry'] = 'Other countries' # Represent only large countries\n#df = px.df_data()\nfig = px.pie(df_data, values='count', names=\"PVCS_OAcountry\",\n            title='Depicted OA for country',\n            color_discrete_sequence=px.colors.sequential.RdBu,\n            labels = df_data['PVCS_OAcountry'], hover_name = 'PVCS_OAcountry',\n            hover_data = {'PVCS_OAcountry':False, 'lat':False, 'lon': False}\n            )\nfig.show()\nfig.write_html(\"data/2_data_viz/1.2.html\")","metadata":{"id":"g_EONtm7gaIE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1.3 Works of art - countries proportions map","metadata":{"id":"nYz-E7P8g7fc"}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom geopy.geocoders import Nominatim\ngeolocator = Nominatim(timeout=10, user_agent = \"myGeolocator\")\n\nimport plotly\nimport plotly.graph_objects as go\nimport csv\n\n# filter cities and number of photos from data\ndf_data = pd.read_csv(\"OAcountry_freq.csv\", encoding=\"utf8\")\nph_geol = px.scatter_mapbox(df_data, lon=df_data['lon'],\n                            lat=df_data['lat'], size=df_data[\"count\"], zoom=2, color=df_data['PVCS_OAcountry'],\n                            color_continuous_scale=px.colors.cyclical.Twilight,\n                            #color_discrete_sequence=px.colors.sequential.RdBu,\n                            title=\"Depicted OA\",\n                            size_max=80,\n                            labels=df_data['PVCS_OAcountry'], hover_name='PVCS_OAcountry',\n                            hover_data={'PVCS_OAcountry':False, 'lat':False, 'lon':False})\n\n# mapbox style\nph_geol.update_layout(mapbox_style='carto-positron')\nph_geol.show()\nph_geol.write_html(\"data/2_data_viz/1.3.html\")","metadata":{"id":"Rea-Mj6qg3II","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.1 Photograps - photographers proportions pie /distribution barchart","metadata":{"id":"iifJu-PthWVN"}},{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px\n\ndf_data2 = pd.read_csv(\"ph_newfreq.csv\", encoding=\"utf8\")\ndf_data2 = df_data2[df_data2[\"count\"]>=20]\n#df_data2.loc[df_data2['count'] < 10, 'AUFN_Faut'] = 'Other photographs' # Represent only large countries\n#df = px.df_data()\nfig2 = px.pie(df_data2, values='count', names=\"AUFN_Faut\",\n            title='Photographs (>=20) for photographer',\n            color_discrete_sequence=px.colors.qualitative.Dark24, #color_discrete_sequence/color_continuous_scale =px.colors.sequential.RdBu,\n            labels = df_data2['AUFN_Faut'], hover_name='AUFN_Faut',\n            hover_data = {'AUFN_Faut':False, 'workplace':True}\n            #sistema le caselle non piene di ph_freq etc penso fillna()\n            )\nfig2.show()\nfig2.write_html(\"data/2_data_viz/2.1.html\")","metadata":{"id":"RrH1jF4rhWHG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.2 Photographs - map distribution based on ateliers locations","metadata":{"id":"pHJdaH1PiEYU"}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n#to show up everything directly in jupyter notebook\n\nimport plotly\nimport plotly.graph_objects as go\nimport csv\n\n#open the whole dataset and select just the AUFN_FAUT column\ndata = pd.read_csv(\"data\\F_OA_selected_data.csv\", encoding=\"UTF-8\")\ndata_d = data[[\"AUFN_Faut\"]]\nprint(len(data_d))\n\n#open the ph_freq dataset and select just the AUFN_FAUT and the workplace column; create a second df on workplace lat and lon\ndata_ph = pd.read_csv(\"data\\ph_freq.csv\", encoding=\"UTF-8\")\ndata_ph_df = data_ph[[\"AUFN_Faut\", \"workplace\"]]\ndata_lat_lon = data_ph[[\"workplace\", \"lat\", \"lon\"]].drop_duplicates(subset=['workplace'])\n\n#merge the two dataset on the AUFN_Faut column\ndata_comp = data_d.merge(data_ph_df, how='left', on=\"AUFN_Faut\").reset_index(drop=True)\nprint(len(data_comp))\nprint(data_comp.head(25))\n\n#count the values on workplace column\ndata_count = pd.DataFrame(data_comp['workplace'].value_counts().reset_index().values, columns=['workplace', 'count'])\nprint(len(data_count))\nprint(data_count.head(25))\n\n#add lat and lon data to the previous df\ndata_df = data_count.merge(data_lat_lon, how='left', on=\"workplace\").reset_index(drop=True)\ndata_df['count'] = pd.to_numeric(data_df['count'])\nprint(len(data_df))\nprint(data_df.head(25))\n\nph_geol = px.scatter_mapbox(data_df, lon=data_df['lon'], lat=data_df['lat'],\n                            size=data_df['count'], zoom=3, color=data_df['workplace'],\n                            size_max=80,\n                            color_discrete_sequence=px.colors.sequential.RdBu,\n                            title=\"Photohgrapher ateliers\",\n                            labels=data_df['workplace'], hover_name=\"workplace\")\n\n# mapbox style\nph_geol.update_layout(mapbox_style='carto-positron')\nph_geol.show()\nph_geol.write_html(\"data/2_data_viz/2.2.html\")","metadata":{"id":"t3ndEp0MiEiX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.3 Photographs - map distribution of anonimous photographs based on place of shooting (limited to immobles)","metadata":{"id":"nfzogXtzioNa"}},{"cell_type":"code","source":"#!pip install regex\nimport pandas as pd\nimport plotly.express as px\nimport regex as re\n\ndata = pd.read_csv(\"UNIQUE_new2.csv\", encoding=\"UTF-8\")\n\n#reduce to only columns rows needed and check\nsup_df = data[['OGTT_OAtype', 'PVCS_OAcountry', 'PVCC_OAtown', 'AUFN_Faut']]\nsup_df = sup_df[sup_df.AUFN_Faut == \"Anonimo\"]\n\ndf_data3 = pd.DataFrame(sup_df[\"PVCS_OAcountry\"].value_counts().reset_index().values, columns=[\"PVCS_OAcountry\", \"count\"])\n\nfig3 = px.pie(df_data3, values='count', names=\"PVCS_OAcountry\",\n              title='Anonimous Photographs (1331/3111) for OAcountry',\n              color_discrete_sequence=px.colors.sequential.Brwnyl,\n              labels = df_data3['PVCS_OAcountry'], hover_name='PVCS_OAcountry',\n              hover_data = {'PVCS_OAcountry':False}\n              )\n\nfig3.update_layout(\n    font = dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        )\n    )\nfig3.show()\n\nsup_df= sup_df[sup_df['OGTT_OAtype'].str.contains(\"architettura|architettura\\ scultura|complesso archeologico|sito archeologico\")== True].reset_index(drop=True)\nsup_df.to_csv(\"daje.csv\")\nprint(sup_df.tail(200))\ndf_data4 = pd.DataFrame(sup_df[\"PVCS_OAcountry\"].value_counts().reset_index().values, columns=[\"PVCS_OAcountry\", \"count\"])\nfig4 = px.pie(df_data4, values='count', names=\"PVCS_OAcountry\",\n              title='Anonimous Photographs (1331/3111) for OAcountry of immobles',\n              color_discrete_sequence=px.colors.sequential.Brwnyl, #px.colors.sequential.RdBu https://plotly.com/python/discrete-color/\n              labels = df_data3['PVCS_OAcountry'], hover_name='PVCS_OAcountry',\n              hover_data = {'PVCS_OAcountry':False}  #sistema le caselle non piene di ph_freq etc penso fillna()\n              )\n\nfig4.update_layout(\n    font = dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        )\n    )\nfig4.show()\nfig4.write_html(\"data/2_data_viz/2.3.html\")","metadata":{"id":"VxUk_S0BioDO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3.1 Annotations - complete/incomplete/missing transcriptions proportions pie","metadata":{"id":"RVfqj_usjf1s"}},{"cell_type":"code","source":"import plotly.express as px\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n#open the needed dataframes\ndata_df = pd.read_csv('data\\F_OA_selected_data.csv')\nall_inv = data_df[[\"INVN_F\"]].rename(columns={\"INVN_F\": \"Inv\"}).reset_index(drop=True)\nOAnotes_df = pd.read_csv('data/1_working_data/OAnotes03.csv')\n\n#define a function to check status of transcriptions\ndef check(row):\n    if \"...\" in str(row[\"Note\"]):\n        status = \"incomplete\"\n    else:\n        status = \"complete\"\n    return status\n\n#apply the function to df and add a status column, check it \nOAnotes_df[\"status\"] = OAnotes_df.apply(check, axis=1)\nprint(OAnotes_df.head(25))\n\n#merge the df with all the inventories to check for missing transcriptions and count according to the status \nmerged = all_inv.merge(OAnotes_df, how='left', on=\"Inv\").reset_index(drop=True)\nnew=pd.DataFrame(merged[\"status\"].value_counts(dropna=False).reset_index().values, columns=[\"status\", \"count\"])\n#change the empty rows with \"missing\" and check\nnew['status'] = new['status'].fillna('missing')\nprint(new.tail(50))\n\nfig = px.pie(new, values='count', names=\"status\",\n              title='Annotations on photographs',\n              color_discrete_sequence=px.colors.sequential.Brwnyl, #px.colors.sequential.RdBu https://plotly.com/python/discrete-color/\n              labels = new['status'], hover_name='status',\n              hover_data = {'status':True}  #sistema le caselle non piene di ph_freq etc penso fillna())\n             )\n\nfig.update_layout(\n    font = dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        )\n    )\nfig.show()\nfig.write_html(\"data/2_data_viz/3.1.html\")","metadata":{"id":"g634EN9LjfuH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3.2 Annotations","metadata":{"id":"-X5aqyMQlSii"}},{"cell_type":"code","source":"","metadata":{"id":"1ldFH4F4liEi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3.3 Annotations and metadata: compared dates distribution","metadata":{"id":"gEWFNsy5liLo"}},{"cell_type":"code","source":"","metadata":{"id":"BD4odwiSluHA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4.1 Annotations - time-place pairs related to ...","metadata":{"id":"qI4y50zSluSH"}},{"cell_type":"code","source":"import plotly.express as px\nimport pandas as pd\n\n#open and merge data from annotations reporting time-place pairs and place coordinates, check for it\nmovement_df = pd.read_csv(\"data/1_working_data/OAnotes05bis.csv\", encoding=\"utf8\")\nplaces_df = pd.read_csv(\"data/1_working_data/places_coordinates2.csv\", encoding=\"utf8\")\nmovement_df_coor = movement_df.merge(places_df, how='left', on=\"place\")\nmovement_df_coor.drop('Unnamed: 0', axis=1, inplace=True)\nprint(movement_df_coor.head(10))\n\n#break the texts at about 30 characters to let them better visualized\nmovement_df_coor['Note_br'] = movement_df_coor.apply(lambda row: ('<br>'.join(str(row.Note)[i:i+30] for i in range(0, len(str(row.Note)), 30))), axis = 1)\n\n#set scatter with d\nfig = px.scatter_geo(movement_df_coor, color=\"date\",\n                  lat=movement_df_coor[\"lat\"].values.tolist(),\n                  lon=movement_df_coor[\"lon\"].values.tolist(),\n                  title=\"Movements\", size=\"date\",\n                  projection=\"natural earth\", scope=\"europe\",\n            labels = movement_df_coor['place'], hover_name='place',\n            hover_data = {'place':False, 'Inv':True, 'Note_br':True}\n            )\nfig.show()\nfig.write_html(\"data/2_data_viz/4.1.html\")","metadata":{"id":"2x3WJ6drlSYT","trusted":true},"execution_count":null,"outputs":[]}]}