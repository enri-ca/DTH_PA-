{"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#Intro\n\nThis Jupyter Notebook has been created for the <a href=\"https://www.unibo.it/it/didattica/insegnamenti/insegnamento/2021/443749\" target=\"_blank\">90154 - Electronic Publishing and Digital Storytelling</a> course, taught by **Prof. Marilena Daquino**, in the framework of the 2nd year of the <a href=\"https://corsi.unibo.it/2cycle/DigitalHumanitiesKnowledge\" target=\"_blank\">DHDK Master Degree</a>, a.a. 2021-22.<br>\nHere listed the main steps for the realization of the project **Partizione Antica**:     \n       \n    1. Data Preparation:\n          - creation of two complexive xml files for F and OA records coming from the Federico Zeri Foundation catalogues\n          - extraction of relevant information for the project from nested xml stucture and structuring them in plain tabular format\n          - extraction from previpus tabular data of unstructured annotations\n    2. Data Elaboration: seeking for furter analysis elements via:\n          - deeper work on photographer for enhancing their information (workplace, timespan of activity, etc.)\n          - deeper work on places for enhancing their gelocation\n          - work on unstructured annotations trough NLP and NER\n    3. Data Visualization\n    - \n    -\n    -","metadata":{"id":"D76tTDYyH-z9"}},{"cell_type":"markdown","source":"# 1. Data preparation\n\nThis research started from a **record data extraction** of the Supino Partizione Antica fund provided from the Federico Zeri Foundation: the original data counted 3.260 records for photographs and 2.634 records for depicted works of art. <br>\nThe original data have been used for **illustrative and didactical purposes only**: all the credits and reuse authorizations must be asked to <a href=\"mailto:fondazionezeri.fototeca@unibo.it\">Federico Zeri Foundation</a>.\n\n**1.1 Creation of the F and OA complexive xml files**\n\nTo allow a better management and manipulation, as well as to anonymize personal data, complexive files (via <a href=\"/content/sample_data/0_Creation_UniqeXML.xquery\" target=\"_blank\">0_Creation_UniqeXML.xquery</a> collection command) have been created and published. \nThey collect:\n\n*   all the single photograph xml files' records in the F_entries.xml file (data/0_source_data)\n*   all the single works of art xml files' records in the OA_entries.xml file (data/0_source_data)\n\n**1.2 Creation of the flat tabular dataset extracting relevant information for the project from the nested xml elements and attributes**\n\nDue to the hypernested and not consistently presence of elements at different levels, <pandas.read_xml> method was not effectively parsing what was needed.\nThe <xml.etree.ElementTree> library has then been preferred because it allows to call for single elements at different nesting levels. Nevertheless, this approach presents some drawbacks as the need of a previous and deep knowledge of the database structure that does not allow to uncover unexpected correlations possible through the exploration of a comprehensive dataset.\n\n**1.3 Preliminary installation**(Uncomment the first line to install the library)\n- libraries\n- imports:\n  - xml.etree.ElementTree, pandas, csv for managing the dataset\n  - ...\n","metadata":{"id":"TYn9-Vu-cI03"}},{"cell_type":"code","source":"#preliminary imports\n!pip install python-csv\n!pip install elementpath\nimport csv\nimport xml.etree.ElementTree as ET\n\n#function to have back the element texts\ndef extract_data(path):\n    if SCHEDA.find(path) != None:\n        name = SCHEDA.find(path).text\n    else:\n        name = None\n    return name","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"4ZW8b0GdYl6l","outputId":"ffd5c978-2c92-4c67-8174-b98bfd519316","trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting python-csv\n  Downloading python-csv-0.0.13.tar.gz (26 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting argparse\n  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\nRequirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-csv) (1.21.6)\nRequirement already satisfied: pandas in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-csv) (1.3.5)\nCollecting matplotlib\n  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting xlrd\n  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting xmltodict\n  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\nCollecting demjson\n  Downloading demjson-2.2.4.tar.gz (131 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.5/131.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[1 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m error in demjson setup command: use_2to3 is invalid.\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\nCollecting elementpath\n  Downloading elementpath-3.0.2-py3-none-any.whl (189 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: elementpath\nSuccessfully installed elementpath-3.0.2\n","output_type":"stream"}]},{"cell_type":"code","source":"#parse the complexive Fxml and OAxml files\nF_tree = ET.parse(\"data/0_source_data/F_entries.xml\")\nF_root = F_tree.getroot()\nF_root.attrib[\"test\"]\n\nOA_tree = ET.parse(\"data/0_source_data/OA_entries.xml\")\nOA_root = OA_tree.getroot()\nOA_root.attrib[\"test2\"]\n\n#set the colums' headers for the choosen elements\nheader = [\"sercdf_F_ser\", \"sercdoa_OA_ser\", \"INVN_F\", \"UBFC_Fshelfmark\", #ids\n          \"PVCS_OAcountry\", \"PVCC_OAtown\", \"LDCN_OArep\", \"PRVC_OAprev_town\", \"AUFI_Fatelier_address\", #places\n          \"AUFN_Faut\", \"SGLT_Ftitle\", \"SGTT_OAtitle\", \"AUTN_OAaut\", #authors/titles\n          \"OGTT_OAtype\", \"AUTB_Fsubj_main\", \"OGTDOA_OAsubj_sub\", #subjects\n          \"ROFI_Fneg\", \"BIBA_OAbib\",#external relations\n          \"OSS_Fnotes\", \"OSS_OAnotes\", #unstructured infos\n         \"FTAN_filename\", \"NCTN_F_entry\", \"NRSCHEDA_OA_entry\", #2ary ids\n        \"DTSI_OAdate\", \"LRD_Fshotdates\", \"DTSI_Fprintdates\", \"DTSF_Fprintdates\", \"AUFA_Faut_dates\"] #time\n\n#setting an empty list\ndata = []\n\n#iterate on F_entries - and on correspondig OA_entries -\n#for extracting elements texts, store them in a list and add it to the data\n#two fields from original data are futherly modify for our purposes\n\nfor SCHEDA in F_root.findall(\"SCHEDA\"):\n    oa_ser = SCHEDA.get(\"sercdoa\")\n    f_ser = SCHEDA.get(\"sercdf\")\n    inv = extract_data(\"./PARAGRAFO/INVN\")\n    container = extract_data(\"./PARAGRAFO/UBFT\")\n    shelf = extract_data(\"./PARAGRAFO/UBFC\")\n    title_f = extract_data(\"./PARAGRAFO/RIPETIZIONE/SGLT\")\n    aut_f = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUFN\") #the original data do not distinguish AUFN and AUFB for collective agents\n    aut_f_dates = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUFA\")#timespan of photographer's actvity\n    aut_f_addr = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUFI\")#place of photographer's actvity as reported in the photograph >AF of variants\n    aut_oa = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUTN\")\n    subj_main = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUTB\")\n    subj_sub = extract_data(\"./PARAGRAFO/RIPETIZIONE/OGTDOA\")\n    notes_f = extract_data(\"./PARAGRAFO/OSS\")\n    neg_num = extract_data(\"./PARAGRAFO/ROFI\")\n    f_entry = extract_data(\"./PARAGRAFO/NCTN\")\n    filename = extract_data(\"./PARAGRAFO/FTAN\")\n    shotdates = extract_data(\"./PARAGRAFO/LRD\")\n    if shotdates != None:\n        #reduce uncertainty: if /ante in field, put 1855 as conventional beginning date\n        #for collodium negatives (accordign to other Zeri cataloguing)\n        if \"/ante\" in shotdates:\n            shotdates.replace(\"/ante\", \"/ ante\")\n        if \"/ ante\" in shotdates:\n        #if re.match(\"/ante|\\/ ante\", shotdates):\n            shotdates = \"1855-\"+shotdates[:-6]\n    printdates_start = extract_data(\"./PARAGRAFO/DTSI\")\n    printdates_end = extract_data(\"./PARAGRAFO/DTSF\")\n\n    for SCHEDA in OA_root.findall(\"SCHEDA\"):\n        if SCHEDA.get(\"sercdoa\") == oa_ser:\n            title_oa = extract_data(\"./PARAGRAFO/SGTT\")\n            date_from_oa = extract_data(\"./PARAGRAFO/DTSI\")\n            date_to_oa = extract_data(\"./PARAGRAFO/DTSF\")\n            country_oa = extract_data(\"./PARAGRAFO/PVCS\") #Original data report just 2 LRCS: name of the country where the shot was taken.\n            town_oa = extract_data(\"./PARAGRAFO/PVCC\") #Original data report just 2 LRCC: name of the country where the shot was taken.\n            rep_oa = extract_data(\"./PARAGRAFO/LDCN\")\n            prev_town_oa = extract_data(\"./PARAGRAFO/RIPETIZIONE/PRVC\")\n            if prev_town_oa != None:\n                #save the previous locations only if in 1800-1899 timespan (PRDU) otherwise put \"NR\" (not relevant)\n                if extract_data(\"./PARAGRAFO/RIPETIZIONE/PRDU\") != None:\n                    if \"1799\" < extract_data(\"./PARAGRAFO/RIPETIZIONE/PRDU\") < \"1900\": #PRDU last date the OA was in that location\n                        prev_town_oa = prev_town_oa + \" | \" + str(extract_data(\"./PARAGRAFO/RIPETIZIONE/PRDU\"))\n                    else:\n                        prev_town_oa = \"NR\"\n            type_oa = extract_data(\"./PARAGRAFO/OGTT\")\n            notes_oa = extract_data(\"./PARAGRAFO/OSS\")\n            oa_entry = extract_data(\"./PARAGRAFO/NRSCHEDA\")\n            beg_date_oa = extract_data(\"./PARAGRAFO/DTSI\")\n            if extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBA\") != None:\n                #save the bib ref only if in 1800-1899 timespan (BIBD) otherwise put \"NR\" (not relevant)\n                if extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBD\") != None:\n                    if \"1799\" < extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBD\") < \"1900\": #PRDU last date the OA was in that location\n                        bib_oa = extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBA\")\n                        bib_oa = bib_oa + \" | \" + str(extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBD\"))\n                    else:\n                        bib_oa = \"NR\"\n            else:\n                bib_oa = None\n\n    row = [oa_ser, f_ser, inv, shelf,\n           country_oa, town_oa, rep_oa, prev_town_oa, aut_f_addr,\n           aut_f, title_f, title_oa, aut_oa,\n           type_oa, subj_main, subj_sub,\n           neg_num, bib_oa,\n           notes_f, notes_oa,\n           filename, f_entry, oa_entry,\n           beg_date_oa, shotdates, printdates_start, printdates_end, aut_f_dates]\n    data.append(row)\n\n#Write the data and their header in a new csv dataset\nwith open(\"data/F_OA_selected_data.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as tabular_data:\n    # create the csv writer\n    writer = csv.writer(tabular_data)\n    writer.writerow(header)\n    writer.writerows(data)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"id":"d_Z8m2qdjB8K","outputId":"fd84ccf1-9ca7-484c-e2db-bba17285e26a","trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!pip install pandas\nimport pandas as pd","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"f_L0Nt8bOhrU","outputId":"40c00cfb-a824-4acc-d8b4-291b0ea5c407","trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /srv/conda/envs/notebook/lib/python3.7/site-packages (1.3.5)\nRequirement already satisfied: pytz>=2017.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (2022.7)\nRequirement already satisfied: python-dateutil>=2.7.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: numpy>=1.17.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (1.21.6)\nRequirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"data_df = pd.read_csv('data/F_OA_selected_data.csv')\n\n#reduce the dataset to just the columns needed, the not-empty and not-duplicates rows\nOAnotes_df = data_df[[\"OSS_OAnotes\"]].dropna()\nOAnotes_df = OAnotes_df.drop_duplicates()\n\n#split multilines rows and once again remove duplicates rows\nOAnotes_df[\"OSS_OAnotes\"] = OAnotes_df[\"OSS_OAnotes\"].str.split(\"&#10;|\"\". Foto \", expand = False)\nOAnotes_df = OAnotes_df.explode(\"OSS_OAnotes\")\nOAnotes_df = OAnotes_df.drop_duplicates()\n\n#save just rows with transcriptions notes (including \"Foto sup \\d{1,4}\" string)\nOAnotes_df = OAnotes_df[OAnotes_df[\"OSS_OAnotes\"].str.contains(\"sup \\d{1,4}\")== True].reset_index(drop=True)\nOAnotes_df = OAnotes_df[OAnotes_df[\"OSS_OAnotes\"].str.startswith(\"La foto \")== False].reset_index(drop=True)\n\n#separe note texts from other infos and remove the column containing the whole infos, save and check the result\nOAnotes_df[[\"Inv\", \"Note\"]] = OAnotes_df[\"OSS_OAnotes\"].str.split(': \"', n=1, expand=True)\nOAnotes_df= OAnotes_df.drop(columns=[\"OSS_OAnotes\"]).reset_index(drop=True)\nprint(\"Photographs which annotations have been transcribed in OA entries: \", OAnotes_df.shape[0], \"(/over 3.222 photographs\") #1839\nprint(OAnotes_df.head(10))\nOAnotes_df.to_csv(\"data/1_working_data/1_OAnotes01.csv\", encoding=\"utf-8\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"f_L0Nt8bOhrU","outputId":"40c00cfb-a824-4acc-d8b4-291b0ea5c407","trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Photographs which annotations have been transcribed in OA entries:  1839 (/over 3.222 photographs\n                                                 Inv  \\\n0                  Foto sup 748, verso: nota anonima   \n1      Foto sup 763, verso: nota anonima manoscritta   \n2      Foto sup 982, verso: nota anonima manoscritta   \n3  Foto sup 893, verso: nota anonima manoscritta:...   \n4      Foto sup 988, verso: nota anonima manoscritta   \n5      Foto sup 991, verso: nota anonima manoscritta   \n6      Foto sup 998, verso: nota manoscritta anonima   \n7     Foto sup 1000, verso: nota anonima manoscritta   \n8     Foto sup 1002, verso: nota anonima manoscritta   \n9     Foto sup 1003, verso: nota anonima manoscritta   \n\n                                                Note  \n0  Near Avezzano and not far from Tagliacozzo. He...  \n1  Aquila. S. Maria di Collemaggio. Founded by Pi...  \n2  The pulpit of San Giovanni del Toro, of the mi...  \n3                                               None  \n4  Here also his buried Sibylla of Burgundy. \"Rex...  \n5  Queen Margherita widow of Carlo III, who died ...  \n6  Piissimi Patris Nicolai Piscicelli optimi pres...  \n7  Rude sarcophagus in the porch of the church. T...  \n8  Amalfi. This campanile is said to date from 11...  \n9  Cloister of the Canonica founded in 1213 by Ca...  \n","output_type":"stream"}]},{"cell_type":"code","source":"#manual checking and adjusting for 1)\"manoscritta:\">\"manoscritta\",\" 2)\"\".\",>\"\".\" 3)\\n\",>\"\n#inv: 943, 75, 559, 1635, 1648, 1702, (1768 non riporta), 1787, 2397, 2789, 2869, 2849,\n# 2984, 2222,2270,2880, saved in data\\OAnotes02.csv\n\n#open the manually modified dataframe, search for unsuseful informations in 'Inv' and eliminate them\nOAnotes2_df = pd.read_csv('data/1_working_data/1_OAnotes02.csv', encoding=\"utf-8\").dropna(subset=['Inv']).reset_index(drop=True)\npattern = 'Foto |, (.+)'\nOAnotes2_df[\"Inv\"] = OAnotes2_df[\"Inv\"].replace(to_replace=pattern, value='', regex=True).reset_index(drop=True)\n\n#check and save the third version of OAnotes_df\nprint(OAnotes2_df.head(15))\nOAnotes2_df.to_csv(\"data/1_working_data/1_OAnotes03.csv\", encoding=\"utf-8\")\n\n#check how many of them are incomplete\nannotations_incompleted = OAnotes2_df[OAnotes2_df['Note'].str.contains(\"[...]\")== True].reset_index(drop=True)\nprint(\"Photographs which transcribed annotations are likely to be incomplete: \", annotations_incompleted.shape[0], \"(/over \",OAnotes_df.shape[0],\" transcribed)\")\n\n#create the corpus to be passed with spacy\ncorpus = \"\"\nfor OAnote in OAnotes2_df[\"Note\"]:\n    corpus = corpus+\"---\"+str(OAnote)+\"---\\n\"\nwith open(\"data/OAnotes_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n        f.write(corpus)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":276},"id":"tNX-1to7PfDw","outputId":"fb472629-61eb-4438-828b-8cd0b3848817","trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"    Unnamed: 0       Inv                                               Note\n0            0   sup 748  Near Avezzano and not far from Tagliacozzo. He...\n1            1   sup 763  Aquila. S. Maria di Collemaggio. Founded by Pi...\n2            2   sup 982  The pulpit of San Giovanni del Toro, of the mi...\n3            3   sup 893  Clara pudicicie dux Paulabianca potentis / A g...\n4            4   sup 988  Here also his buried Sibylla of Burgundy. \"Rex...\n5            5   sup 991  Queen Margherita widow of Carlo III, who died ...\n6            6   sup 998  Piissimi Patris Nicolai Piscicelli optimi pres...\n7            7  sup 1000  Rude sarcophagus in the porch of the church. T...\n8            8  sup 1002  Amalfi. This campanile is said to date from 11...\n9            9  sup 1003  Cloister of the Canonica founded in 1213 by Ca...\n10          10  sup 1012  In cloister of Amalfi Duomo. Sarc. of an archb...\n11          11  sup 1005  Amalfi. Cloister of San Francesco founded by t...\n12          12   sup 874  Urbino. / Palazzo Ducale, the old Montefeltro ...\n13          13   sup 882  Santa Casa. Loreto. / The Annunciation by the ...\n14          14  sup 1007  The great Oscan godders of maternity. This of ...\nPhotographs which transcribed annotations are likely to be incomplete:  1830 (/over  1839  transcribed)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2. Data elaboration\n\n\n","metadata":{"id":"WvgsBKu0cKpF"}},{"cell_type":"markdown","source":"1. Work on photographers","metadata":{"id":"66Ejy3lGkspx"}},{"cell_type":"code","source":"!pip install SPARQLWrapper\n!pip install geopy\nfrom csv import DictReader\nfrom SPARQLWrapper import SPARQLWrapper, JSON\nimport pandas as pd\nimport ssl\nfrom geopy.geocoders import Nominatim","metadata":{"id":"DRaKAR62kXIV","trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting SPARQLWrapper\n  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\nCollecting rdflib>=6.1.1\n  Downloading rdflib-6.2.0-py3-none-any.whl (500 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.3/500.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting isodate\n  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.7/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (65.6.3)\nCollecting pyparsing\n  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: importlib-metadata in /srv/conda/envs/notebook/lib/python3.7/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (4.11.4)\nRequirement already satisfied: typing-extensions>=3.6.4 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata->rdflib>=6.1.1->SPARQLWrapper) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata->rdflib>=6.1.1->SPARQLWrapper) (3.11.0)\nRequirement already satisfied: six in /srv/conda/envs/notebook/lib/python3.7/site-packages (from isodate->rdflib>=6.1.1->SPARQLWrapper) (1.16.0)\nInstalling collected packages: pyparsing, isodate, rdflib, SPARQLWrapper\nSuccessfully installed SPARQLWrapper-2.0.0 isodate-0.6.1 pyparsing-3.0.9 rdflib-6.2.0\nCollecting geopy\n  Downloading geopy-2.3.0-py3-none-any.whl (119 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting geographiclib<3,>=1.52\n  Downloading geographiclib-2.0-py3-none-any.whl (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: geographiclib, geopy\nSuccessfully installed geographiclib-2.0 geopy-2.3.0\n","output_type":"stream"}]},{"cell_type":"code","source":"ssl._create_default_https_context = ssl._create_unverified_context\ngeolocator = Nominatim(timeout=10, user_agent=\"myGeolocator\")","metadata":{"id":"XuizLixzktEN","trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# functions\n# define a function to open file in reading mode\ndef process_csv(data_file_path):\n    import csv\n    source = open(data_file_path, mode=\"r\", encoding=\"UTF8\")\n    source_reader = csv.DictReader(source)\n    source_data = list(source_reader)\n    return source_data\n\n#define a function for transforming lists of elements in strings\ndef write_string(source, output_txt_name):\n    string = \"\"\n    for source_data in source:\n        string = string+source_data+\"|\"\n    string = string[:-1]\n    with open(output_txt_name, \"w\", encoding=\"utf8\") as f:\n        f.write(string)\n    return string\n\n#define a function to query endpoints\ndef query_endpoint(endpoint_url, SPRQL_query):\n    get_endpoint = endpoint_url\n    sparql_w = SPARQLWrapper(get_endpoint)\n    sparql_w.setQuery(SPRQL_query)\n    sparql_w.setReturnFormat(JSON)\n    spqrl_w_res = sparql_w.query().convert()\n    return spqrl_w_res\n\n#define a function to manipulate results and have back 1. a set of wd_URI corresponding to our wd_names,\n# 2. update of ph_matrix, 3. not matched wd_names\n\ndef manipulate(spqrl_w_res, dataset_to_enhance, resNF_txt_name, resF_txt_name):\n    res_all = set()\n    res_dic = {}\n    res_NF_tem = set()\n    res_F = set()\n    for res in spqrl_w_res[\"results\"][\"bindings\"]:\n        res_all.add((res[\"fLabel\"][\"value\"], res[\"f\"][\"value\"]))\n        for datum in dataset_to_enhance:\n            if datum[\"ph_wd_URI\"]:\n                continue\n            else:\n                if datum[\"ph_wd_name\"] not in res_dic:\n                    if res[\"fLabel\"][\"value\"] == datum[\"ph_wd_name\"]:\n                        res_F.add(res[\"f\"][\"value\"])\n                        new_pairs = {\"ph_wd_URI\": res[\"f\"][\"value\"]}\n                        res_dic.update({datum[\"ph_wd_name\"]: new_pairs})\n                        datum.update([(\"ph_wd_URI\", res[\"f\"][\"value\"])])\n                    else:\n                        res_NF_tem.add(datum[\"ph_wd_name\"])\n    res_NF_def = res_NF_tem - set(list(res_dic.keys()))\n    with open(resNF_txt_name, \"w\", encoding=\"utf8\") as f:\n        f.write(str(res_NF_def))\n    with open(resF_txt_name, \"w\", encoding=\"utf8\") as f:\n        f.write(str(res_F))\n    print(\"labels matched: \", len(res_F))\n    print(\"labels not found2: \", len(res_NF_def))\n    return res_F, res_NF_def #, res_all","metadata":{"id":"PDzPD39uj7pn","trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n# open source data with pandas\ndata_df = pd.read_csv(\"data/F_OA_selected_data.csv\")\n\n#initialize a photograph's frequency dataframe\nph_freq = pd.DataFrame(data_df[\"AUFN_Faut\"].value_counts().reset_index().values, columns=[\"AUFN_Faut\", \"count\"])\n\n#extend dataframe colums to host next datas\nph_freq[\"ph_wd_name\"], ph_freq[\"ph_wd_URI\"], ph_freq[\"gender\"], ph_freq[\"workplace\"], ph_freq[\"lat\"], ph_freq[\"lon\"],\\\nph_freq[\"born\"], ph_freq[\"died\"], ph_freq[\"lat\"] = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]","metadata":{"id":"sz5v6r-4k7UO","trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#create the firts string for the SPARQL query by \n#normalizing (personal) names in form \"surname, name\" to \"name surname\" as in wikidata\n#and create a list of the modified names tobe added to the dataframe\n\nfirst_ph_names_string =\"\"\nph_wd_name_list = []\nfor ph in ph_freq.index:\n    ph_name = str(ph_freq[\"AUFN_Faut\"][ph])\n    # reverse only (personal) names in form \"surname, name\" > \"name surname\"\n    if \", \" in ph_name:\n        ph_split = ph_name.split(\", \")\n        ph_wd_name = ph_split[1] + \" \" + ph_split[0]\n    else:\n        ph_wd_name = ph_name\n    ph_wd_name_list.append(ph_wd_name)\n    first_ph_names_string = first_ph_names_string + ph_wd_name + \"|\"\nfirst_ph_names_string = first_ph_names_string[:-1]\n\n#show a sample of the string\nprint(first_ph_names_string[0:200])","metadata":{"id":"1XL0DDREpSb5","trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Anonimo|Fratelli Alinari|Romualdo Moscioni|Brogi|Giorgio Sommer|Jean Laurent|Incorpora|Giraudon|Paolo Lombardi|Naya|Carlo Baldassarre Simelli|Pietro Poppi|Séraphin-Médéric  Mieusement|Robert Rive|John\n","output_type":"stream"}]},{"cell_type":"code","source":"#add ph_wd_name_list to the dataframe and show a sample of the current dataframe\nph_freq[\"ph_wd_name\"] = ph_wd_name_list\nprint(ph_freq.head(10))\n#save the dataframe in a csv file and open it as a dictionary to iterate\nph_freq.to_csv(\"data/1_working_data/2_PH_freq_01.csv\", encoding=\"utf-8\")","metadata":{"id":"1XL0DDREpSb5","trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"            AUFN_Faut count         ph_wd_name ph_wd_URI gender workplace lat  \\\n0             Anonimo  1336            Anonimo                                  \n1   Alinari, Fratelli   556   Fratelli Alinari                                  \n2  Moscioni, Romualdo   159  Romualdo Moscioni                                  \n3               Brogi   158              Brogi                                  \n4     Sommer, Giorgio   147     Giorgio Sommer                                  \n5       Laurent, Jean    73       Jean Laurent                                  \n6           Incorpora    57          Incorpora                                  \n7            Giraudon    55           Giraudon                                  \n8     Lombardi, Paolo    53     Paolo Lombardi                                  \n9                Naya    48               Naya                                  \n\n  lon born died  \n0                \n1                \n2                \n3                \n4                \n5                \n6                \n7                \n8                \n9                \n","output_type":"stream"}]},{"cell_type":"code","source":"ph_matrix = process_csv(\"data/1_working_data/2_PH_freq_01.csv\")\n\n#prepare the first query string to collect wikidata URI\nfirst_ph_SPARQL_query = \"\"\"\nSELECT DISTINCT ?f ?fLabel\nWHERE\n{    { ?f wdt:P106 wd:Q33231 } UNION { ?f wdt:P31 wd:Q672070}. #P106_has_for_occupation wd:Q33231_photographer \n                                                                #P31_is instance wd:Q672070_studios\n    ?f rdfs:label ?fLabel.\n     FILTER regex(?fLabel, \\\" \"\"\"+first_ph_names_string+\"\"\" \\\")\n     FILTER(LANG(?fLabel) = \"en\").\n}\"\"\"\n\n#perform the first SPARQL query and result manipulation\nfirst_ph_wd_res = query_endpoint(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\", first_ph_SPARQL_query)\nfirst_ph_manipulate = manipulate(first_ph_wd_res, ph_matrix, \"ph_NF.txt\", \"ph_F.txt\")\nfirst_F_set = first_ph_manipulate[0]\nfirst_NF = first_ph_manipulate[1]","metadata":{"id":"3z0IeTINsOwE","trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"labels matched:  45\nlabels not found2:  67\n","output_type":"stream"}]},{"cell_type":"code","source":"#after revising first results, refine the unmatched labels\nnew_list = []\nfor ph_wd_NF in first_NF:\n    if \"  \" in ph_wd_NF:\n        ph_wd_new = ph_wd_NF.replace(\"  \", \" \")\n    elif \"Fratelli\" in ph_wd_NF:\n        ph_wd_new = ph_wd_NF.replace(\"Fratelli\", \"\")\n    elif \"&\" in ph_wd_NF:\n        ph_wd_new = ph_wd_NF.replace(\"&\", \"and\")\n    elif \"Brogi\" == ph_wd_NF:\n        ph_wd_new = \"Giacomo Brogi\"\n    elif \"Incorpora\" == ph_wd_NF:\n        ph_wd_new = \"Giuseppe Incorpora\"\n    elif \"Giraudon\" == ph_wd_NF:\n        ph_wd_new = \"Adolphe Giraudon\"\n    else:\n        continue\n    new_list.append(ph_wd_new)\n    for ph_data in ph_matrix:\n        if ph_data[\"ph_wd_name\"] == ph_wd_NF:\n            ph_data.update([(\"ph_wd_name\", ph_wd_new)])\n\n#from the new modified names, by using the function obtain a second string to query \nsecond_ph_string = write_string(new_list, \"ph_string2.txt\")","metadata":{"id":"PjIOKX7Yts7A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prepare the second query string to collect wikidata URI\nsecond_ph_SPARQL_query = \"\"\"\nSELECT DISTINCT ?f ?fLabel\nWHERE\n{    { ?f wdt:P106 wd:Q33231 } UNION { ?f wdt:P31 wd:Q672070}. #P106_has_for_occupation wd:Q33231_photographer \n                                                                #P31_is instance wd:Q672070_studios\n    ?f rdfs:label ?fLabel.\n     FILTER regex(?fLabel, \\\" \"\"\"+second_ph_string+\"\"\" \\\")\n     FILTER(LANG(?fLabel) = \"en\").\n}\n\"\"\"\n#perform the second SPARQL query and result manipulation\nsecond_ph_wd_res = query_endpoint(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\", second_ph_SPARQL_query)\nsecond_manipulate = manipulate(second_ph_wd_res, ph_matrix, \"ph_NF2\", \"ph_F2.txt\")\nsecond_F_set = second_manipulate[0]","metadata":{"id":"eZjQSnZsuRZX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#obtain the list of found wikidata URI\ncomplex_F_set = second_F_set.union(first_F_set)\n#print(complex_F_set)\n\n#prepare the thirtd string to be passed in SPARQL query\nthird_ph_string_URI =\"\"\nfor F_URI in complex_F_set:\n    third_ph_string_URI = third_ph_string_URI+\"<\"+F_URI+\">\"\n#with open(\"ph_URI.txt\", \"w\", encoding=\"utf8\") as f:\n#    f.write(stringURI)\n#print(stringURI)\n\n#third query\nthird_ph_SPARQL_query = \"\"\"\nSELECT DISTINCT ?ph ?genderLabel ?countryLabel ?birthyear ?deathyear\n    WHERE\n    { VALUES ?ph {\"\"\"+stringURI+\"\"\"} \n        ?ph rdfs:label ?phLabel;\n        wdt:P937 ?country; #P937_worklocation\n        #wdt:P27 ?citiz;        \n        wdt:P569 ?birth;\n        wdt:P570 ?death.\n        OPTIONAL {FILTER(LANG(?fLabel) = \"en\").\n                    ?ph wdt:P21 ?gender;\n                    #wdt:P937 ?worklocation; #P937_worklocation\n        }\n        BIND(year(?birth) AS ?birthyear)\n        BIND(year(?death) AS ?deathyear)\n\n        #BIND(COALESCE(?worklocation, ?citiz, \"NaN\") AS ?country).\n        #BIND(IF(BOUND(?worklocation),?worklocation,?citiz) AS ?country).\n    SERVICE wikibase:label {bd:serviceParam wikibase:language \"en\".}     \n    }\"\"\"\n#OPTIONAL { ?ph wdt:P569 ?birthdate;        wdt:P570 ?deathdate.} ci servono...\n\n#perform the third query\nthird_ph_wd_res = query_endpoint(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\", third_ph_SPARQL_query)\n\n#manipulate results\nwd_total_dic = {}\nfor result in third_ph_wd_res[\"results\"][\"bindings\"]:\n    item_key = result[\"ph\"][\"value\"]\n    item_value = {\"workplace\": result[\"countryLabel\"][\"value\"],\n                  \"born\": result[\"birthyear\"][\"value\"],\n                  \"died\": result[\"deathyear\"][\"value\"]}\n    if item_key not in wd_total_dic:\n        wd_total_dic.update({item_key: item_value})\n        for ph_data in ph_matrix:\n            if ph_data[\"ph_wd_URI\"] == item_key:\n                item2=item_value.items()\n                ph_data.update(item2)\n\nwd_total_list = list(wd_total_dic.values())\nprint(wd_total_list)\n\nkeys = wd_total_list[0].keys()\nwith open(\"ph_wd_total.csv\", \"w\", encoding=\"UTF8\", newline=\"\") as output_file:\n    dict_writer = csv.DictWriter(output_file, keys)\n    dict_writer.writeheader()\n    dict_writer.writerows(wd_total_list)","metadata":{"id":"RsSNGDE8v1py"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Work on places","metadata":{"id":"0xSgal3tQruP"}},{"cell_type":"code","source":"#define function to store lat-lon from a list of places\ndef get_coordinates(list, df):\n    for place in list:\n        if place not in df[\"place\"].unique().tolist():\n            if geolocator.geocode(place) != None:\n                lon = geolocator.geocode(place).longitude\n                lat = geolocator.geocode(place).latitude\n            else:\n                lon = \"NaN\"\n                lat = \"NaN\"\n            new_place = [place, lat, lon]\n            df.loc[len(df)] = new_place\n        df.to_csv(\"data\\places_coordinates.csv\", encoding=\"UTF-8\")","metadata":{"id":"Z_E9BD3XQrSn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#open the saved file\ndata_df = pd.read_csv(\"data\\F_OA_selected_data.csv\")\nph_freq_df = pd.read_csv(\"data\\ph_freq2.csv\")\n\n#reduce columns and change column name, check first rows\nplaces_F = ph_freq_df[['workplace', \"lat\", \"lon\"]].dropna()\nplaces = places_F.rename(columns={\"workplace\": \"place\"})\nplaces.head()\n\n#extract towns and country unique names from original dataframe\ntowns_OA = data_df['PVCC_OAtown'].unique().tolist()\ncountries_OA = data_df['PVCS_OAcountry'].unique().tolist()\n\n#obtain coordinates from the two list and store them in a df\nget_coordinates(countries_OA, places)\nget_coordinates(towns_OA, places)","metadata":{"id":"Lnd5nBkERNH5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. Work on Annotations","metadata":{"id":"8ZRD3mhvZ1z3"}},{"cell_type":"code","source":"#!pip install spacy\nimport spacy\nfrom spacy.matcher import Matcher\nfrom spacy.attrs import POS","metadata":{"id":"3qzXjNVGbnY6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#open the file with annotations texts\nwith open(\"data/OAnotes_corpus.txt\", mode=\"r\") as f:\n    contents = f.read()\n \n#load the nlp model and look for {\"LEMMA\": \"I\"}, {POS: 'VERB'} pattern (first person verbs)\nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = Matcher(nlp.vocab)\nmatcher.add(\"PA_creator\", [[{\"LEMMA\": \"I\"}, {POS: 'VERB'}]])\ndoc = nlp(contents)\nmatches = matcher(doc)\n\nmatched = []\nfor match_id,start,end in matches:\n    I_verb = str(doc[start:end])\n    matched.append(I_verb)\nprint(matched)\n#I doubt, I was, I think(3), I discovered, I have(4), I saw(4), I AM(2?), I told, I respected, I farn, me look,\n# I believe, I put","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"cOWvR2Z3Z62_","outputId":"c55a54a4-08ee-4072-8021-4bad942ea9ce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a dataframe from list of matched and check occurrencies\nmatched_df = pd.DataFrame()\nmatched_df[\"Match\"] = matched\nmatched_freq = pd.DataFrame(matched_df[\"Match\"].value_counts().reset_index().values, columns=[\"Match\", \"count\"])\nprint(matched_freq.head(25))","metadata":{"id":"WLMGHtjXcNko"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data visualization","metadata":{"id":"sxPYRxMacLDt"}},{"cell_type":"markdown","source":"\n---\n**Analyse**\npandas library in order to examine our data.\n     \n       \n    1. Data Preparation:\n          - creation of two complexive xml files for F and OA records coming from the Federico Zeri Foundation catalogues\n          - extraction from nested xml stucture of relevant information for the project and structuring them in plain tabular format\n    2. Data Elaboration: seeking for furter analysis elements via:\n          - deeper work on photographer for enhance their information\n          - deeper work on places\n          - work on unstructured annotations: NER\n     2. Data Visualization\n","metadata":{"id":"rSDRDeMOjB8S"}},{"cell_type":"markdown","source":"0. Data overview","metadata":{"id":"7U57aNxDdeOH"}},{"cell_type":"code","source":"import pandas as pd\nimport pandas_profiling as pp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pprint\npp = pprint.PrettyPrinter(indent=1)","metadata":{"id":"pgU6VKzNdtTp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# parse the csv into a dataframe\ndata_df = pd.read_csv('data\\F_OA_selected_data.csv')\n\n# reduce the dataset to just the columns needed\ndata_df = data_df[['INVN_F', 'PVCS_OAcountry', 'PVCC_OAtown', 'LDCN_OArep', 'PRVC_OAprev_town',\n          'AUFN_Faut', 'LRD_Fdates', 'OGTT_OAtype', 'AUTB_Fsubj_main', 'OGTDOA_OAsubj_sub', 'AUTN_OAaut']]\nprint(data_df.head(15))","metadata":{"id":"Kk-RBvonXIBI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import pandas_profiling as pp\nreport = pp.ProfileReport(data_df, title=\"Partizione Antica Fund - overview\")\nreport.to_file(\"ProfileReport_sup.html\")","metadata":{"id":"CDgOvQUheO2B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169},"id":"7Hb6VMBkf7LJ","outputId":"8a7d3687-1578-40fe-99da-12a242f965f6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1.1 Works of art - typology proportions pie","metadata":{"id":"JtgJeWIngJcr"}},{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px\n\n# filter cities and number of photos from data\ndata = pd.read_csv(\"data/F_OA_selected_data.csv\", encoding=\"UTF-8\")\nOAt_df = pd.DataFrame(data[\"OGTT_OAtype\"].value_counts().reset_index().values, columns=[\"OGTT_OAtype\", \"count\"])\n\nfig = px.pie(OAt_df, values='count', names=\"OGTT_OAtype\",\n            title='OA typologies',\n            color_discrete_sequence=px.colors.sequential.RdBu,\n            labels = OAt_df['OGTT_OAtype'], hover_name = 'OGTT_OAtype',\n            hover_data = {'OGTT_OAtype':False}\n            )\nfig.show()\nfig.write_html(\"data/2_data_viz/1.1.html\")","metadata":{"id":"saYsZpZ2gI6x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1.2 Works of art - countries proportions pie\n\n","metadata":{"id":"7rauFSANgaYi"}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nimport plotly\nimport plotly.graph_objects as go\nimport csv\n\n# filter cities and number of photos from data\ndata = pd.read_csv(\"UNIQUE_new2.csv\", encoding=\"utf8\")\nsup_df = data[['PVCS_OAcountry', 'PVCC_OAtown', 'LDCN_OArep']]\n\n#create a pie chart of supposed OA country\ndf_data = pd.read_csv(\"OAcountry_freq.csv\", encoding=\"utf8\")\ndf_data.loc[df_data['count'] < 10, 'PVCS_OAcountry'] = 'Other countries' # Represent only large countries\n#df = px.df_data()\nfig = px.pie(df_data, values='count', names=\"PVCS_OAcountry\",\n            title='Depicted OA for country',\n            color_discrete_sequence=px.colors.sequential.RdBu,\n            labels = df_data['PVCS_OAcountry'], hover_name = 'PVCS_OAcountry',\n            hover_data = {'PVCS_OAcountry':False, 'lat':False, 'lon': False}\n            )\nfig.show()\nfig.write_html(\"data/2_data_viz/1.2.html\")","metadata":{"id":"g_EONtm7gaIE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1.3 Works of art - countries proportions map","metadata":{"id":"nYz-E7P8g7fc"}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom geopy.geocoders import Nominatim\ngeolocator = Nominatim(timeout=10, user_agent = \"myGeolocator\")\n\nimport plotly\nimport plotly.graph_objects as go\nimport csv\n\n# filter cities and number of photos from data\ndf_data = pd.read_csv(\"OAcountry_freq.csv\", encoding=\"utf8\")\nph_geol = px.scatter_mapbox(df_data, lon=df_data['lon'],\n                            lat=df_data['lat'], size=df_data[\"count\"], zoom=2, color=df_data['PVCS_OAcountry'],\n                            color_continuous_scale=px.colors.cyclical.Twilight,\n                            #color_discrete_sequence=px.colors.sequential.RdBu,\n                            title=\"Depicted OA\",\n                            size_max=80,\n                            labels=df_data['PVCS_OAcountry'], hover_name='PVCS_OAcountry',\n                            hover_data={'PVCS_OAcountry':False, 'lat':False, 'lon':False})\n\n# mapbox style\nph_geol.update_layout(mapbox_style='carto-positron')\nph_geol.show()\nph_geol.write_html(\"data/2_data_viz/1.3.html\")","metadata":{"id":"Rea-Mj6qg3II"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.1 Photograps - photographers proportions pie /distribution barchart","metadata":{"id":"iifJu-PthWVN"}},{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px\n\ndf_data2 = pd.read_csv(\"ph_newfreq.csv\", encoding=\"utf8\")\ndf_data2 = df_data2[df_data2[\"count\"]>=20]\n#df_data2.loc[df_data2['count'] < 10, 'AUFN_Faut'] = 'Other photographs' # Represent only large countries\n#df = px.df_data()\nfig2 = px.pie(df_data2, values='count', names=\"AUFN_Faut\",\n            title='Photographs (>=20) for photographer',\n            color_discrete_sequence=px.colors.qualitative.Dark24, #color_discrete_sequence/color_continuous_scale =px.colors.sequential.RdBu,\n            labels = df_data2['AUFN_Faut'], hover_name='AUFN_Faut',\n            hover_data = {'AUFN_Faut':False, 'workplace':True}\n            #sistema le caselle non piene di ph_freq etc penso fillna()\n            )\nfig2.show()\nfig2.write_html(\"data/2_data_viz/2.1.html\")","metadata":{"id":"RrH1jF4rhWHG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.2 Photographs - map distribution based on ateliers locations","metadata":{"id":"pHJdaH1PiEYU"}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n#to show up everything directly in jupyter notebook\n\nimport plotly\nimport plotly.graph_objects as go\nimport csv\n\n#open the whole dataset and select just the AUFN_FAUT column\ndata = pd.read_csv(\"data\\F_OA_selected_data.csv\", encoding=\"UTF-8\")\ndata_d = data[[\"AUFN_Faut\"]]\nprint(len(data_d))\n\n#open the ph_freq dataset and select just the AUFN_FAUT and the workplace column; create a second df on workplace lat and lon\ndata_ph = pd.read_csv(\"data\\ph_freq.csv\", encoding=\"UTF-8\")\ndata_ph_df = data_ph[[\"AUFN_Faut\", \"workplace\"]]\ndata_lat_lon = data_ph[[\"workplace\", \"lat\", \"lon\"]].drop_duplicates(subset=['workplace'])\n\n#merge the two dataset on the AUFN_Faut column\ndata_comp = data_d.merge(data_ph_df, how='left', on=\"AUFN_Faut\").reset_index(drop=True)\nprint(len(data_comp))\nprint(data_comp.head(25))\n\n#count the values on workplace column\ndata_count = pd.DataFrame(data_comp['workplace'].value_counts().reset_index().values, columns=['workplace', 'count'])\nprint(len(data_count))\nprint(data_count.head(25))\n\n#add lat and lon data to the previous df\ndata_df = data_count.merge(data_lat_lon, how='left', on=\"workplace\").reset_index(drop=True)\ndata_df['count'] = pd.to_numeric(data_df['count'])\nprint(len(data_df))\nprint(data_df.head(25))\n\nph_geol = px.scatter_mapbox(data_df, lon=data_df['lon'], lat=data_df['lat'],\n                            size=data_df['count'], zoom=3, color=data_df['workplace'],\n                            size_max=80,\n                            color_discrete_sequence=px.colors.sequential.RdBu,\n                            title=\"Photohgrapher ateliers\",\n                            labels=data_df['workplace'], hover_name=\"workplace\")\n\n# mapbox style\nph_geol.update_layout(mapbox_style='carto-positron')\nph_geol.show()\nph_geol.write_html(\"data/2_data_viz/2.2.html\")","metadata":{"id":"t3ndEp0MiEiX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.3 Photographs - map distribution of anonimous photographs based on place of shooting (limited to immobles)","metadata":{"id":"nfzogXtzioNa"}},{"cell_type":"code","source":"#!pip install regex\nimport pandas as pd\nimport plotly.express as px\nimport regex as re\n\ndata = pd.read_csv(\"UNIQUE_new2.csv\", encoding=\"UTF-8\")\n\n#reduce to only columns rows needed and check\nsup_df = data[['OGTT_OAtype', 'PVCS_OAcountry', 'PVCC_OAtown', 'AUFN_Faut']]\nsup_df = sup_df[sup_df.AUFN_Faut == \"Anonimo\"]\n\ndf_data3 = pd.DataFrame(sup_df[\"PVCS_OAcountry\"].value_counts().reset_index().values, columns=[\"PVCS_OAcountry\", \"count\"])\n\nfig3 = px.pie(df_data3, values='count', names=\"PVCS_OAcountry\",\n              title='Anonimous Photographs (1331/3111) for OAcountry',\n              color_discrete_sequence=px.colors.sequential.Brwnyl,\n              labels = df_data3['PVCS_OAcountry'], hover_name='PVCS_OAcountry',\n              hover_data = {'PVCS_OAcountry':False}\n              )\n\nfig3.update_layout(\n    font = dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        )\n    )\nfig3.show()\n\nsup_df= sup_df[sup_df['OGTT_OAtype'].str.contains(\"architettura|architettura\\ scultura|complesso archeologico|sito archeologico\")== True].reset_index(drop=True)\nsup_df.to_csv(\"daje.csv\")\nprint(sup_df.tail(200))\ndf_data4 = pd.DataFrame(sup_df[\"PVCS_OAcountry\"].value_counts().reset_index().values, columns=[\"PVCS_OAcountry\", \"count\"])\nfig4 = px.pie(df_data4, values='count', names=\"PVCS_OAcountry\",\n              title='Anonimous Photographs (1331/3111) for OAcountry of immobles',\n              color_discrete_sequence=px.colors.sequential.Brwnyl, #px.colors.sequential.RdBu https://plotly.com/python/discrete-color/\n              labels = df_data3['PVCS_OAcountry'], hover_name='PVCS_OAcountry',\n              hover_data = {'PVCS_OAcountry':False}  #sistema le caselle non piene di ph_freq etc penso fillna()\n              )\n\nfig4.update_layout(\n    font = dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        )\n    )\nfig4.show()\nfig4.write_html(\"data/2_data_viz/2.3.html\")","metadata":{"id":"VxUk_S0BioDO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3.1 Annotations - complete/incomplete/missing transcriptions proportions pie","metadata":{"id":"RVfqj_usjf1s"}},{"cell_type":"code","source":"import plotly.express as px\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n#open the needed dataframes\ndata_df = pd.read_csv('data\\F_OA_selected_data.csv')\nall_inv = data_df[[\"INVN_F\"]].rename(columns={\"INVN_F\": \"Inv\"}).reset_index(drop=True)\nOAnotes_df = pd.read_csv('data/1_working_data/OAnotes03.csv')\n\n#define a function to check status of transcriptions\ndef check(row):\n    if \"...\" in str(row[\"Note\"]):\n        status = \"incomplete\"\n    else:\n        status = \"complete\"\n    return status\n\n#apply the function to df and add a status column, check it \nOAnotes_df[\"status\"] = OAnotes_df.apply(check, axis=1)\nprint(OAnotes_df.head(25))\n\n#merge the df with all the inventories to check for missing transcriptions and count according to the status \nmerged = all_inv.merge(OAnotes_df, how='left', on=\"Inv\").reset_index(drop=True)\nnew=pd.DataFrame(merged[\"status\"].value_counts(dropna=False).reset_index().values, columns=[\"status\", \"count\"])\n#change the empty rows with \"missing\" and check\nnew['status'] = new['status'].fillna('missing')\nprint(new.tail(50))\n\nfig = px.pie(new, values='count', names=\"status\",\n              title='Annotations on photographs',\n              color_discrete_sequence=px.colors.sequential.Brwnyl, #px.colors.sequential.RdBu https://plotly.com/python/discrete-color/\n              labels = new['status'], hover_name='status',\n              hover_data = {'status':True}  #sistema le caselle non piene di ph_freq etc penso fillna())\n             )\n\nfig.update_layout(\n    font = dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        )\n    )\nfig.show()\nfig.write_html(\"data/2_data_viz/3.1.html\")","metadata":{"id":"g634EN9LjfuH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3.2 Annotations","metadata":{"id":"-X5aqyMQlSii"}},{"cell_type":"code","source":"","metadata":{"id":"1ldFH4F4liEi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3.3 Annotations and metadata: compared dates distribution","metadata":{"id":"gEWFNsy5liLo"}},{"cell_type":"code","source":"","metadata":{"id":"BD4odwiSluHA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4.1 Annotations - time-place pairs related to ...","metadata":{"id":"qI4y50zSluSH"}},{"cell_type":"code","source":"import plotly.express as px\nimport pandas as pd\n\n#open and merge data from annotations reporting time-place pairs and place coordinates, check for it\nmovement_df = pd.read_csv(\"data/1_working_data/OAnotes05bis.csv\", encoding=\"utf8\")\nplaces_df = pd.read_csv(\"data/1_working_data/places_coordinates2.csv\", encoding=\"utf8\")\nmovement_df_coor = movement_df.merge(places_df, how='left', on=\"place\")\nmovement_df_coor.drop('Unnamed: 0', axis=1, inplace=True)\nprint(movement_df_coor.head(10))\n\n#break the texts at about 30 characters to let them better visualized\nmovement_df_coor['Note_br'] = movement_df_coor.apply(lambda row: ('<br>'.join(str(row.Note)[i:i+30] for i in range(0, len(str(row.Note)), 30))), axis = 1)\n\n#set scatter with d\nfig = px.scatter_geo(movement_df_coor, color=\"date\",\n                  lat=movement_df_coor[\"lat\"].values.tolist(),\n                  lon=movement_df_coor[\"lon\"].values.tolist(),\n                  title=\"Movements\", size=\"date\",\n                  projection=\"natural earth\", scope=\"europe\",\n            labels = movement_df_coor['place'], hover_name='place',\n            hover_data = {'place':False, 'Inv':True, 'Note_br':True}\n            )\nfig.show()\nfig.write_html(\"data/2_data_viz/4.1.html\")","metadata":{"id":"2x3WJ6drlSYT"},"execution_count":null,"outputs":[]}]}