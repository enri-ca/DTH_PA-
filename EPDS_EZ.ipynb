{
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Intro\n",
        "\n",
        "This Jupyter Notebook has been created for the <a href=\"https://www.unibo.it/it/didattica/insegnamenti/insegnamento/2021/443749\" target=\"_blank\">90154 - Electronic Publishing and Digital Storytelling</a> course, taught by **Prof. Marilena Daquino**, in the framework of the 2nd year of the <a href=\"https://corsi.unibo.it/2cycle/DigitalHumanitiesKnowledge\" target=\"_blank\">DHDK Master Degree</a>, a.a. 2021-22.<br>\n",
        "Here listed the main steps for the realization of the project **Partizione Antica**:     \n",
        "       \n",
        "    1. Data Preparation:\n",
        "          - creation of two complexive xml files for F and OA records coming from the Federico Zeri Foundation catalogues\n",
        "          - extraction of relevant information for the project from nested xml stucture and structuring them in plain tabular format\n",
        "          - extraction from previpus tabular data of unstructured annotations\n",
        "    2. Data Elaboration: seeking for furter analysis elements via:\n",
        "          - deeper work on photographer for enhancing their information (workplace, timespan of activity, etc.)\n",
        "          - deeper work on places for enhancing their gelocation\n",
        "          - work on unstructured annotations trough NLP and NER\n",
        "    3. Data Visualization\n",
        "    - \n",
        "    -\n",
        "    -"
      ],
      "metadata": {
        "id": "D76tTDYyH-z9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data preparation\n",
        "\n",
        "This research started from a **record data extraction** of the Supino Partizione Antica fund provided from the Federico Zeri Foundation: the original data counted 3.260 records for photographs and 2.634 records for depicted works of art. <br>\n",
        "The original data have been used for **illustrative and didactical purposes only**: all the credits and reuse authorizations must be asked to <a href=\"mailto:fondazionezeri.fototeca@unibo.it\">Federico Zeri Foundation</a>.\n",
        "\n",
        "**1.1 Creation of the F and OA complexive xml files**\n",
        "\n",
        "To allow a better management and manipulation, as well as to anonymize personal data, complexive files (via <a href=\"/content/sample_data/0_Creation_UniqeXML.xquery\" target=\"_blank\">0_Creation_UniqeXML.xquery</a> collection command) have been created and published. \n",
        "They collect:\n",
        "\n",
        "*   all the single photograph xml files' records in the F_entries.xml file (data/0_source_data)\n",
        "*   all the single works of art xml files' records in the OA_entries.xml file (data/0_source_data)\n",
        "\n",
        "**1.2 Creation of the flat tabular dataset extracting relevant information for the project from the nested xml elements and attributes**\n",
        "\n",
        "Due to the hypernested and not consistently presence of elements at different levels, <pandas.read_xml> method was not effectively parsing what was needed.\n",
        "The <xml.etree.ElementTree> library has then been preferred because it allows to call for single elements at different nesting levels. Nevertheless, this approach presents some drawbacks as the need of a previous and deep knowledge of the database structure that does not allow to uncover unexpected correlations possible through the exploration of a comprehensive dataset.\n",
        "\n",
        "**1.3 Preliminary installation**(Uncomment the first line to install the library)\n",
        "- libraries\n",
        "- imports:\n",
        "  - xml.etree.ElementTree, pandas, csv for managing the dataset\n",
        "  - ...\n"
      ],
      "metadata": {
        "id": "TYn9-Vu-cI03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preliminary imports\n",
        "!pip install python-csv\n",
        "!pip install elementpath\n",
        "import csv\n",
        "#from csv import DictReader\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "#function to have back the text element required if present and join texts if repeated\n",
        "def extract_data(path):\n",
        "    if \"RIPETIZIONE\" not in path:\n",
        "        if SCHEDA.find(path) != None:\n",
        "             name = SCHEDA.find(path).text\n",
        "        else:\n",
        "            name = None\n",
        "    else:\n",
        "        if SCHEDA.findall(path) != None:\n",
        "            name = \"\"\n",
        "            list_name = SCHEDA.findall(path)#.text)\n",
        "            for obj in list_name:\n",
        "                name = str(obj.text)+ \" | \"\n",
        "        else:\n",
        "            name = None\n",
        "    return name"
      ],
      "metadata": {
        "id": "4ZW8b0GdYl6l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ffd5c978-2c92-4c67-8174-b98bfd519316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-csv\n",
            "  Downloading python-csv-0.0.13.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting argparse\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from python-csv) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from python-csv) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from python-csv) (3.2.2)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.8/dist-packages (from python-csv) (1.2.0)\n",
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Collecting demjson\n",
            "  Downloading demjson-2.2.4.tar.gz (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.5/131.5 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting leven\n",
            "  Downloading leven-1.0.4.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jtutils\n",
            "  Downloading jtutils-0.0.8.tar.gz (7.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from jtutils->python-csv) (2.25.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from jtutils->python-csv) (4.6.3)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.7.2-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from leven->python-csv) (1.15.0)\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 KB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->python-csv) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->python-csv) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->python-csv) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->python-csv) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->python-csv) (2022.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->jtutils->python-csv) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->jtutils->python-csv) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->jtutils->python-csv) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->jtutils->python-csv) (2022.12.7)\n",
            "Collecting urllib3[socks]~=1.26\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 KB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting exceptiongroup>=1.0.0rc9\n",
            "  Downloading exceptiongroup-1.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium->jtutils->python-csv) (2.4.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium->jtutils->python-csv) (22.2.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from urllib3[socks]~=1.26->selenium->jtutils->python-csv) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: python-csv, demjson, jtutils, leven\n",
            "  Building wheel for python-csv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-csv: filename=python_csv-0.0.13-py3-none-any.whl size=34812 sha256=98a37625697b7deaf44838172aeae211d2a06a77220db7c3fefe08787e9f5740\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/6d/f9/3318b26ba626c29f7f21e0d83429d1dc719fcaaf63b9fb41c6\n",
            "  Building wheel for demjson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for demjson: filename=demjson-2.2.4-py3-none-any.whl size=73565 sha256=6f3c8a39bec34efc55faa2444d8277479abd03854984d41e78dc8003f18d5f9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/c3/a5/ad09d152d6b4b24c3f3b6fe2793092ac03e2521e08240d28cf\n",
            "  Building wheel for jtutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jtutils: filename=jtutils-0.0.8-py3-none-any.whl size=7902 sha256=420ef45c0b31009148fd9303b52b21d9d9ba8b380f94195fbe817f5384346fd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/72/88/d497f181662fef12b38d0e52a38d6abd265f857ada7befb097\n",
            "  Building wheel for leven (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for leven: filename=leven-1.0.4-cp38-cp38-linux_x86_64.whl size=67517 sha256=68b2fa530bc567626f94f2924edf4bdc6a57b5c4d55c8df51aa6aaf73f7265f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/22/72/cf8c57daff32b8a1b8c93cac1e74226afbc6364e695dc5644b\n",
            "Successfully built python-csv demjson jtutils leven\n",
            "Installing collected packages: nose, demjson, argparse, xmltodict, urllib3, sniffio, outcome, leven, h11, exceptiongroup, async-generator, wsproto, trio, trio-websocket, selenium, jtutils, python-csv\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed argparse-1.4.0 async-generator-1.10 demjson-2.2.4 exceptiongroup-1.1.0 h11-0.14.0 jtutils-0.0.8 leven-1.0.4 nose-1.3.7 outcome-1.2.0 python-csv-0.0.13 selenium-4.7.2 sniffio-1.3.0 trio-0.22.0 trio-websocket-0.9.2 urllib3-1.26.14 wsproto-1.2.0 xmltodict-0.13.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting elementpath\n",
            "  Downloading elementpath-3.0.2-py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: elementpath\n",
            "Successfully installed elementpath-3.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parse the complexive Fxml and OAxml files\n",
        "\n",
        "F_tree = ET.parse(\"data/0_source_data/F_entries.xml\")\n",
        "F_root = F_tree.getroot()\n",
        "F_root.attrib[\"test\"]\n",
        "\n",
        "OA_tree = ET.parse(\"data/0_source_data/OA_entries.xml\")\n",
        "OA_root = OA_tree.getroot()\n",
        "OA_root.attrib[\"test2\"]\n",
        "\n",
        "#set the colums' headers for the choosen elements\n",
        "header = [\"sercdf_F_ser\", \"sercdoa_OA_ser\", \"INVN_F\", \"UBFC_Fshelfmark\", #ids\n",
        "          \"PVCS_OAcountry\", \"PVCC_OAtown\", \"LDCN_OArep\", \"PRVC_OAprev_town\", \"AUFI_Fatelier_address\", #places\n",
        "          \"AUFN_Faut\", \"SGLT_Ftitle\", \"SGTT_OAtitle\", \"AUTN_OAaut\", #authors/titles\n",
        "          \"OGTT_OAtype\", \"AUTB_Fsubj_main\", \"OGTDOA_OAsubj_sub\", #subjects\n",
        "          \"ROFI_Fneg\", \"BIBA_OAbib\",#external relations\n",
        "          \"OSS_Fnotes\", \"OSS_OAnotes\", #unstructured infos\n",
        "         \"FTAN_filename\", \"NCTN_F_entry\", \"NRSCHEDA_OA_entry\", #2ary ids\n",
        "        \"DTSI_OAdate\", \"LRD_Fshotdates\", \"DTSI_Fprintdates\", \"DTSF_Fprintdates\", \"AUFA_Faut_dates\"] #time\n",
        "\n",
        "#setting an empty list\n",
        "data = []\n",
        "\n",
        "#iterate on UNIQUE_F SCHEDA - and on correspondig UNIQUE_OA SCHEDA - \n",
        "#for extracting elements texts, store them in a list and add it to the data\n",
        "#two fields from original data are futherly modify for our purposes\n",
        "\n",
        "for SCHEDA in F_root.findall(\"SCHEDA\"):\n",
        "    oa_ser = SCHEDA.get(\"sercdoa\")\n",
        "    f_ser = SCHEDA.get(\"sercdf\")\n",
        "    inv = extract_data(\"./PARAGRAFO/INVN\")\n",
        "    container = extract_data(\"./PARAGRAFO/UBFT\")\n",
        "    shelf = extract_data(\"./PARAGRAFO/UBFC\")\n",
        "    title_f = extract_data(\"./PARAGRAFO/RIPETIZIONE/SGLT\")\n",
        "    aut_f = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUFN\") #the original data do not distinguish AUFN and AUFB for collective agents\n",
        "    aut_f_dates = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUFA\")#timespan of photographer's actvity\n",
        "    aut_f_addr = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUFI\")#place of photographer's actvity as reported in the photograph >AF of variants\n",
        "    aut_oa = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUTN\")\n",
        "    subj_main = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUTB\")\n",
        "    subj_sub = extract_data(\"./PARAGRAFO/RIPETIZIONE/OGTDOA\")\n",
        "    notes_f = extract_data(\"./PARAGRAFO/OSS\")\n",
        "    neg_num = extract_data(\"./PARAGRAFO/ROFI\")\n",
        "    f_entry = extract_data(\"./PARAGRAFO/NCTN\")\n",
        "    filename = extract_data(\"./PARAGRAFO/FTAN\")\n",
        "    shotdates = extract_data(\"./PARAGRAFO/LRD\")\n",
        "    if shotdates != None:\n",
        "        #reduce uncertainty: if /ante in field, put 1826 as conventional beginning date\n",
        "        #first negative https://en.wikipedia.org/wiki/Negative_(photography)\n",
        "        if \"/ante\" in shotdates:\n",
        "            shotdates.replace(\"/ante\", \"/ ante\")\n",
        "        if \"/ ante\" in shotdates:\n",
        "            shotdates = \"1826-\"+shotdates[:-6]\n",
        "    printdates_start = extract_data(\"./PARAGRAFO/DTSI\")\n",
        "    printdates_end = extract_data(\"./PARAGRAFO/DTSF\")\n",
        "\n",
        "    for SCHEDA in OA_root.findall(\"SCHEDA\"):\n",
        "        if SCHEDA.get(\"sercdoa\") == oa_ser:\n",
        "            title_oa = extract_data(\"./PARAGRAFO/SGTT\")\n",
        "            date_from_oa = extract_data(\"./PARAGRAFO/DTSI\")\n",
        "            date_to_oa = extract_data(\"./PARAGRAFO/DTSF\")\n",
        "            country_oa = extract_data(\"./PARAGRAFO/PVCS\") #Original data report just 2 LRCS: name of the country where the shot was taken.\n",
        "            town_oa = extract_data(\"./PARAGRAFO/PVCC\") #Original data report just 2 LRCC: name of the country where the shot was taken.\n",
        "            rep_oa = extract_data(\"./PARAGRAFO/LDCN\")\n",
        "            prev_town_oa = extract_data(\"./PARAGRAFO/RIPETIZIONE/PRVC\")\n",
        "            if prev_town_oa != None:\n",
        "                #save the previous locations only if in 1800-1899 timespan (PRDU) otherwise put \"NR\" (not relevant)\n",
        "                if extract_data(\"./PARAGRAFO/RIPETIZIONE/PRDU\") != None:\n",
        "                    if \"1799\" < extract_data(\"./PARAGRAFO/RIPETIZIONE/PRDU\") < \"1900\": #PRDU last date the OA was in that location\n",
        "                        prev_town_oa = prev_town_oa + \" | \" + str(extract_data(\"./PARAGRAFO/RIPETIZIONE/PRDU\"))\n",
        "                    else:\n",
        "                        prev_town_oa = \"NR\"\n",
        "            type_oa = extract_data(\"./PARAGRAFO/OGTT\")\n",
        "            notes_oa = extract_data(\"./PARAGRAFO/OSS\")\n",
        "            oa_entry = extract_data(\"./PARAGRAFO/NRSCHEDA\")\n",
        "            beg_date_oa = extract_data(\"./PARAGRAFO/DTSI\")\n",
        "            if extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBA\") != None:\n",
        "                #save the previous locations only if in 1800-1899 timespan (PRDU) otherwise put \"NR\" (not relevant)\n",
        "                if extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBD\") != None:\n",
        "                    if \"1799\" < extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBD\") < \"1900\": #PRDU last date the OA was in that location\n",
        "                        bib_oa = extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBA\")\n",
        "                        bib_oa = bib_oa + \" | \" + str(extract_data(\"./PARAGRAFO/RIPETIZIONE/BIBD\"))\n",
        "                    else:\n",
        "                        bib_oa = \"NR\"\n",
        "            else:\n",
        "                bib_oa = None\n",
        "\n",
        "    row = [oa_ser, f_ser, inv, shelf,\n",
        "           country_oa, town_oa, rep_oa, prev_town_oa, aut_f_addr,\n",
        "           aut_f, title_f, title_oa, aut_oa,\n",
        "           type_oa, subj_main, subj_sub,\n",
        "           neg_num, bib_oa,\n",
        "           notes_f, notes_oa,\n",
        "           filename, f_entry, oa_entry,\n",
        "           beg_date_oa, shotdates, printdates_start, printdates_end, aut_f_dates]\n",
        "    data.append(row)\n",
        "\n",
        "#Write the data and their header in a new csv dataset\n",
        "with open(\"data/F_OA_selected_data.csv\", \"w\", encoding=\"UTF8\", newline=\"\") as tabular_data:\n",
        "    # create the csv writer\n",
        "    writer = csv.writer(tabular_data)\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(data)"
      ],
      "metadata": {
        "id": "d_Z8m2qdjB8K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "fd84ccf1-9ca7-484c-e2db-bba17285e26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-84c82309c9a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#parse the complexive Fxml and OAxml files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mF_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/0_source_data/F_entries.xml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mF_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mF_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrib\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, parser)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     \"\"\"\n\u001b[1;32m   1201\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, parser)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mclose_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mclose_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/0_source_data/F_entries.xml'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pEJ4cZ32j3of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_df = pd.read_csv('data/F_OA_selected_data.csv')\n",
        "\n",
        "#reduce the dataset to just the columns needed, the not-empty and not-duplicates rows\n",
        "OAnotes_df = data_df[[\"OSS_OAnotes\"]].dropna()\n",
        "OAnotes_df = OAnotes_df.drop_duplicates()\n",
        "\n",
        "#split multilines rows and once again remove duplicates rows\n",
        "OAnotes_df[\"OSS_OAnotes\"] = OAnotes_df[\"OSS_OAnotes\"].str.split(\"&#10;|\"\". Foto \", expand = False)\n",
        "OAnotes_df = OAnotes_df.explode(\"OSS_OAnotes\")\n",
        "OAnotes_df = OAnotes_df.drop_duplicates()\n",
        "\n",
        "#save just rows with transcriptions notes (including \"Foto sup \\d{1,4}\" string) and eliminate oher not pertinent (\"La foto\")\n",
        "OAnotes_df = OAnotes_df[OAnotes_df[\"OSS_OAnotes\"].str.contains(\"sup \\d{1,4}\")== True].reset_index(drop=True)\n",
        "OAnotes_df = OAnotes_df[OAnotes_df[\"OSS_OAnotes\"].str.startswith(\"La foto \")== False].reset_index(drop=True)\n",
        "\n",
        "#OAnotes_df.to_csv(\"OAnotes01.csv\", encoding=\"UTF-8\")\n",
        "print(\"Inventories with transcriptions: \", OAnotes_df.shape[0], \"(/over 3.222 photographs\") #1670\n",
        "\n",
        "#separe note texts from other infos and remove the column containing the source infos, save and check the result\n",
        "OAnotes_df[[\"Inv\", \"Note\"]] = OAnotes_df[\"OSS_OAnotes\"].str.split(': \"', n=1, expand=True)\n",
        "OAnotes_df= OAnotes_df.drop(columns=[\"OSS_OAnotes\"]).reset_index(drop=True)\n",
        "OAnotes_df.to_csv(\"data\\OAnotes01.csv\", encoding=\"utf-8\")\n",
        "print(OAnotes_df.head(40))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "f_L0Nt8bOhrU",
        "outputId": "40c00cfb-a824-4acc-d8b4-291b0ea5c407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d147292ae914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/F_OA_selected_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#reduce the dataset to just the columns needed, the not-empty and not-duplicates rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mOAnotes_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"OSS_OAnotes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/F_OA_selected_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#manual checking and adjusting for 1)\"manoscritta:\">\"manoscritta\",\" 2)\"\".\",>\"\".\" 3)\\n\",>\"\n",
        "#inv: 943, 75, 559, 1635, 1648, 1702, (1768 non riporta), 1787, 2397, 2789, 2869, 2849,\n",
        "# 2984, 2222,2270,2880, saved in data\\OAnotes02.csv\n",
        "\n",
        "#open the manually modified dataframe, search for unsuseful informations in 'Inv' and eliminate them\n",
        "OAnotes2_df = pd.read_csv('data/1_working_data/OAnotes02.csv', encoding=\"utf-8\").dropna(subset=['Inv']).reset_index(drop=True)\n",
        "pattern = 'Foto |, (.+)'\n",
        "OAnotes2_df[\"Inv\"] = OAnotes2_df[\"Inv\"].replace(to_replace=pattern, value='', regex=True).reset_index(drop=True)\n",
        "\n",
        "#check and save the third version of OAnotes_df\n",
        "print(OAnotes2_df.head(15))\n",
        "annotations_tot = OAnotes2_df.shape[0]\n",
        "print(\"Photographs which annotations have been transcribed in OA entries: \", annotations_tot, \"(/over 3222 photographs)\") #1670\n",
        "\n",
        "#check how many of them are incomplete\n",
        "annotations_incompleted = OAnotes2_df[OAnotes2_df['Note'].str.contains(\"[...]\")== True].reset_index(drop=True)\n",
        "print(\"Photographs which transcribed annotations is likely to be incomplete: \", annotations_incompleted.shape[0], \"(/over \",annotations_tot,\" transcribed)\")\n",
        "OAnotes2_df.to_csv(\"data\\OAnotes03.csv\", encoding=\"utf-8\")\n",
        "\n",
        "#create the corpus to be passed with spacy\n",
        "corpus = \"\"\n",
        "for OAnote in OAnotes2_df[\"Note\"]:\n",
        "    corpus = corpus+\"---\"+str(OAnote)+\"---\\n\"\n",
        "with open(\"data\\OAnotes04_corpus.txt\", \"w\", encoding=\"utf8\") as f:\n",
        "        f.write(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "tNX-1to7PfDw",
        "outputId": "fb472629-61eb-4438-828b-8cd0b3848817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-805364423887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#open the manually modified dataframe, search for unsuseful informations in 'Inv' and eliminate them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mOAnotes2_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/1_working_data/OAnotes02.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Inv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Foto |, (.+)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mOAnotes2_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Inv\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOAnotes2_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Inv\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_replace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data elaboration\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WvgsBKu0cKpF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Work on photographers"
      ],
      "metadata": {
        "id": "66Ejy3lGkspx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install SPARQLWrapper\n",
        "#!pip install geopy\n",
        "from csv import DictReader\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import pandas as pd\n",
        "import ssl\n",
        "from geopy.geocoders import Nominatim"
      ],
      "metadata": {
        "id": "DRaKAR62kXIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "geolocator = Nominatim(timeout=10, user_agent=\"myGeolocator\")"
      ],
      "metadata": {
        "id": "XuizLixzktEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# functions\n",
        "# define a function to open file in reading mode\n",
        "def process_csv(data_file_path):\n",
        "    import csv\n",
        "    source = open(data_file_path, mode=\"r\", encoding=\"UTF8\")\n",
        "    source_reader = csv.DictReader(source)\n",
        "    source_data = list(source_reader)\n",
        "    return source_data\n",
        "\n",
        "#define a function for transforming lists of elements in strings\n",
        "def write_string(source, output_txt_name):\n",
        "    string = \"\"\n",
        "    for source_data in source:\n",
        "        string = string+source_data+\"|\"\n",
        "    string = string[:-1]\n",
        "    with open(output_txt_name, \"w\", encoding=\"utf8\") as f:\n",
        "        f.write(string)\n",
        "    return string\n",
        "\n",
        "#define a function to query endpoints\n",
        "def query_endpoint(endpoint_url, SPRQL_query):\n",
        "    get_endpoint = endpoint_url\n",
        "    sparql_w = SPARQLWrapper(get_endpoint)\n",
        "    sparql_w.setQuery(SPRQL_query)\n",
        "    sparql_w.setReturnFormat(JSON)\n",
        "    spqrl_w_res = sparql_w.query().convert()\n",
        "    return spqrl_w_res\n",
        "\n",
        "#define a function to manipulate results and have back 1. a set of wd_URI corresponding to our wd_names,\n",
        "# 2. update of ph_matrix, 3. not matched wd_names\n",
        "\n",
        "def manipulate(spqrl_w_res, dataset_to_enhance, resNF_txt_name, resF_txt_name):\n",
        "    res_all = set()\n",
        "    res_dic = {}\n",
        "    res_NF_tem = set()\n",
        "    res_F = set()\n",
        "    for res in spqrl_w_res[\"results\"][\"bindings\"]:\n",
        "        res_all.add((res[\"fLabel\"][\"value\"], res[\"f\"][\"value\"]))\n",
        "        for datum in dataset_to_enhance:\n",
        "            if datum[\"ph_wd_URI\"]:\n",
        "                continue\n",
        "            else:\n",
        "                if datum[\"ph_wd_name\"] not in res_dic:\n",
        "                    if res[\"fLabel\"][\"value\"] == datum[\"ph_wd_name\"]:\n",
        "                        res_F.add(res[\"f\"][\"value\"])\n",
        "                        new_pairs = {\"ph_wd_URI\": res[\"f\"][\"value\"]}\n",
        "                        res_dic.update({datum[\"ph_wd_name\"]: new_pairs})\n",
        "                        datum.update([(\"ph_wd_URI\", res[\"f\"][\"value\"])])\n",
        "                    else:\n",
        "                        res_NF_tem.add(datum[\"ph_wd_name\"])\n",
        "    res_NF_def = res_NF_tem - set(list(res_dic.keys()))\n",
        "    with open(resNF_txt_name, \"w\", encoding=\"utf8\") as f:\n",
        "        f.write(str(res_NF_def))\n",
        "    with open(resF_txt_name, \"w\", encoding=\"utf8\") as f:\n",
        "        f.write(str(res_F))\n",
        "    print(\"labels matched: \", len(res_F))\n",
        "    print(\"labels not found2: \", len(res_NF_def))\n",
        "    return res_F, res_NF_def #, res_all"
      ],
      "metadata": {
        "id": "PDzPD39uj7pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open source data with pandas\n",
        "df_data = pd.read_csv(\"data\\F_OA_selected_data.csv\")\n",
        "\n",
        "#initialize a photograph's frequency dataframe\n",
        "ph_freq = pd.DataFrame(df_data[\"AUFN_Faut\"].value_counts().reset_index().values, columns=[\"AUFN_Faut\", \"count\"])\n",
        "\n",
        "#extend dataframe colums to host next datas\n",
        "ph_freq[\"ph_wd_name\"], ph_freq[\"ph_wd_URI\"], ph_freq[\"gender\"], ph_freq[\"workplace\"], ph_freq[\"lat\"], ph_freq[\"lon\"],\\\n",
        "ph_freq[\"born\"], ph_freq[\"died\"], ph_freq[\"lat\"] = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]"
      ],
      "metadata": {
        "id": "sz5v6r-4k7UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the firts string for the SPARQL query by \n",
        "#normalizing (personal) names in form \"surname, name\" to \"name surname\" as in wikidata\n",
        "#and create a list of the modified names tobe added to the dataframe\n",
        "\n",
        "first_ph_names_string =\"\"\n",
        "ph_wd_name_list = []\n",
        "for ph in ph_freq.index:\n",
        "    ph_name = str(ph_freq[\"AUFN_Faut\"][ph])\n",
        "    # reverse only (personal) names in form \"surname, name\" > \"name surname\"\n",
        "    if \", \" in ph_name:    \n",
        "        ph_split = ph_name.split(\", \")\n",
        "        ph_wd_name = ph_split[1] + \" \" + ph_split[0]\n",
        "    else:\n",
        "        ph_wd_name = ph_name\n",
        "    ph_wd_name_list.append(ph_wd_name)\n",
        "    first_ph_names_string = first_ph_names_string + ph_wd_name + \"|\"\n",
        "first_ph_names_string = first_ph_names_string[:-1]\n",
        "#save the string to text\n",
        "with open(\"data\\ph_string1.txt\", \"w\", encoding=\"utf8\") as f:\n",
        "    f.write(first_string)\n",
        "\n",
        "#add ph_wd_name_list to the dataframe\n",
        "ph_freq[\"ph_wd_name\"] = [ph_wd_name_list]\n",
        "#save the dataframe in a csv file\n",
        "ph_freq.to_csv(\"data\\1_ph_freq.csv\", encoding=\"utf-8\")\n",
        "\n",
        "#open the dataframe as dictionary\n",
        "ph_matrix = process_csv(\"ph_freq.csv\")"
      ],
      "metadata": {
        "id": "1XL0DDREpSb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the first query string to collect wikidata URI\n",
        "first_ph_SPARQL_query = \"\"\"\n",
        "SELECT DISTINCT ?f ?fLabel\n",
        "WHERE\n",
        "{    { ?f wdt:P106 wd:Q33231 } UNION { ?f wdt:P31 wd:Q672070}. #P106_has_for_occupation wd:Q33231_photographer \n",
        "                                                                #P31_is instance wd:Q672070_studios\n",
        "    ?f rdfs:label ?fLabel.\n",
        "     FILTER regex(?fLabel, \\\" \"\"\"+first_ph_names_string+\"\"\" \\\")\n",
        "     FILTER(LANG(?fLabel) = \"en\").\n",
        "}\"\"\""
      ],
      "metadata": {
        "id": "3z0IeTINsOwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#perform the first SPARQL query and result manipulation\n",
        "first_ph_wd_res = query_endpoint(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\", first_ph_SPARQL_query)\n",
        "first_ph_manipulate = manipulate(first_ph_wd_res, ph_matrix, \"ph_NF.txt\", \"ph_F.txt\")\n",
        "first_F_set = first_ph_manipulate[0]\n",
        "first_NF = first_ph_manipulate[1]"
      ],
      "metadata": {
        "id": "KU7kl6G9s0Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#after revising first results, refine the unmatched labels\n",
        "new_list = []\n",
        "for ph_wd_NF in first_NF:\n",
        "    if \"  \" in ph_wd_NF:\n",
        "        ph_wd_new = ph_wd_NF.replace(\"  \", \" \")\n",
        "    elif \"Fratelli\" in ph_wd_NF:\n",
        "        ph_wd_new = ph_wd_NF.replace(\"Fratelli\", \"\")\n",
        "    elif \"&\" in ph_wd_NF:\n",
        "        ph_wd_new = ph_wd_NF.replace(\"&\", \"and\")\n",
        "    elif \"Brogi\" == ph_wd_NF:\n",
        "        ph_wd_new = \"Giacomo Brogi\"\n",
        "    elif \"Incorpora\" == ph_wd_NF:\n",
        "        ph_wd_new = \"Giuseppe Incorpora\"\n",
        "    elif \"Giraudon\" == ph_wd_NF:\n",
        "        ph_wd_new = \"Adolphe Giraudon\"\n",
        "    else:\n",
        "        continue\n",
        "    new_list.append(ph_wd_new)\n",
        "    for ph_data in ph_matrix:\n",
        "        if ph_data[\"ph_wd_name\"] == ph_wd_NF:\n",
        "            ph_data.update([(\"ph_wd_name\", ph_wd_new)])\n",
        "\n",
        "#from the new modified names, by using the function obtain a second string to query \n",
        "second_ph_string = write_string(new_list, \"ph_string2.txt\")"
      ],
      "metadata": {
        "id": "PjIOKX7Yts7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the second query string to collect wikidata URI\n",
        "second_ph_SPARQL_query = \"\"\"\n",
        "SELECT DISTINCT ?f ?fLabel\n",
        "WHERE\n",
        "{    { ?f wdt:P106 wd:Q33231 } UNION { ?f wdt:P31 wd:Q672070}. #P106_has_for_occupation wd:Q33231_photographer \n",
        "                                                                #P31_is instance wd:Q672070_studios\n",
        "    ?f rdfs:label ?fLabel.\n",
        "     FILTER regex(?fLabel, \\\" \"\"\"+second_ph_string+\"\"\" \\\")\n",
        "     FILTER(LANG(?fLabel) = \"en\").\n",
        "}\n",
        "\"\"\"\n",
        "#perform the second SPARQL query and result manipulation\n",
        "second_ph_wd_res = query_endpoint(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\", second_ph_SPARQL_query)\n",
        "second_manipulate = manipulate(second_ph_wd_res, ph_matrix, \"ph_NF2\", \"ph_F2.txt\")\n",
        "second_F_set = second_manipulate[0]"
      ],
      "metadata": {
        "id": "eZjQSnZsuRZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#obtain the list of found wikidata URI\n",
        "complex_F_set = second_F_set.union(first_F_set)\n",
        "#print(complex_F_set)\n",
        "\n",
        "#prepare the thirtd string to be passed in SPARQL query\n",
        "third_ph_string_URI =\"\"\n",
        "for F_URI in complex_F_set:\n",
        "    third_ph_string_URI = third_ph_string_URI+\"<\"+F_URI+\">\"\n",
        "#with open(\"ph_URI.txt\", \"w\", encoding=\"utf8\") as f:\n",
        "#    f.write(stringURI)\n",
        "#print(stringURI)\n",
        "\n",
        "#third query\n",
        "third_ph_SPARQL_query = \"\"\"\n",
        "SELECT DISTINCT ?ph ?genderLabel ?countryLabel ?birthyear ?deathyear\n",
        "    WHERE\n",
        "    { VALUES ?ph {\"\"\"+stringURI+\"\"\"} \n",
        "        ?ph rdfs:label ?phLabel;\n",
        "        wdt:P937 ?country; #P937_worklocation\n",
        "        #wdt:P27 ?citiz;        \n",
        "        wdt:P569 ?birth;\n",
        "        wdt:P570 ?death.\n",
        "        OPTIONAL {FILTER(LANG(?fLabel) = \"en\").\n",
        "                    ?ph wdt:P21 ?gender;\n",
        "                    #wdt:P937 ?worklocation; #P937_worklocation\n",
        "        }\n",
        "        BIND(year(?birth) AS ?birthyear)\n",
        "        BIND(year(?death) AS ?deathyear)\n",
        "\n",
        "        #BIND(COALESCE(?worklocation, ?citiz, \"NaN\") AS ?country).\n",
        "        #BIND(IF(BOUND(?worklocation),?worklocation,?citiz) AS ?country).\n",
        "    SERVICE wikibase:label {bd:serviceParam wikibase:language \"en\".}     \n",
        "    }\"\"\"\n",
        "#OPTIONAL { ?ph wdt:P569 ?birthdate;        wdt:P570 ?deathdate.} ci servono...\n",
        "\n",
        "#perform the third query\n",
        "third_ph_wd_res = query_endpoint(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\", third_ph_SPARQL_query)\n",
        "\n",
        "#manipulate results\n",
        "wd_total_dic = {}\n",
        "for result in third_ph_wd_res[\"results\"][\"bindings\"]:\n",
        "    item_key = result[\"ph\"][\"value\"]\n",
        "    item_value = {\"workplace\": result[\"countryLabel\"][\"value\"],\n",
        "                  \"born\": result[\"birthyear\"][\"value\"],\n",
        "                  \"died\": result[\"deathyear\"][\"value\"]}\n",
        "    if item_key not in wd_total_dic:\n",
        "        wd_total_dic.update({item_key: item_value})\n",
        "        for ph_data in ph_matrix:\n",
        "            if ph_data[\"ph_wd_URI\"] == item_key:\n",
        "                item2=item_value.items()\n",
        "                ph_data.update(item2)\n",
        "\n",
        "wd_total_list = list(wd_total_dic.values())\n",
        "print(wd_total_list)\n",
        "\n",
        "keys = wd_total_list[0].keys()\n",
        "with open(\"ph_wd_total.csv\", \"w\", encoding=\"UTF8\", newline=\"\") as output_file:\n",
        "    dict_writer = csv.DictWriter(output_file, keys)\n",
        "    dict_writer.writeheader()\n",
        "    dict_writer.writerows(wd_total_list)"
      ],
      "metadata": {
        "id": "RsSNGDE8v1py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Work on places"
      ],
      "metadata": {
        "id": "0xSgal3tQruP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define function to store lat-lon from a list of places\n",
        "def get_coordinates(list, df):\n",
        "    for place in list:\n",
        "        if place not in df[\"place\"].unique().tolist():\n",
        "            if geolocator.geocode(place) != None:\n",
        "                lon = geolocator.geocode(place).longitude\n",
        "                lat = geolocator.geocode(place).latitude\n",
        "            else:\n",
        "                lon = \"NaN\"\n",
        "                lat = \"NaN\"\n",
        "            new_place = [place, lat, lon]\n",
        "            df.loc[len(df)] = new_place\n",
        "        df.to_csv(\"data\\places_coordinates.csv\", encoding=\"UTF-8\")"
      ],
      "metadata": {
        "id": "Z_E9BD3XQrSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#open the saved file\n",
        "data_df = pd.read_csv(\"data\\F_OA_selected_data.csv\")\n",
        "ph_freq_df = pd.read_csv(\"data\\ph_freq2.csv\")\n",
        "\n",
        "#reduce columns and change column name, check first rows\n",
        "places_F = ph_freq_df[['workplace', \"lat\", \"lon\"]].dropna()\n",
        "places = places_F.rename(columns={\"workplace\": \"place\"})\n",
        "places.head()\n",
        "\n",
        "#extract towns and country unique names from original dataframe\n",
        "towns_OA = data_df['PVCC_OAtown'].unique().tolist()\n",
        "countries_OA = data_df['PVCS_OAcountry'].unique().tolist()\n",
        "\n",
        "#obtain coordinates from the two list and store them in a df\n",
        "get_coordinates(countries_OA, places)\n",
        "get_coordinates(towns_OA, places)"
      ],
      "metadata": {
        "id": "Lnd5nBkERNH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Work on Annotations"
      ],
      "metadata": {
        "id": "8ZRD3mhvZ1z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install spacy\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.attrs import POS"
      ],
      "metadata": {
        "id": "3qzXjNVGbnY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#open the file with annotations texts\n",
        "with open(\"data/OAnotes_corpus.txt\", mode=\"r\") as f:\n",
        "    contents = f.read()\n",
        " \n",
        "#load the nlp model and look for {\"LEMMA\": \"I\"}, {POS: 'VERB'} pattern (first person verbs)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add(\"PA_creator\", [[{\"LEMMA\": \"I\"}, {POS: 'VERB'}]])\n",
        "doc = nlp(contents)\n",
        "matches = matcher(doc)\n",
        "\n",
        "matched = []\n",
        "for match_id,start,end in matches:\n",
        "    I_verb = str(doc[start:end])\n",
        "    matched.append(I_verb)\n",
        "print(matched)\n",
        "#I doubt, I was, I think(3), I discovered, I have(4), I saw(4), I AM(2?), I told, I respected, I farn, me look,\n",
        "# I believe, I put"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "cOWvR2Z3Z62_",
        "outputId": "c55a54a4-08ee-4072-8021-4bad942ea9ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-fe0bd4229862>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#open the file with annotations texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\\OAnotes_corpus.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#load the nlp model and look for {\"LEMMA\": \"I\"}, {POS: 'VERB'} pattern (first person verbs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\OAnotes_corpus.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a dataframe from list of matched and check occurrencies\n",
        "matched_df = pd.DataFrame()\n",
        "matched_df[\"Match\"] = matched\n",
        "matched_freq = pd.DataFrame(matched_df[\"Match\"].value_counts().reset_index().values, columns=[\"Match\", \"count\"])\n",
        "print(matched_freq.head(25))"
      ],
      "metadata": {
        "id": "WLMGHtjXcNko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data visualization"
      ],
      "metadata": {
        "id": "sxPYRxMacLDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "**Analyse**\n",
        "pandas library in order to examine our data.\n",
        "     \n",
        "       \n",
        "    1. Data Preparation:\n",
        "          - creation of two complexive xml files for F and OA records coming from the Federico Zeri Foundation catalogues\n",
        "          - extraction from nested xml stucture of relevant information for the project and structuring them in plain tabular format\n",
        "    2. Data Elaboration: seeking for furter analysis elements via:\n",
        "          - deeper work on photographer for enhance their information\n",
        "          - deeper work on places\n",
        "          - work on unstructured annotations: NER\n",
        "     2. Data Visualization\n"
      ],
      "metadata": {
        "id": "rSDRDeMOjB8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. Data overview"
      ],
      "metadata": {
        "id": "7U57aNxDdeOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pandas_profiling as pp\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=1)"
      ],
      "metadata": {
        "id": "pgU6VKzNdtTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse the csv into a dataframe\n",
        "data_df = pd.read_csv('data\\F_OA_selected_data.csv')\n",
        "\n",
        "# reduce the dataset to just the columns needed\n",
        "data_df = data_df[['INVN_F', 'PVCS_OAcountry', 'PVCC_OAtown', 'LDCN_OArep', 'PRVC_OAprev_town',\n",
        "          'AUFN_Faut', 'LRD_Fdates', 'OGTT_OAtype', 'AUTB_Fsubj_main', 'OGTDOA_OAsubj_sub', 'AUTN_OAaut']]\n",
        "print(data_df.head(15))"
      ],
      "metadata": {
        "id": "Kk-RBvonXIBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import pandas_profiling as pp\n",
        "report = pp.ProfileReport(data_df, title=\"Partizione Antica Fund - overview\")\n",
        "report.to_file(\"ProfileReport_sup.html\")"
      ],
      "metadata": {
        "id": "CDgOvQUheO2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "7Hb6VMBkf7LJ",
        "outputId": "8a7d3687-1578-40fe-99da-12a242f965f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-07d3306fc65e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'report' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 Works of art - typology proportions pie"
      ],
      "metadata": {
        "id": "JtgJeWIngJcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# filter cities and number of photos from data\n",
        "data = pd.read_csv(\"data/F_OA_selected_data.csv\", encoding=\"UTF-8\")\n",
        "OAt_df = pd.DataFrame(data[\"OGTT_OAtype\"].value_counts().reset_index().values, columns=[\"OGTT_OAtype\", \"count\"])\n",
        "\n",
        "fig = px.pie(OAt_df, values='count', names=\"OGTT_OAtype\",\n",
        "            title='OA typologies',\n",
        "            color_discrete_sequence=px.colors.sequential.RdBu,\n",
        "            labels = OAt_df['OGTT_OAtype'], hover_name = 'OGTT_OAtype',\n",
        "            hover_data = {'OGTT_OAtype':False}\n",
        "            )\n",
        "fig.show()\n",
        "fig.write_html(\"data/2_data_viz/1.1.html\")"
      ],
      "metadata": {
        "id": "saYsZpZ2gI6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 Works of art - countries proportions pie\n",
        "\n"
      ],
      "metadata": {
        "id": "7rauFSANgaYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "import plotly\n",
        "import plotly.graph_objects as go\n",
        "import csv\n",
        "\n",
        "# filter cities and number of photos from data\n",
        "data = pd.read_csv(\"UNIQUE_new2.csv\", encoding=\"utf8\")\n",
        "sup_df = data[['PVCS_OAcountry', 'PVCC_OAtown', 'LDCN_OArep']]\n",
        "\n",
        "#create a pie chart of supposed OA country\n",
        "df_data = pd.read_csv(\"OAcountry_freq.csv\", encoding=\"utf8\")\n",
        "df_data.loc[df_data['count'] < 10, 'PVCS_OAcountry'] = 'Other countries' # Represent only large countries\n",
        "#df = px.df_data()\n",
        "fig = px.pie(df_data, values='count', names=\"PVCS_OAcountry\",\n",
        "            title='Depicted OA for country',\n",
        "            color_discrete_sequence=px.colors.sequential.RdBu,\n",
        "            labels = df_data['PVCS_OAcountry'], hover_name = 'PVCS_OAcountry',\n",
        "            hover_data = {'PVCS_OAcountry':False, 'lat':False, 'lon': False}\n",
        "            )\n",
        "fig.show()\n",
        "fig.write_html(\"data/2_data_viz/1.2.html\")"
      ],
      "metadata": {
        "id": "g_EONtm7gaIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3 Works of art - countries proportions map"
      ],
      "metadata": {
        "id": "nYz-E7P8g7fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from geopy.geocoders import Nominatim\n",
        "geolocator = Nominatim(timeout=10, user_agent = \"myGeolocator\")\n",
        "\n",
        "import plotly\n",
        "import plotly.graph_objects as go\n",
        "import csv\n",
        "\n",
        "# filter cities and number of photos from data\n",
        "df_data = pd.read_csv(\"OAcountry_freq.csv\", encoding=\"utf8\")\n",
        "ph_geol = px.scatter_mapbox(df_data, lon=df_data['lon'],\n",
        "                            lat=df_data['lat'], size=df_data[\"count\"], zoom=2, color=df_data['PVCS_OAcountry'],\n",
        "                            color_continuous_scale=px.colors.cyclical.Twilight,\n",
        "                            #color_discrete_sequence=px.colors.sequential.RdBu,\n",
        "                            title=\"Depicted OA\",\n",
        "                            size_max=80,\n",
        "                            labels=df_data['PVCS_OAcountry'], hover_name='PVCS_OAcountry',\n",
        "                            hover_data={'PVCS_OAcountry':False, 'lat':False, 'lon':False})\n",
        "\n",
        "# mapbox style\n",
        "ph_geol.update_layout(mapbox_style='carto-positron')\n",
        "ph_geol.show()\n",
        "ph_geol.write_html(\"data/2_data_viz/1.3.html\")"
      ],
      "metadata": {
        "id": "Rea-Mj6qg3II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Photograps - photographers proportions pie /distribution barchart"
      ],
      "metadata": {
        "id": "iifJu-PthWVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "df_data2 = pd.read_csv(\"ph_newfreq.csv\", encoding=\"utf8\")\n",
        "df_data2 = df_data2[df_data2[\"count\"]>=20]\n",
        "#df_data2.loc[df_data2['count'] < 10, 'AUFN_Faut'] = 'Other photographs' # Represent only large countries\n",
        "#df = px.df_data()\n",
        "fig2 = px.pie(df_data2, values='count', names=\"AUFN_Faut\",\n",
        "            title='Photographs (>=20) for photographer',\n",
        "            color_discrete_sequence=px.colors.qualitative.Dark24, #color_discrete_sequence/color_continuous_scale =px.colors.sequential.RdBu,\n",
        "            labels = df_data2['AUFN_Faut'], hover_name='AUFN_Faut',\n",
        "            hover_data = {'AUFN_Faut':False, 'workplace':True}\n",
        "            #sistema le caselle non piene di ph_freq etc penso fillna()\n",
        "            )\n",
        "fig2.show()\n",
        "fig2.write_html(\"data/2_data_viz/2.1.html\")"
      ],
      "metadata": {
        "id": "RrH1jF4rhWHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 Photographs - map distribution based on ateliers locations"
      ],
      "metadata": {
        "id": "pHJdaH1PiEYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "#to show up everything directly in jupyter notebook\n",
        "\n",
        "import plotly\n",
        "import plotly.graph_objects as go\n",
        "import csv\n",
        "\n",
        "#open the whole dataset and select just the AUFN_FAUT column\n",
        "data = pd.read_csv(\"data\\F_OA_selected_data.csv\", encoding=\"UTF-8\")\n",
        "data_d = data[[\"AUFN_Faut\"]]\n",
        "print(len(data_d))\n",
        "\n",
        "#open the ph_freq dataset and select just the AUFN_FAUT and the workplace column; create a second df on workplace lat and lon\n",
        "data_ph = pd.read_csv(\"data\\ph_freq.csv\", encoding=\"UTF-8\")\n",
        "data_ph_df = data_ph[[\"AUFN_Faut\", \"workplace\"]]\n",
        "data_lat_lon = data_ph[[\"workplace\", \"lat\", \"lon\"]].drop_duplicates(subset=['workplace'])\n",
        "\n",
        "#merge the two dataset on the AUFN_Faut column\n",
        "data_comp = data_d.merge(data_ph_df, how='left', on=\"AUFN_Faut\").reset_index(drop=True)\n",
        "print(len(data_comp))\n",
        "print(data_comp.head(25))\n",
        "\n",
        "#count the values on workplace column\n",
        "data_count = pd.DataFrame(data_comp['workplace'].value_counts().reset_index().values, columns=['workplace', 'count'])\n",
        "print(len(data_count))\n",
        "print(data_count.head(25))\n",
        "\n",
        "#add lat and lon data to the previous df\n",
        "data_df = data_count.merge(data_lat_lon, how='left', on=\"workplace\").reset_index(drop=True)\n",
        "data_df['count'] = pd.to_numeric(data_df['count'])\n",
        "print(len(data_df))\n",
        "print(data_df.head(25))\n",
        "\n",
        "ph_geol = px.scatter_mapbox(data_df, lon=data_df['lon'], lat=data_df['lat'],\n",
        "                            size=data_df['count'], zoom=3, color=data_df['workplace'],\n",
        "                            size_max=80,\n",
        "                            color_discrete_sequence=px.colors.sequential.RdBu,\n",
        "                            title=\"Photohgrapher ateliers\",\n",
        "                            labels=data_df['workplace'], hover_name=\"workplace\")\n",
        "\n",
        "# mapbox style\n",
        "ph_geol.update_layout(mapbox_style='carto-positron')\n",
        "ph_geol.show()\n",
        "ph_geol.write_html(\"data/2_data_viz/2.2.html\")"
      ],
      "metadata": {
        "id": "t3ndEp0MiEiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 Photographs - map distribution of anonimous photographs based on place of shooting (limited to immobles)"
      ],
      "metadata": {
        "id": "nfzogXtzioNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install regex\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import regex as re\n",
        "\n",
        "data = pd.read_csv(\"UNIQUE_new2.csv\", encoding=\"UTF-8\")\n",
        "\n",
        "#reduce to only columns rows needed and check\n",
        "sup_df = data[['OGTT_OAtype', 'PVCS_OAcountry', 'PVCC_OAtown', 'AUFN_Faut']]\n",
        "sup_df = sup_df[sup_df.AUFN_Faut == \"Anonimo\"]\n",
        "\n",
        "df_data3 = pd.DataFrame(sup_df[\"PVCS_OAcountry\"].value_counts().reset_index().values, columns=[\"PVCS_OAcountry\", \"count\"])\n",
        "\n",
        "fig3 = px.pie(df_data3, values='count', names=\"PVCS_OAcountry\",\n",
        "              title='Anonimous Photographs (1331/3111) for OAcountry',\n",
        "              color_discrete_sequence=px.colors.sequential.Brwnyl,\n",
        "              labels = df_data3['PVCS_OAcountry'], hover_name='PVCS_OAcountry',\n",
        "              hover_data = {'PVCS_OAcountry':False}\n",
        "              )\n",
        "\n",
        "fig3.update_layout(\n",
        "    font = dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        )\n",
        "    )\n",
        "fig3.show()\n",
        "\n",
        "sup_df= sup_df[sup_df['OGTT_OAtype'].str.contains(\"architettura|architettura\\ scultura|complesso archeologico|sito archeologico\")== True].reset_index(drop=True)\n",
        "sup_df.to_csv(\"daje.csv\")\n",
        "print(sup_df.tail(200))\n",
        "df_data4 = pd.DataFrame(sup_df[\"PVCS_OAcountry\"].value_counts().reset_index().values, columns=[\"PVCS_OAcountry\", \"count\"])\n",
        "fig4 = px.pie(df_data4, values='count', names=\"PVCS_OAcountry\",\n",
        "              title='Anonimous Photographs (1331/3111) for OAcountry of immobles',\n",
        "              color_discrete_sequence=px.colors.sequential.Brwnyl, #px.colors.sequential.RdBu https://plotly.com/python/discrete-color/\n",
        "              labels = df_data3['PVCS_OAcountry'], hover_name='PVCS_OAcountry',\n",
        "              hover_data = {'PVCS_OAcountry':False}  #sistema le caselle non piene di ph_freq etc penso fillna()\n",
        "              )\n",
        "\n",
        "fig4.update_layout(\n",
        "    font = dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        )\n",
        "    )\n",
        "fig4.show()\n",
        "fig4.write_html(\"data/2_data_viz/2.3.html\")"
      ],
      "metadata": {
        "id": "VxUk_S0BioDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Annotations - complete/incomplete/missing transcriptions proportions pie"
      ],
      "metadata": {
        "id": "RVfqj_usjf1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "#open the needed dataframes\n",
        "data_df = pd.read_csv('data\\F_OA_selected_data.csv')\n",
        "all_inv = data_df[[\"INVN_F\"]].rename(columns={\"INVN_F\": \"Inv\"}).reset_index(drop=True)\n",
        "OAnotes_df = pd.read_csv('data/1_working_data/OAnotes03.csv')\n",
        "\n",
        "#define a function to check status of transcriptions\n",
        "def check(row):\n",
        "    if \"...\" in str(row[\"Note\"]):\n",
        "        status = \"incomplete\"\n",
        "    else:\n",
        "        status = \"complete\"\n",
        "    return status\n",
        "\n",
        "#apply the function to df and add a status column, check it \n",
        "OAnotes_df[\"status\"] = OAnotes_df.apply(check, axis=1)\n",
        "print(OAnotes_df.head(25))\n",
        "\n",
        "#merge the df with all the inventories to check for missing transcriptions and count according to the status \n",
        "merged = all_inv.merge(OAnotes_df, how='left', on=\"Inv\").reset_index(drop=True)\n",
        "new=pd.DataFrame(merged[\"status\"].value_counts(dropna=False).reset_index().values, columns=[\"status\", \"count\"])\n",
        "#change the empty rows with \"missing\" and check\n",
        "new['status'] = new['status'].fillna('missing')\n",
        "print(new.tail(50))\n",
        "\n",
        "fig = px.pie(new, values='count', names=\"status\",\n",
        "              title='Annotations on photographs',\n",
        "              color_discrete_sequence=px.colors.sequential.Brwnyl, #px.colors.sequential.RdBu https://plotly.com/python/discrete-color/\n",
        "              labels = new['status'], hover_name='status',\n",
        "              hover_data = {'status':True}  #sistema le caselle non piene di ph_freq etc penso fillna())\n",
        "             )\n",
        "\n",
        "fig.update_layout(\n",
        "    font = dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        )\n",
        "    )\n",
        "fig.show()\n",
        "fig.write_html(\"data/2_data_viz/3.1.html\")"
      ],
      "metadata": {
        "id": "g634EN9LjfuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 Annotations"
      ],
      "metadata": {
        "id": "-X5aqyMQlSii"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ldFH4F4liEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3 Annotations and metadata: compared dates distribution"
      ],
      "metadata": {
        "id": "gEWFNsy5liLo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BD4odwiSluHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1 Annotations - time-place pairs related to ..."
      ],
      "metadata": {
        "id": "qI4y50zSluSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "#open and merge data from annotations reporting time-place pairs and place coordinates, check for it\n",
        "movement_df = pd.read_csv(\"data/1_working_data/OAnotes05bis.csv\", encoding=\"utf8\")\n",
        "places_df = pd.read_csv(\"data/1_working_data/places_coordinates2.csv\", encoding=\"utf8\")\n",
        "movement_df_coor = movement_df.merge(places_df, how='left', on=\"place\")\n",
        "movement_df_coor.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "print(movement_df_coor.head(10))\n",
        "\n",
        "#break the texts at about 30 characters to let them better visualized\n",
        "movement_df_coor['Note_br'] = movement_df_coor.apply(lambda row: ('<br>'.join(str(row.Note)[i:i+30] for i in range(0, len(str(row.Note)), 30))), axis = 1)\n",
        "\n",
        "#set scatter with d\n",
        "fig = px.scatter_geo(movement_df_coor, color=\"date\",\n",
        "                  lat=movement_df_coor[\"lat\"].values.tolist(),\n",
        "                  lon=movement_df_coor[\"lon\"].values.tolist(),\n",
        "                  title=\"Movements\", size=\"date\",\n",
        "                  projection=\"natural earth\", scope=\"europe\",\n",
        "            labels = movement_df_coor['place'], hover_name='place',\n",
        "            hover_data = {'place':False, 'Inv':True, 'Note_br':True}\n",
        "            )\n",
        "fig.show()\n",
        "fig.write_html(\"data/2_data_viz/4.1.html\")"
      ],
      "metadata": {
        "id": "2x3WJ6drlSYT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}