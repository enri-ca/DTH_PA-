{
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Intro\n",
        "\n",
        "This Jupyter Notebook has been created for the <a href=\"https://www.unibo.it/it/didattica/insegnamenti/insegnamento/2021/443749\" target=\"_blank\">90154 - Electronic Publishing and Digital Storytelling</a> course, taught by **Prof. Marilena Daquino**, in the framework of the 2nd year of the <a href=\"https://corsi.unibo.it/2cycle/DigitalHumanitiesKnowledge\" target=\"_blank\">DHDK Master Degree</a>, a.a. 2021-22.<br>\n",
        "Here listed the main steps for the realization of the project **Partizione Antica**: \n",
        "https://mybinder.org/v2/gh/https%3A%2F%2Fenri-ca.github.io%2FEPDS_EZ/main\n",
        "      \n",
        "       \n",
        "    1. Data Preparation:\n",
        "          - creation of two complexive xml files for F and OA records coming from the Federico Zeri Foundation catalogues\n",
        "          - extraction of relevant information for the project from nested xml stucture and structuring them in plain tabular format\n",
        "    2. Data Elaboration: seeking for furter analysis elements via:\n",
        "          - deeper work on photographer for enhance their information\n",
        "          - deeper work on places enhance their gelocation\n",
        "          - work on unstructured annotations: NER\n",
        "     2. Data Visualization\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "D76tTDYyH-z9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data preparation\n",
        "\n",
        "This research started from a **record data extraction** provided from the Federico Zeri Foundation: the original data counted 3.260 F and 2.634 OA records - respectively for the photographs, and for the depicted works of art - of the Supino Partizione Antica fund. <br>\n",
        "The original data have been used for **illustrative and didactical purposes only**: all the credits and reuse authorizations must be asked to <a href=\"mailto:fondazionezeri.fototeca@unibo.it\">Federico Zeri Foundation</a>.\n",
        "\n",
        "**1.1 Creation of the F and OA complexive xml files**\n",
        "\n",
        "To allow a better management and manipulation, as well as to anonymize personal data, complexive files (via <a href=\"/content/sample_data/0_Creation_UniqeXML.xquery\" target=\"_blank\">0_Creation_UniqeXML.xquery</a> collection command) have been created and published. \n",
        "They collect:\n",
        "\n",
        "*   all the single photograph xml files' records in the F_entries.xml file (data/0_source_data)\n",
        "*   all the single works of art xml files' records in the OA_entries.xml file (data/0_source_data)\n",
        "\n",
        "**1.2 Creation of the flat tabular dataset extracting relevant information for the project from the nested xml elements and attributes**\n",
        "\n",
        "Due to the hypernested and not consistently presence of elements at different levels, <pandas.read_xml> method was not effectively parsing what was needed.\n",
        "The <xml.etree.ElementTree> library has then been preferred because it allows to call for single elements at different nesting levels. Nevertheless, this approach presents some drawbacks as the need of a previous and deep knowledge of the database structure that does not allow to uncover unexpected correlations possible through the exploration of a comprehensive dataset.\n",
        "\n",
        "**1.3 Preliminary installation**(Uncomment the first line to install the library)\n",
        "- libraries\n",
        "- imports:\n",
        "  - xml.etree.ElementTree, pandas, csv for managing the dataset\n",
        "  - ...\n"
      ],
      "metadata": {
        "id": "TYn9-Vu-cI03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preliminary imports\n",
        "import csv\n",
        "#from csv import DictReader\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "\n",
        "#function to have back the text element required if present without\n",
        "def extract_data(path):\n",
        "    #name = SCHEDA.find(path).tag\n",
        "    if SCHEDA.find(path) != None:\n",
        "         name = SCHEDA.find(path).text\n",
        "    else:\n",
        "        name = None\n",
        "    return name"
      ],
      "metadata": {
        "id": "4ZW8b0GdYl6l",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parse the complexive Fxml and OAxml files\n",
        "\n",
        "F_tree = ET.parse('data/0_source_data/F_entries.xml')\n",
        "F_root = F_tree.getroot()\n",
        "F_root.attrib['test']\n",
        "\n",
        "OA_tree = ET.parse('data/0_source_data/F_entries.xml')\n",
        "OA_root = OA_tree.getroot()\n",
        "OA_root.attrib['test2']\n",
        "\n",
        "#set the colums' headers for the choosen elements\n",
        "header = [\"sercdf_F_ser\", \"sercdoa_OA_ser\", \"INVN_F\", \"UBFC_Fshelfmark\", #ids\n",
        "          \"PVCS_OAcountry\", \"PVCC_OAtown\", \"LDCN_OArep\", \"PRVC_OAprev_town\", \"AUFI_Fatelier_town\", #places\n",
        "          \"AUFN_Faut\", \"SGLT_Ftitle\", \"SGTT_OAtitle\", \"AUTN_OAaut\", #authors/titles\n",
        "          \"OGTT_OAtype\", \"AUTB_Fsubj_main\", \"OGTDOA_OAsubj_sub\", #subjects\n",
        "          \"ROFI_Fneg\", #external relations\n",
        "          \"OSS_Fnotes\", \"OSS_OAnotes\", #unstructured infos\n",
        "         \"FTAN_filename\", \"NCTN_F_entry\", \"NRSCHEDA_OA_entry\", #2ary ids\n",
        "          \"LRD_Fshotdates\", \"DTSI_Fprintdates\", \"DTSF_Fprintdates\", \"AUFA_Faut_dates\"] #time\n",
        "\n",
        "#setting an empty list\n",
        "data = []\n",
        "\n",
        "#iterate on UNIQUE_F SCHEDA - and on correspondig UNIQUE_OA SCHEDA - \n",
        "#for extracting elements texts, store them in a list and add it to the data\n",
        "#two fields from original data are futherly modify for our purposes\n",
        "\n",
        "for SCHEDA in F_root.findall(\"SCHEDA\"):\n",
        "    oa_ser = SCHEDA.get(\"sercdoa\")\n",
        "    f_ser = SCHEDA.get(\"sercdf\")\n",
        "    inv = extract_data(\"./PARAGRAFO/INVN\")\n",
        "    container = extract_data(\"./PARAGRAFO/UBFT\")\n",
        "    shelf = extract_data(\"./PARAGRAFO/UBFC\")\n",
        "    title_f = extract_data(\"./PARAGRAFO/RIPETIZIONE/SGLT\")\n",
        "    aut_f = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUFN\") #the original data do not distinguish AUFN and AUFB for collective agents\n",
        "    aut_f_dates = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUFA\")#timespan of photographer's actvity\n",
        "    aut_f_place = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUFI\")#place of photographer's actvity as reported in the photograph\n",
        "    aut_oa = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUTN\")\n",
        "    subj_main = extract_data(\"./PARAGRAFO/RIPETIZIONE/AUTB\")\n",
        "    subj_sub = extract_data(\"./PARAGRAFO/RIPETIZIONE/OGTDOA\")\n",
        "    notes_f = extract_data(\"./PARAGRAFO/OSS\")\n",
        "    neg_num = extract_data(\"./PARAGRAFO/ROFI\")\n",
        "    f_entry = extract_data(\"./PARAGRAFO/NCTN\")\n",
        "    filename = extract_data(\"./PARAGRAFO/FTAN\")\n",
        "    shotdates = extract_data(\"./PARAGRAFO/LRD\")\n",
        "    if shotdates != None:\n",
        "        #reduce uncertainty: if /ante in field, put 1826 as conventional beginning date\n",
        "        #first negative https://en.wikipedia.org/wiki/Negative_(photography)\n",
        "        if \"/ante\" in shotdates:\n",
        "            shotdates.replace(\"/ante\", \"/ ante\")\n",
        "        if \"/ ante\" in shotdates:\n",
        "            shotdates = \"1826-\"+shotdates[:-6] \n",
        "    printdates_start = extract_data(\"./PARAGRAFO/DTSI\")\n",
        "    printdates_end = extract_data(\"./PARAGRAFO/DTSF\")\n",
        "\n",
        "    for SCHEDA in OA_root.findall(\"SCHEDA\"):\n",
        "        if SCHEDA.get(\"sercdoa\") == oa_ser:\n",
        "            title_oa = extract_data(\"./PARAGRAFO/SGTT\")\n",
        "            date_from_oa = extract_data(\"./PARAGRAFO/DTSI\")\n",
        "            date_to_oa = extract_data(\"./PARAGRAFO/DTSF\")\n",
        "            country_oa = extract_data(\"./PARAGRAFO/PVCS\") #Original data report just 2 LRCS: name of the country where the shot was taken.\n",
        "            town_oa = extract_data(\"./PARAGRAFO/PVCC\") #Original data report just 2 LRCC: name of the country where the shot was taken.\n",
        "            rep_oa = extract_data(\"./PARAGRAFO/LDCN\")\n",
        "            prev_town_oa = extract_data(\"./PARAGRAFO/RIPETIZIONE/PRVC\")\n",
        "            if prev_town_oa != None:\n",
        "                #save the previous locations only if in 1800-1899 timespan (PRDU) otherwise put \"NR\" (not relevant)\n",
        "                if extract_data(\"./PARAGRAFO/RIPETIZIONE/PRDU\") != None:\n",
        "                    if \"1799\" < extract_data(\"./PARAGRAFO/RIPETIZIONE/PRDU\") < \"1900\": #PRDU last date the OA was in that location\n",
        "                        prev_town_oa = prev_town_oa + \" | \" + str(extract_data(\"./PARAGRAFO/RIPETIZIONE/PRDU\"))\n",
        "                    else:\n",
        "                        prev_town_oa = \"NR\"\n",
        "            type_oa = extract_data(\"./PARAGRAFO/OGTT\")\n",
        "            notes_oa = extract_data(\"./PARAGRAFO/OSS\")\n",
        "            oa_entry = extract_data(\"./PARAGRAFO/NRSCHEDA\")\n",
        "\n",
        "    row = [oa_ser, f_ser, inv, shelf,\n",
        "           country_oa, town_oa, rep_oa, prev_town_oa, aut_f_place,\n",
        "           aut_f, title_f, title_oa, aut_oa,\n",
        "           type_oa, subj_main, subj_sub,\n",
        "           neg_num,\n",
        "           notes_f, notes_oa,\n",
        "           filename, f_entry, oa_entry,\n",
        "           shotdates, printdates_start, printdates_end, aut_f_dates]\n",
        "    data.append(row)\n",
        "\n",
        "#Write the data and their header in a new csv dataset\n",
        "with open('data/data/F_OA_selected_data', 'w', encoding='UTF8', newline='') as tabular_data:\n",
        "    writer = csv.writer(tabular_data)\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(data)"
      ],
      "metadata": {
        "trusted": true,
        "id": "d_Z8m2qdjB8K"
      },
      "execution_count": null,
      "outputs": []
      "tags": [
        "output_scroll",
        ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pEJ4cZ32j3of"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data elaboration\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WvgsBKu0cKpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from csv import DictReader\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import pandas as pd\n",
        "import ssl\n",
        "from geopy.geocoders import Nominatim"
      ],
      "metadata": {
        "id": "DRaKAR62kXIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "geolocator = Nominatim(timeout=10, user_agent = \"myGeolocator\")"
      ],
      "metadata": {
        "id": "XuizLixzktEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "66Ejy3lGkspx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# functions\n",
        "# define a function to open file in reading mode\n",
        "def process_csv(data_file_path):\n",
        "    import csv\n",
        "    source = open(data_file_path, mode=\"r\", encoding=\"UTF8\")\n",
        "    source_reader = csv.DictReader(source)\n",
        "    source_data = list(source_reader)\n",
        "    return source_data\n",
        "\n",
        "#define a function for transforming lists of elements in strings\n",
        "def write_string(source, output_txt_name):\n",
        "    string = \"\"\n",
        "    for source_data in source:\n",
        "        string = string+source_data+\"|\"\n",
        "    string = string[:-1]\n",
        "    with open(output_txt_name, \"w\", encoding=\"utf8\") as f:\n",
        "        f.write(string)\n",
        "    return string\n",
        "\n",
        "#define a function to query endpoints\n",
        "def query_endpoint(endpoint_url, SPRQL_query):\n",
        "    get_endpoint = endpoint_url\n",
        "    sparql_w = SPARQLWrapper(get_endpoint)\n",
        "    sparql_w.setQuery(SPRQL_query)\n",
        "    sparql_w.setReturnFormat(JSON)\n",
        "    spqrl_w_res = sparql_w.query().convert()\n",
        "    return spqrl_w_res\n",
        "\n",
        "#define a function to manipulate results and have back 1. a set of wd_URI corresponding to our wd_names,\n",
        "# 2. update of ph_matrix, 3. not matched wd_names\n",
        "\n",
        "def manipulate(spqrl_w_res, dataset_to_enhance, resNF_txt_name, resF_txt_name):\n",
        "    res_all = set()\n",
        "    res_dic = {}\n",
        "    res_NF_tem = set()\n",
        "    res_F = set()\n",
        "    for res in spqrl_w_res[\"results\"][\"bindings\"]:\n",
        "        res_all.add((res[\"fLabel\"][\"value\"], res[\"f\"][\"value\"]))\n",
        "        for datum in dataset_to_enhance:\n",
        "            if datum[\"ph_wd_URI\"]:\n",
        "                continue\n",
        "            else:\n",
        "                if datum[\"ph_wd_name\"] not in res_dic:\n",
        "                    if res[\"fLabel\"][\"value\"] == datum[\"ph_wd_name\"]:\n",
        "                        res_F.add(res[\"f\"][\"value\"])\n",
        "                        new_pairs = {\"ph_wd_URI\": res[\"f\"][\"value\"]}\n",
        "                        res_dic.update({datum[\"ph_wd_name\"]: new_pairs})\n",
        "                        datum.update([(\"ph_wd_URI\", res[\"f\"][\"value\"])])\n",
        "                    else:\n",
        "                        res_NF_tem.add(datum[\"ph_wd_name\"])\n",
        "    res_NF_def = res_NF_tem - set(list(res_dic.keys()))\n",
        "    with open(resNF_txt_name, \"w\", encoding=\"utf8\") as f:\n",
        "        f.write(str(res_NF_def))\n",
        "    with open(resF_txt_name, \"w\", encoding=\"utf8\") as f:\n",
        "        f.write(str(res_F))\n",
        "    print(\"labels matched: \", len(res_F))\n",
        "    print(\"labels not found2: \", len(res_NF_def))\n",
        "    return res_F, res_NF_def #, res_all"
      ],
      "metadata": {
        "id": "PDzPD39uj7pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open source data with pandas\n",
        "df_data = pd.read_csv(\"data\\F_OA_selected_data.csv\")\n",
        "\n",
        "#initialize a photograph's frequency dataframe\n",
        "ph_freq = pd.DataFrame(df_data[\"AUFN_Faut\"].value_counts().reset_index().values, columns=[\"AUFN_Faut\", \"count\"])\n",
        "\n",
        "#extend dataframe colums to host next datas\n",
        "ph_freq[\"ph_wd_name\"], ph_freq[\"ph_wd_URI\"], ph_freq[\"gender\"], ph_freq[\"workplace\"], ph_freq[\"lat\"], ph_freq[\"lon\"],\\\n",
        "ph_freq[\"born\"], ph_freq[\"died\"], ph_freq[\"lat\"] = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]"
      ],
      "metadata": {
        "id": "sz5v6r-4k7UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the firts string for the SPARQL query by \n",
        "#normalizing (personal) names in form \"surname, name\" to \"name surname\" as in wikidata\n",
        "#and create a list of the modified names tobe added to the dataframe\n",
        "\n",
        "first_ph_names_string =\"\"\n",
        "ph_wd_name_list = []\n",
        "for ph in ph_freq.index:\n",
        "    ph_name = str(ph_freq[\"AUFN_Faut\"][ph])\n",
        "    # reverse only (personal) names in form \"surname, name\" > \"name surname\"\n",
        "    if \", \" in ph_name:    \n",
        "        ph_split = ph_name.split(\", \")\n",
        "        ph_wd_name = ph_split[1] + \" \" + ph_split[0]\n",
        "    else:\n",
        "        ph_wd_name = ph_name\n",
        "    ph_wd_name_list.append(ph_wd_name)\n",
        "    first_ph_names_string = first_ph_names_string + ph_wd_name + \"|\"\n",
        "first_ph_names_string = first_ph_names_string[:-1]\n",
        "#save the string to text\n",
        "with open(\"data\\ph_string1.txt\", \"w\", encoding=\"utf8\") as f:\n",
        "    f.write(first_string)\n",
        "\n",
        "#add ph_wd_name_list to the dataframe\n",
        "ph_freq[\"ph_wd_name\"] = [ph_wd_name_list]\n",
        "#save the dataframe in a csv file\n",
        "ph_freq.to_csv(\"data\\1_ph_freq.csv\", encoding=\"utf-8\")\n",
        "\n",
        "#open the dataframe as dictionary\n",
        "ph_matrix = process_csv(\"ph_freq.csv\")"
      ],
      "metadata": {
        "id": "1XL0DDREpSb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the first query string to collect wikidata URI\n",
        "first_ph_SPARQL_query = \"\"\"\n",
        "SELECT DISTINCT ?f ?fLabel\n",
        "WHERE\n",
        "{    { ?f wdt:P106 wd:Q33231 } UNION { ?f wdt:P31 wd:Q672070}. #P106_has_for_occupation wd:Q33231_photographer \n",
        "                                                                #P31_is instance wd:Q672070_studios\n",
        "    ?f rdfs:label ?fLabel.\n",
        "     FILTER regex(?fLabel, \\\" \"\"\"+first_ph_names_string+\"\"\" \\\")\n",
        "     FILTER(LANG(?fLabel) = \"en\").\n",
        "}\"\"\""
      ],
      "metadata": {
        "id": "3z0IeTINsOwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#perform the first SPARQL query and result manipulation\n",
        "first_ph_wd_res = query_endpoint(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\", first_ph_SPARQL_query)\n",
        "first_ph_manipulate = manipulate(first_ph_wd_res, ph_matrix, \"ph_NF.txt\", \"ph_F.txt\")\n",
        "first_F_set = first_ph_manipulate[0]\n",
        "first_NF = first_ph_manipulate[1]"
      ],
      "metadata": {
        "id": "KU7kl6G9s0Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#after revising first results, refine the unmatched labels\n",
        "new_list = []\n",
        "for ph_wd_NF in first_NF:\n",
        "    if \"  \" in ph_wd_NF:\n",
        "        ph_wd_new = ph_wd_NF.replace(\"  \", \" \")\n",
        "    elif \"Fratelli\" in ph_wd_NF:\n",
        "        ph_wd_new = ph_wd_NF.replace(\"Fratelli\", \"\")\n",
        "    elif \"&\" in ph_wd_NF:\n",
        "        ph_wd_new = ph_wd_NF.replace(\"&\", \"and\")\n",
        "    elif \"Brogi\" == ph_wd_NF:\n",
        "        ph_wd_new = \"Giacomo Brogi\"\n",
        "    elif \"Incorpora\" == ph_wd_NF:\n",
        "        ph_wd_new = \"Giuseppe Incorpora\"\n",
        "    elif \"Giraudon\" == ph_wd_NF:\n",
        "        ph_wd_new = \"Adolphe Giraudon\"\n",
        "    else:\n",
        "        continue\n",
        "    new_list.append(ph_wd_new)\n",
        "    for ph_data in ph_matrix:\n",
        "        if ph_data[\"ph_wd_name\"] == ph_wd_NF:\n",
        "            ph_data.update([(\"ph_wd_name\", ph_wd_new)])\n",
        "\n",
        "#from the new modified names, by using the function obtain a second string to query \n",
        "second_ph_string = write_string(new_list, \"ph_string2.txt\")"
      ],
      "metadata": {
        "id": "PjIOKX7Yts7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the second query string to collect wikidata URI\n",
        "second_ph_SPARQL_query = \"\"\"\n",
        "SELECT DISTINCT ?f ?fLabel\n",
        "WHERE\n",
        "{    { ?f wdt:P106 wd:Q33231 } UNION { ?f wdt:P31 wd:Q672070}. #P106_has_for_occupation wd:Q33231_photographer \n",
        "                                                                #P31_is instance wd:Q672070_studios\n",
        "    ?f rdfs:label ?fLabel.\n",
        "     FILTER regex(?fLabel, \\\" \"\"\"+second_ph_string+\"\"\" \\\")\n",
        "     FILTER(LANG(?fLabel) = \"en\").\n",
        "}\n",
        "\"\"\"\n",
        "#perform the second SPARQL query and result manipulation\n",
        "second_ph_wd_res = query_endpoint(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\", second_ph_SPARQL_query)\n",
        "second_manipulate = manipulate(second_ph_wd_res, ph_matrix, \"ph_NF2\", \"ph_F2.txt\")\n",
        "second_F_set = second_manipulate[0]"
      ],
      "metadata": {
        "id": "eZjQSnZsuRZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#obtain the list of found wikidata URI\n",
        "complex_F_set = second_F_set.union(first_F_set)\n",
        "#print(complex_F_set)\n",
        "\n",
        "#prepare the thirtd string to be passed in SPARQL query\n",
        "third_ph_string_URI =\"\"\n",
        "for F_URI in complex_F_set:\n",
        "    third_ph_string_URI = third_ph_string_URI+\"<\"+F_URI+\">\"\n",
        "#with open(\"ph_URI.txt\", \"w\", encoding=\"utf8\") as f:\n",
        "#    f.write(stringURI)\n",
        "#print(stringURI)\n",
        "\n",
        "#third query\n",
        "third_ph_SPARQL_query = \"\"\"\n",
        "SELECT DISTINCT ?ph ?genderLabel ?countryLabel ?birthyear ?deathyear\n",
        "    WHERE\n",
        "    { VALUES ?ph {\"\"\"+stringURI+\"\"\"} \n",
        "        ?ph rdfs:label ?phLabel;\n",
        "        wdt:P937 ?country; #P937_worklocation\n",
        "        #wdt:P27 ?citiz;        \n",
        "        wdt:P569 ?birth;\n",
        "        wdt:P570 ?death.\n",
        "        OPTIONAL {FILTER(LANG(?fLabel) = \"en\").\n",
        "                    ?ph wdt:P21 ?gender;\n",
        "                    #wdt:P937 ?worklocation; #P937_worklocation\n",
        "        }\n",
        "        BIND(year(?birth) AS ?birthyear)\n",
        "        BIND(year(?death) AS ?deathyear)\n",
        "\n",
        "        #BIND(COALESCE(?worklocation, ?citiz, \"NaN\") AS ?country).\n",
        "        #BIND(IF(BOUND(?worklocation),?worklocation,?citiz) AS ?country).\n",
        "    SERVICE wikibase:label {bd:serviceParam wikibase:language \"en\".}     \n",
        "    }\"\"\"\n",
        "#OPTIONAL { ?ph wdt:P569 ?birthdate;        wdt:P570 ?deathdate.} ci servono...\n",
        "\n",
        "#perform the third query\n",
        "third_ph_wd_res = query_endpoint(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\", third_ph_SPARQL_query)\n",
        "\n",
        "#manipulate results\n",
        "wd_total_dic = {}\n",
        "for result in third_ph_wd_res[\"results\"][\"bindings\"]:\n",
        "    item_key = result[\"ph\"][\"value\"]\n",
        "    item_value = {\"workplace\": result[\"countryLabel\"][\"value\"],\n",
        "                  \"born\": result[\"birthyear\"][\"value\"],\n",
        "                  \"died\": result[\"deathyear\"][\"value\"]}\n",
        "    if item_key not in wd_total_dic:\n",
        "        wd_total_dic.update({item_key: item_value})\n",
        "        for ph_data in ph_matrix:\n",
        "            if ph_data[\"ph_wd_URI\"] == item_key:\n",
        "                item2=item_value.items()\n",
        "                ph_data.update(item2)\n",
        "\n",
        "wd_total_list = list(wd_total_dic.values())\n",
        "print(wd_total_list)\n",
        "\n",
        "keys = wd_total_list[0].keys()\n",
        "with open(\"ph_wd_total.csv\", \"w\", encoding=\"UTF8\", newline=\"\") as output_file:\n",
        "    dict_writer = csv.DictWriter(output_file, keys)\n",
        "    dict_writer.writeheader()\n",
        "    dict_writer.writerows(wd_total_list)"
      ],
      "metadata": {
        "id": "RsSNGDE8v1py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data visualization"
      ],
      "metadata": {
        "id": "sxPYRxMacLDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nuova sezione"
      ],
      "metadata": {
        "id": "pGcc1H-2cGWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "**Analyse**\n",
        "pandas library in order to examine our data.\n",
        "     \n",
        "       \n",
        "    1. Data Preparation:\n",
        "          - creation of two complexive xml files for F and OA records coming from the Federico Zeri Foundation catalogues\n",
        "          - extraction from nested xml stucture of relevant information for the project and structuring them in plain tabular format\n",
        "    2. Data Elaboration: seeking for furter analysis elements via:\n",
        "          - deeper work on photographer for enhance their information\n",
        "          - deeper work on places\n",
        "          - work on unstructured annotations: NER\n",
        "     2. Data Visualization\n"
      ],
      "metadata": {
        "id": "_CBY57CbY2B0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "**Analyse**\n",
        "pandas library in order to examine our data.\n",
        "     \n",
        "       \n",
        "    1. Data Preparation:\n",
        "          - creation of two complexive xml files for F and OA records coming from the Federico Zeri Foundation catalogues\n",
        "          - extraction from nested xml stucture of relevant information for the project and structuring them in plain tabular format\n",
        "    2. Data Elaboration: seeking for furter analysis elements via:\n",
        "          - deeper work on photographer for enhance their information\n",
        "          - deeper work on places\n",
        "          - work on unstructured annotations: NER\n",
        "     2. Data Visualization\n"
      ],
      "metadata": {
        "id": "rSDRDeMOjB8S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kk-RBvonXIBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
